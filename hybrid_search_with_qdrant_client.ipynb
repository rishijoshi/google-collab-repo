{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPPCWQpMtM5lzo3awoOeCkR"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "e86f5398733d424097c5bfd13dfac39d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FileUploadModel",
          "model_module_version": "1.5.0",
          "state": {
            "_counter": 0,
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FileUploadModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "FileUploadView",
            "accept": ".pdf",
            "button_style": "",
            "data": [],
            "description": "Choose PDF",
            "description_tooltip": null,
            "disabled": false,
            "error": "",
            "icon": "upload",
            "layout": "IPY_MODEL_0b5a39a6318a450d9070da8d20c8382a",
            "metadata": [],
            "multiple": false,
            "style": "IPY_MODEL_bde6e399975342ddb8997450b87be5e6"
          }
        },
        "0b5a39a6318a450d9070da8d20c8382a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bde6e399975342ddb8997450b87be5e6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        },
        "f65efc21c32244d1a9e57bf469d22466": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FileUploadModel",
          "model_module_version": "1.5.0",
          "state": {
            "_counter": 0,
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FileUploadModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "FileUploadView",
            "accept": ".pdf,.md",
            "button_style": "",
            "data": [],
            "description": "Choose Files",
            "description_tooltip": null,
            "disabled": false,
            "error": "",
            "icon": "upload",
            "layout": "IPY_MODEL_0d09532573d54da5bfbd5cb327882b6e",
            "metadata": [],
            "multiple": true,
            "style": "IPY_MODEL_be66be03bd754782bca7fb001e7971b0"
          }
        },
        "0d09532573d54da5bfbd5cb327882b6e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "be66be03bd754782bca7fb001e7971b0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        },
        "70c2b8e136fc435d8fc013d79c4013ef": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FileUploadModel",
          "model_module_version": "1.5.0",
          "state": {
            "_counter": 0,
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FileUploadModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "FileUploadView",
            "accept": ".pdf,.md",
            "button_style": "",
            "data": [],
            "description": "Choose Files",
            "description_tooltip": null,
            "disabled": false,
            "error": "",
            "icon": "upload",
            "layout": "IPY_MODEL_2aaf2d22c83e416e930cd74e261de936",
            "metadata": [],
            "multiple": true,
            "style": "IPY_MODEL_b29d4b667301420a83707574cfdbf9e5"
          }
        },
        "2aaf2d22c83e416e930cd74e261de936": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b29d4b667301420a83707574cfdbf9e5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        },
        "8335f47a98384b3b84ddd2cf55041a64": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ButtonView",
            "button_style": "",
            "description": "Start Ingestion",
            "disabled": true,
            "icon": "",
            "layout": "IPY_MODEL_907654fdf8274042b11ab5f60e5cf052",
            "style": "IPY_MODEL_7141585f94ed4fcc8302051072f2738c",
            "tooltip": "Click to start ingestion of selected files"
          }
        },
        "907654fdf8274042b11ab5f60e5cf052": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7141585f94ed4fcc8302051072f2738c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        },
        "c8f8529c0b9945628f1ae6a20e0041c2": {
          "model_module": "@jupyter-widgets/output",
          "model_name": "OutputModel",
          "model_module_version": "1.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/output",
            "_model_module_version": "1.0.0",
            "_model_name": "OutputModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/output",
            "_view_module_version": "1.0.0",
            "_view_name": "OutputView",
            "layout": "IPY_MODEL_cabdb67687224e71b3d2d6daf638f7d0",
            "msg_id": "",
            "outputs": []
          }
        },
        "cabdb67687224e71b3d2d6daf638f7d0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c71e4c547d8744bfaac76426e8c32656": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FileUploadModel",
          "model_module_version": "1.5.0",
          "state": {
            "_counter": 0,
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FileUploadModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "FileUploadView",
            "accept": ".pdf,.md",
            "button_style": "",
            "data": [],
            "description": "Choose Files",
            "description_tooltip": null,
            "disabled": false,
            "error": "",
            "icon": "upload",
            "layout": "IPY_MODEL_29edaeda177d4b92bad00b631bcaa3e7",
            "metadata": [],
            "multiple": true,
            "style": "IPY_MODEL_efec971084ad4e2fb5bcf42f22d442c8"
          }
        },
        "29edaeda177d4b92bad00b631bcaa3e7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "efec971084ad4e2fb5bcf42f22d442c8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        },
        "f32e3b4835114e9e87c32aa046f52893": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ButtonView",
            "button_style": "",
            "description": "Start Ingestion",
            "disabled": true,
            "icon": "",
            "layout": "IPY_MODEL_66c75cd36d9944a694b6dc508f596909",
            "style": "IPY_MODEL_8e71e6b2be6246168b1fb804882c5535",
            "tooltip": "Click to start ingestion of selected files"
          }
        },
        "66c75cd36d9944a694b6dc508f596909": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8e71e6b2be6246168b1fb804882c5535": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        },
        "4b097d69c2b5444a80d9b2c0669d2ac6": {
          "model_module": "@jupyter-widgets/output",
          "model_name": "OutputModel",
          "model_module_version": "1.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/output",
            "_model_module_version": "1.0.0",
            "_model_name": "OutputModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/output",
            "_view_module_version": "1.0.0",
            "_view_name": "OutputView",
            "layout": "IPY_MODEL_81279f304fac441b97f03448c30a8f7e",
            "msg_id": "",
            "outputs": []
          }
        },
        "81279f304fac441b97f03448c30a8f7e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "465f05bdcc594872963ba0009c795210": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FileUploadModel",
          "model_module_version": "1.5.0",
          "state": {
            "_counter": 62,
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FileUploadModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "FileUploadView",
            "accept": ".pdf,.md",
            "button_style": "",
            "data": [
              null,
              null,
              null,
              null,
              null,
              null,
              null,
              null,
              null,
              null,
              null,
              null,
              null,
              null,
              null,
              null,
              null,
              null,
              null,
              null,
              null,
              null,
              null,
              null,
              null,
              null,
              null,
              null,
              null,
              null,
              null,
              null,
              null,
              null,
              null,
              null,
              null,
              null,
              null,
              null,
              null,
              null,
              null,
              null,
              null,
              null,
              null,
              null,
              null,
              null,
              null,
              null,
              null,
              null,
              null,
              null,
              null,
              null,
              null,
              null,
              null,
              null
            ],
            "description": "Choose Files",
            "description_tooltip": null,
            "disabled": false,
            "error": "",
            "icon": "upload",
            "layout": "IPY_MODEL_7aa1912dfe5a454cbb6a4ce89eec789c",
            "metadata": [
              {
                "name": "cal.com_docs_api-reference_v2_teams-verified-resources_get-list-of-verified-emails-of-a-team.md",
                "type": "text/markdown",
                "size": 29588,
                "lastModified": 1755129159000
              },
              {
                "name": "cal.com_docs_api-reference_v2_teams-verified-resources_get-list-of-verified-phone-numbers-of-a-team.md",
                "type": "text/markdown",
                "size": 29617,
                "lastModified": 1755129159000
              },
              {
                "name": "cal.com_docs_api-reference_v2_teams-verified-resources_get-verified-email-of-a-team-by-id.md",
                "type": "text/markdown",
                "size": 29396,
                "lastModified": 1755129159000
              },
              {
                "name": "cal.com_docs_api-reference_v2_teams-verified-resources_get-verified-phone-number-of-a-team-by-id.md",
                "type": "text/markdown",
                "size": 29399,
                "lastModified": 1755129159000
              },
              {
                "name": "cal.com_docs_api-reference_v2_teams-verified-resources_request-email-verification-code.md",
                "type": "text/markdown",
                "size": 28024,
                "lastModified": 1755129159000
              },
              {
                "name": "cal.com_docs_api-reference_v2_teams-verified-resources_request-phone-number-verification-code.md",
                "type": "text/markdown",
                "size": 28083,
                "lastModified": 1755129159000
              },
              {
                "name": "cal.com_docs_api-reference_v2_teams-verified-resources_verify-a-phone-number-for-an-org-team.md",
                "type": "text/markdown",
                "size": 29678,
                "lastModified": 1755129159000
              },
              {
                "name": "cal.com_docs_api-reference_v2_teams-verified-resources_verify-an-email-for-a-team.md",
                "type": "text/markdown",
                "size": 29631,
                "lastModified": 1755129159000
              },
              {
                "name": "cal.com_docs_api-reference_v2_v1-v2-differences.md",
                "type": "text/markdown",
                "size": 28074,
                "lastModified": 1755129159000
              },
              {
                "name": "cal.com_docs_api-reference_v2_verified-resources_get-list-of-verified-emails.md",
                "type": "text/markdown",
                "size": 29302,
                "lastModified": 1755129159000
              },
              {
                "name": "cal.com_docs_api-reference_v2_verified-resources_get-list-of-verified-phone-numbers.md",
                "type": "text/markdown",
                "size": 29343,
                "lastModified": 1755129159000
              },
              {
                "name": "cal.com_docs_api-reference_v2_verified-resources_get-verified-email-by-id.md",
                "type": "text/markdown",
                "size": 29144,
                "lastModified": 1755129159000
              },
              {
                "name": "cal.com_docs_api-reference_v2_verified-resources_get-verified-phone-number-by-id.md",
                "type": "text/markdown",
                "size": 29133,
                "lastModified": 1755129159000
              },
              {
                "name": "cal.com_docs_api-reference_v2_verified-resources_request-email-verification-code.md",
                "type": "text/markdown",
                "size": 27996,
                "lastModified": 1755129159000
              },
              {
                "name": "cal.com_docs_api-reference_v2_verified-resources_request-phone-number-verification-code.md",
                "type": "text/markdown",
                "size": 27972,
                "lastModified": 1755129159000
              },
              {
                "name": "cal.com_docs_api-reference_v2_verified-resources_verify-a-phone-number.md",
                "type": "text/markdown",
                "size": 29360,
                "lastModified": 1755129159000
              },
              {
                "name": "cal.com_docs_api-reference_v2_verified-resources_verify-an-email.md",
                "type": "text/markdown",
                "size": 29358,
                "lastModified": 1755129159000
              },
              {
                "name": "cal.com_docs_api-reference_v2_webhooks_create-a-webhook.md",
                "type": "text/markdown",
                "size": 29121,
                "lastModified": 1755129159000
              },
              {
                "name": "cal.com_docs_api-reference_v2_webhooks_delete-a-webhook.md",
                "type": "text/markdown",
                "size": 28135,
                "lastModified": 1755129159000
              },
              {
                "name": "cal.com_docs_api-reference_v2_webhooks_get-a-webhook.md",
                "type": "text/markdown",
                "size": 28030,
                "lastModified": 1755129159000
              },
              {
                "name": "cal.com_docs_api-reference_v2_webhooks_get-all-webhooks.md",
                "type": "text/markdown",
                "size": 28416,
                "lastModified": 1755129159000
              },
              {
                "name": "cal.com_docs_api-reference_v2_webhooks_update-a-webhook.md",
                "type": "text/markdown",
                "size": 29183,
                "lastModified": 1755129159000
              },
              {
                "name": "cal.com_docs_api-reference_v2.md",
                "type": "text/markdown",
                "size": 254137,
                "lastModified": 1755129159000
              },
              {
                "name": "cal.com_docs_developing_guides_api_how-to-setup-api-in-a-local-instance.md",
                "type": "text/markdown",
                "size": 3096,
                "lastModified": 1755129159000
              },
              {
                "name": "cal.com_docs_developing_guides_appstore-and-integration_build-a-greeter-app.md",
                "type": "text/markdown",
                "size": 9069,
                "lastModified": 1755129159000
              },
              {
                "name": "cal.com_docs_developing_guides_appstore-and-integration_build-an-app.md",
                "type": "text/markdown",
                "size": 6455,
                "lastModified": 1755129159000
              },
              {
                "name": "cal.com_docs_developing_guides_appstore-and-integration_how-to-show-assigned-people-from-a-crm.md",
                "type": "text/markdown",
                "size": 4577,
                "lastModified": 1755129159000
              },
              {
                "name": "cal.com_docs_developing_introduction.md",
                "type": "text/markdown",
                "size": 6249,
                "lastModified": 1755129159000
              },
              {
                "name": "cal.com_docs_developing_local-development.md",
                "type": "text/markdown",
                "size": 96507,
                "lastModified": 1755129159000
              },
              {
                "name": "cal.com_docs_developing_open-source-contribution__22https__github.com_vercel_next.js_22.md",
                "type": "text/markdown",
                "size": 3121,
                "lastModified": 1755129159000
              },
              {
                "name": "cal.com_docs_developing_open-source-contribution__22https__trpc.io__22.md",
                "type": "text/markdown",
                "size": 6249,
                "lastModified": 1755129159000
              },
              {
                "name": "cal.com_docs_developing_open-source-contribution__22https__www.postgresql.org__22.md",
                "type": "text/markdown",
                "size": 6249,
                "lastModified": 1755129159000
              },
              {
                "name": "cal.com_docs_developing_open-source-contribution__22https__www.prisma.io__22.md",
                "type": "text/markdown",
                "size": 6249,
                "lastModified": 1755129159000
              },
              {
                "name": "cal.com_docs_developing_open-source-contribution__22https__www.typescriptlang.org__22.md",
                "type": "text/markdown",
                "size": 6249,
                "lastModified": 1755129159000
              },
              {
                "name": "cal.com_docs_developing_open-source-contribution__22https__zod.dev__22.md",
                "type": "text/markdown",
                "size": 6249,
                "lastModified": 1755129159000
              },
              {
                "name": "cal.com_docs_developing_open-source-contribution_code-styling.md",
                "type": "text/markdown",
                "size": 12969,
                "lastModified": 1755129159000
              },
              {
                "name": "cal.com_docs_developing_open-source-contribution_contributors-guide.md",
                "type": "text/markdown",
                "size": 187373,
                "lastModified": 1755129159000
              },
              {
                "name": "cal.com_docs_developing_open-source-contribution_introduction.md",
                "type": "text/markdown",
                "size": 61940,
                "lastModified": 1755129159000
              },
              {
                "name": "cal.com_docs_developing_open-source-contribution_pull-requests.md",
                "type": "text/markdown",
                "size": 30485,
                "lastModified": 1755129159000
              },
              {
                "name": "cal.com_docs_how-to-guides_how-to-build-an-app.md",
                "type": "text/markdown",
                "size": 38765,
                "lastModified": 1755129159000
              },
              {
                "name": "cal.com_docs_platform_atoms.md",
                "type": "text/markdown",
                "size": 3771,
                "lastModified": 1755129159000
              },
              {
                "name": "cal.com_docs_platform_faq.md",
                "type": "text/markdown",
                "size": 6393,
                "lastModified": 1755129159000
              },
              {
                "name": "cal.com_docs_platform_guides.md",
                "type": "text/markdown",
                "size": 2918,
                "lastModified": 1755129159000
              },
              {
                "name": "cal.com_docs_platform_introduction.md",
                "type": "text/markdown",
                "size": 4579,
                "lastModified": 1755129159000
              },
              {
                "name": "cal.com_docs_platform_quickstart.md",
                "type": "text/markdown",
                "size": 19139,
                "lastModified": 1755129159000
              },
              {
                "name": "cal.com_docs_platform_setup.md",
                "type": "text/markdown",
                "size": 2723,
                "lastModified": 1755129159000
              },
              {
                "name": "cal.com_docs_self-hosting_apps_install-apps_introduction.md",
                "type": "text/markdown",
                "size": 3795,
                "lastModified": 1755129159000
              },
              {
                "name": "cal.com_docs_self-hosting_database-migrations.md",
                "type": "text/markdown",
                "size": 20733,
                "lastModified": 1755129159000
              },
              {
                "name": "cal.com_docs_self-hosting_deployments_aws.md",
                "type": "text/markdown",
                "size": 15321,
                "lastModified": 1755129159000
              },
              {
                "name": "cal.com_docs_self-hosting_deployments_azure.md",
                "type": "text/markdown",
                "size": 27347,
                "lastModified": 1755129159000
              },
              {
                "name": "cal.com_docs_self-hosting_deployments_elestio.md",
                "type": "text/markdown",
                "size": 4719,
                "lastModified": 1755129159000
              },
              {
                "name": "cal.com_docs_self-hosting_deployments_gcp.md",
                "type": "text/markdown",
                "size": 120745,
                "lastModified": 1755129159000
              },
              {
                "name": "cal.com_docs_self-hosting_deployments_railway.md",
                "type": "text/markdown",
                "size": 4983,
                "lastModified": 1755129159000
              },
              {
                "name": "cal.com_docs_self-hosting_deployments_render.md",
                "type": "text/markdown",
                "size": 4777,
                "lastModified": 1755129159000
              },
              {
                "name": "cal.com_docs_self-hosting_deployments_vercel.md",
                "type": "text/markdown",
                "size": 29759,
                "lastModified": 1755129159000
              },
              {
                "name": "cal.com_docs_self-hosting_deployments.md",
                "type": "text/markdown",
                "size": 15321,
                "lastModified": 1755129159000
              },
              {
                "name": "cal.com_docs_self-hosting_docker.md",
                "type": "text/markdown",
                "size": 36190,
                "lastModified": 1755129159000
              },
              {
                "name": "cal.com_docs_self-hosting_installation.md",
                "type": "text/markdown",
                "size": 108725,
                "lastModified": 1755129159000
              },
              {
                "name": "cal.com_docs_self-hosting_license-key.md",
                "type": "text/markdown",
                "size": 2369,
                "lastModified": 1755129159000
              },
              {
                "name": "cal.com_docs_self-hosting_sso-setup.md",
                "type": "text/markdown",
                "size": 25749,
                "lastModified": 1755129159000
              },
              {
                "name": "cal.com_docs_self-hosting_upgrading.md",
                "type": "text/markdown",
                "size": 2785,
                "lastModified": 1755129159000
              },
              {
                "name": "cal.com_docs_self-hosting.md",
                "type": "text/markdown",
                "size": 108725,
                "lastModified": 1755129159000
              }
            ],
            "multiple": true,
            "style": "IPY_MODEL_39fcc31d3f6d45859f95cb449328f852"
          }
        },
        "7aa1912dfe5a454cbb6a4ce89eec789c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "39fcc31d3f6d45859f95cb449328f852": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        },
        "9f89549e4c3c4eb1a029170795fb47b9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ButtonView",
            "button_style": "",
            "description": "Start Ingestion",
            "disabled": false,
            "icon": "",
            "layout": "IPY_MODEL_adc8212b185144708b2ecc169d4be567",
            "style": "IPY_MODEL_96da2fb0af1d4bb78f7c3e866362e4e2",
            "tooltip": "Click to start ingestion of selected files"
          }
        },
        "adc8212b185144708b2ecc169d4be567": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "96da2fb0af1d4bb78f7c3e866362e4e2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        },
        "e4feeed8d92c42d38474b392eefa534b": {
          "model_module": "@jupyter-widgets/output",
          "model_name": "OutputModel",
          "model_module_version": "1.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/output",
            "_model_module_version": "1.0.0",
            "_model_name": "OutputModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/output",
            "_view_module_version": "1.0.0",
            "_view_name": "OutputView",
            "layout": "IPY_MODEL_6948556847e94fb59fca35c2d79d88c2",
            "msg_id": "13b7a880-6f50-470c-b418-f8a9e33f8a2d",
            "outputs": [
              {
                "output_type": "stream",
                "name": "stdout",
                "text": [
                  "🔄 Starting ingestion process...\n",
                  "Processing 10 documents from a list.\n",
                  "Extracting content from Markdown: /content/cal.com_docs_api-reference_v2_teams-verified-resources_get-list-of-verified-emails-of-a-team.md\n",
                  "Found image reference: 'https://cal.com/calcom-docs.svg', attempting to load from '/content/https://cal.com/calcom-docs.svg'\n",
                  "⚠️ Image file not found: /content/https://cal.com/calcom-docs.svg\n",
                  "Found image reference: 'https://cal.com/calcom-docs-light.svg', attempting to load from '/content/https://cal.com/calcom-docs-light.svg'\n",
                  "⚠️ Image file not found: /content/https://cal.com/calcom-docs-light.svg\n",
                  "Extracted text from markdown /content/cal.com_docs_api-reference_v2_teams-verified-resources_get-list-of-verified-emails-of-a-team.md. Text length: 24952\n",
                  "Split into 32 text chunks from markdown\n",
                  "✅ Extracted 32 chunks (text and image) from markdown /content/cal.com_docs_api-reference_v2_teams-verified-resources_get-list-of-verified-emails-of-a-team.md\n",
                  "Extracting content from Markdown: /content/cal.com_docs_api-reference_v2_teams-verified-resources_get-list-of-verified-phone-numbers-of-a-team.md\n",
                  "Found image reference: 'https://cal.com/calcom-docs.svg', attempting to load from '/content/https://cal.com/calcom-docs.svg'\n",
                  "⚠️ Image file not found: /content/https://cal.com/calcom-docs.svg\n",
                  "Found image reference: 'https://cal.com/calcom-docs-light.svg', attempting to load from '/content/https://cal.com/calcom-docs-light.svg'\n",
                  "⚠️ Image file not found: /content/https://cal.com/calcom-docs-light.svg\n",
                  "Extracted text from markdown /content/cal.com_docs_api-reference_v2_teams-verified-resources_get-list-of-verified-phone-numbers-of-a-team.md. Text length: 24974\n",
                  "Split into 32 text chunks from markdown\n",
                  "✅ Extracted 32 chunks (text and image) from markdown /content/cal.com_docs_api-reference_v2_teams-verified-resources_get-list-of-verified-phone-numbers-of-a-team.md\n",
                  "Extracting content from Markdown: /content/cal.com_docs_api-reference_v2_teams-verified-resources_get-verified-email-of-a-team-by-id.md\n",
                  "Found image reference: 'https://cal.com/calcom-docs.svg', attempting to load from '/content/https://cal.com/calcom-docs.svg'\n",
                  "⚠️ Image file not found: /content/https://cal.com/calcom-docs.svg\n",
                  "Found image reference: 'https://cal.com/calcom-docs-light.svg', attempting to load from '/content/https://cal.com/calcom-docs-light.svg'\n",
                  "⚠️ Image file not found: /content/https://cal.com/calcom-docs-light.svg\n",
                  "Extracted text from markdown /content/cal.com_docs_api-reference_v2_teams-verified-resources_get-verified-email-of-a-team-by-id.md. Text length: 24794\n",
                  "Split into 31 text chunks from markdown\n",
                  "✅ Extracted 31 chunks (text and image) from markdown /content/cal.com_docs_api-reference_v2_teams-verified-resources_get-verified-email-of-a-team-by-id.md\n",
                  "Extracting content from Markdown: /content/cal.com_docs_api-reference_v2_teams-verified-resources_get-verified-phone-number-of-a-team-by-id.md\n",
                  "Found image reference: 'https://cal.com/calcom-docs.svg', attempting to load from '/content/https://cal.com/calcom-docs.svg'\n",
                  "⚠️ Image file not found: /content/https://cal.com/calcom-docs.svg\n",
                  "Found image reference: 'https://cal.com/calcom-docs-light.svg', attempting to load from '/content/https://cal.com/calcom-docs-light.svg'\n",
                  "⚠️ Image file not found: /content/https://cal.com/calcom-docs-light.svg\n",
                  "Extracted text from markdown /content/cal.com_docs_api-reference_v2_teams-verified-resources_get-verified-phone-number-of-a-team-by-id.md. Text length: 24790\n",
                  "Split into 31 text chunks from markdown\n",
                  "✅ Extracted 31 chunks (text and image) from markdown /content/cal.com_docs_api-reference_v2_teams-verified-resources_get-verified-phone-number-of-a-team-by-id.md\n",
                  "Extracting content from Markdown: /content/cal.com_docs_api-reference_v2_teams-verified-resources_request-email-verification-code.md\n",
                  "Found image reference: 'https://cal.com/calcom-docs.svg', attempting to load from '/content/https://cal.com/calcom-docs.svg'\n",
                  "⚠️ Image file not found: /content/https://cal.com/calcom-docs.svg\n",
                  "Found image reference: 'https://cal.com/calcom-docs-light.svg', attempting to load from '/content/https://cal.com/calcom-docs-light.svg'\n",
                  "⚠️ Image file not found: /content/https://cal.com/calcom-docs-light.svg\n",
                  "Extracted text from markdown /content/cal.com_docs_api-reference_v2_teams-verified-resources_request-email-verification-code.md. Text length: 23922\n",
                  "Split into 30 text chunks from markdown\n",
                  "✅ Extracted 30 chunks (text and image) from markdown /content/cal.com_docs_api-reference_v2_teams-verified-resources_request-email-verification-code.md\n",
                  "Extracting content from Markdown: /content/cal.com_docs_api-reference_v2_teams-verified-resources_request-phone-number-verification-code.md\n",
                  "Found image reference: 'https://cal.com/calcom-docs.svg', attempting to load from '/content/https://cal.com/calcom-docs.svg'\n",
                  "⚠️ Image file not found: /content/https://cal.com/calcom-docs.svg\n",
                  "Found image reference: 'https://cal.com/calcom-docs-light.svg', attempting to load from '/content/https://cal.com/calcom-docs-light.svg'\n",
                  "⚠️ Image file not found: /content/https://cal.com/calcom-docs-light.svg\n",
                  "Extracted text from markdown /content/cal.com_docs_api-reference_v2_teams-verified-resources_request-phone-number-verification-code.md. Text length: 23976\n",
                  "Split into 30 text chunks from markdown\n",
                  "✅ Extracted 30 chunks (text and image) from markdown /content/cal.com_docs_api-reference_v2_teams-verified-resources_request-phone-number-verification-code.md\n",
                  "Extracting content from Markdown: /content/cal.com_docs_api-reference_v2_teams-verified-resources_verify-a-phone-number-for-an-org-team.md\n",
                  "Found image reference: 'https://cal.com/calcom-docs.svg', attempting to load from '/content/https://cal.com/calcom-docs.svg'\n",
                  "⚠️ Image file not found: /content/https://cal.com/calcom-docs.svg\n",
                  "Found image reference: 'https://cal.com/calcom-docs-light.svg', attempting to load from '/content/https://cal.com/calcom-docs-light.svg'\n",
                  "⚠️ Image file not found: /content/https://cal.com/calcom-docs-light.svg\n",
                  "Extracted text from markdown /content/cal.com_docs_api-reference_v2_teams-verified-resources_verify-a-phone-number-for-an-org-team.md. Text length: 25048\n",
                  "Split into 32 text chunks from markdown\n",
                  "✅ Extracted 32 chunks (text and image) from markdown /content/cal.com_docs_api-reference_v2_teams-verified-resources_verify-a-phone-number-for-an-org-team.md\n",
                  "Extracting content from Markdown: /content/cal.com_docs_api-reference_v2_teams-verified-resources_verify-an-email-for-a-team.md\n",
                  "Found image reference: 'https://cal.com/calcom-docs.svg', attempting to load from '/content/https://cal.com/calcom-docs.svg'\n",
                  "⚠️ Image file not found: /content/https://cal.com/calcom-docs.svg\n",
                  "Found image reference: 'https://cal.com/calcom-docs-light.svg', attempting to load from '/content/https://cal.com/calcom-docs-light.svg'\n",
                  "⚠️ Image file not found: /content/https://cal.com/calcom-docs-light.svg\n",
                  "Extracted text from markdown /content/cal.com_docs_api-reference_v2_teams-verified-resources_verify-an-email-for-a-team.md. Text length: 25010\n",
                  "Split into 32 text chunks from markdown\n",
                  "✅ Extracted 32 chunks (text and image) from markdown /content/cal.com_docs_api-reference_v2_teams-verified-resources_verify-an-email-for-a-team.md\n",
                  "Extracting content from Markdown: /content/cal.com_docs_api-reference_v2_v1-v2-differences.md\n",
                  "Found image reference: 'https://cal.com/calcom-docs.svg', attempting to load from '/content/https://cal.com/calcom-docs.svg'\n",
                  "⚠️ Image file not found: /content/https://cal.com/calcom-docs.svg\n",
                  "Found image reference: 'https://cal.com/calcom-docs-light.svg', attempting to load from '/content/https://cal.com/calcom-docs-light.svg'\n",
                  "⚠️ Image file not found: /content/https://cal.com/calcom-docs-light.svg\n",
                  "Extracted text from markdown /content/cal.com_docs_api-reference_v2_v1-v2-differences.md. Text length: 23469\n",
                  "Split into 30 text chunks from markdown\n",
                  "✅ Extracted 30 chunks (text and image) from markdown /content/cal.com_docs_api-reference_v2_v1-v2-differences.md\n",
                  "Extracting content from Markdown: /content/cal.com_docs_api-reference_v2_verified-resources_get-list-of-verified-emails.md\n",
                  "Found image reference: 'https://cal.com/calcom-docs.svg', attempting to load from '/content/https://cal.com/calcom-docs.svg'\n",
                  "⚠️ Image file not found: /content/https://cal.com/calcom-docs.svg\n",
                  "Found image reference: 'https://cal.com/calcom-docs-light.svg', attempting to load from '/content/https://cal.com/calcom-docs-light.svg'\n",
                  "⚠️ Image file not found: /content/https://cal.com/calcom-docs-light.svg\n",
                  "Extracted text from markdown /content/cal.com_docs_api-reference_v2_verified-resources_get-list-of-verified-emails.md. Text length: 24708\n",
                  "Split into 31 text chunks from markdown\n",
                  "✅ Extracted 31 chunks (text and image) from markdown /content/cal.com_docs_api-reference_v2_verified-resources_get-list-of-verified-emails.md\n"
                ]
              },
              {
                "output_type": "stream",
                "name": "stdout",
                "text": [
                  "✅ BM25 corpus loaded from bm25_calcom_corpus.json\n",
                  "🔄 Starting ingestion process...\n",
                  "Attempting to ingest 311 chunks.\n",
                  "Processing chunk 1/311 (ID: cal.com_docs_api-reference_v2_teams-verified-resources_get-list-of-verified-emails-of-a-team.md_chunk_0, Type: markdown)\n"
                ]
              },
              {
                "output_type": "stream",
                "name": "stdout",
                "text": [
                  "✅ Embedding generated for chunk cal.com_docs_api-reference_v2_teams-verified-resources_get-list-of-verified-emails-of-a-team.md_chunk_0\n",
                  "✅ Point prepared for chunk cal.com_docs_api-reference_v2_teams-verified-resources_get-list-of-verified-emails-of-a-team.md_chunk_0. Total points prepared: 1\n",
                  "✅ Tokenized and filtered chunk cal.com_docs_api-reference_v2_teams-verified-resources_get-list-of-verified-emails-of-a-team.md_chunk_0 for BM25.\n",
                  "Processing chunk 2/311 (ID: cal.com_docs_api-reference_v2_teams-verified-resources_get-list-of-verified-emails-of-a-team.md_chunk_1, Type: markdown)\n"
                ]
              },
              {
                "output_type": "stream",
                "name": "stdout",
                "text": [
                  "✅ Embedding generated for chunk cal.com_docs_api-reference_v2_teams-verified-resources_get-list-of-verified-emails-of-a-team.md_chunk_1\n",
                  "✅ Point prepared for chunk cal.com_docs_api-reference_v2_teams-verified-resources_get-list-of-verified-emails-of-a-team.md_chunk_1. Total points prepared: 2\n",
                  "✅ Tokenized and filtered chunk cal.com_docs_api-reference_v2_teams-verified-resources_get-list-of-verified-emails-of-a-team.md_chunk_1 for BM25.\n",
                  "Processing chunk 3/311 (ID: cal.com_docs_api-reference_v2_teams-verified-resources_get-list-of-verified-emails-of-a-team.md_chunk_2, Type: markdown)\n"
                ]
              },
              {
                "output_type": "stream",
                "name": "stdout",
                "text": [
                  "✅ Embedding generated for chunk cal.com_docs_api-reference_v2_teams-verified-resources_get-list-of-verified-emails-of-a-team.md_chunk_2\n",
                  "✅ Point prepared for chunk cal.com_docs_api-reference_v2_teams-verified-resources_get-list-of-verified-emails-of-a-team.md_chunk_2. Total points prepared: 3\n",
                  "✅ Tokenized and filtered chunk cal.com_docs_api-reference_v2_teams-verified-resources_get-list-of-verified-emails-of-a-team.md_chunk_2 for BM25.\n",
                  "Processing chunk 4/311 (ID: cal.com_docs_api-reference_v2_teams-verified-resources_get-list-of-verified-emails-of-a-team.md_chunk_3, Type: markdown)\n"
                ]
              },
              {
                "output_type": "stream",
                "name": "stdout",
                "text": [
                  "✅ Embedding generated for chunk cal.com_docs_api-reference_v2_teams-verified-resources_get-list-of-verified-emails-of-a-team.md_chunk_3\n",
                  "✅ Point prepared for chunk cal.com_docs_api-reference_v2_teams-verified-resources_get-list-of-verified-emails-of-a-team.md_chunk_3. Total points prepared: 4\n",
                  "✅ Tokenized and filtered chunk cal.com_docs_api-reference_v2_teams-verified-resources_get-list-of-verified-emails-of-a-team.md_chunk_3 for BM25.\n",
                  "Processing chunk 5/311 (ID: cal.com_docs_api-reference_v2_teams-verified-resources_get-list-of-verified-emails-of-a-team.md_chunk_4, Type: markdown)\n"
                ]
              },
              {
                "output_type": "stream",
                "name": "stdout",
                "text": [
                  "✅ Embedding generated for chunk cal.com_docs_api-reference_v2_teams-verified-resources_get-list-of-verified-emails-of-a-team.md_chunk_4\n",
                  "✅ Point prepared for chunk cal.com_docs_api-reference_v2_teams-verified-resources_get-list-of-verified-emails-of-a-team.md_chunk_4. Total points prepared: 5\n",
                  "✅ Tokenized and filtered chunk cal.com_docs_api-reference_v2_teams-verified-resources_get-list-of-verified-emails-of-a-team.md_chunk_4 for BM25.\n",
                  "Processing chunk 6/311 (ID: cal.com_docs_api-reference_v2_teams-verified-resources_get-list-of-verified-emails-of-a-team.md_chunk_5, Type: markdown)\n"
                ]
              },
              {
                "output_type": "stream",
                "name": "stdout",
                "text": [
                  "✅ Embedding generated for chunk cal.com_docs_api-reference_v2_teams-verified-resources_get-list-of-verified-emails-of-a-team.md_chunk_5\n",
                  "✅ Point prepared for chunk cal.com_docs_api-reference_v2_teams-verified-resources_get-list-of-verified-emails-of-a-team.md_chunk_5. Total points prepared: 6\n",
                  "✅ Tokenized and filtered chunk cal.com_docs_api-reference_v2_teams-verified-resources_get-list-of-verified-emails-of-a-team.md_chunk_5 for BM25.\n",
                  "Processing chunk 7/311 (ID: cal.com_docs_api-reference_v2_teams-verified-resources_get-list-of-verified-emails-of-a-team.md_chunk_6, Type: markdown)\n"
                ]
              },
              {
                "output_type": "stream",
                "name": "stdout",
                "text": [
                  "✅ Embedding generated for chunk cal.com_docs_api-reference_v2_teams-verified-resources_get-list-of-verified-emails-of-a-team.md_chunk_6\n",
                  "✅ Point prepared for chunk cal.com_docs_api-reference_v2_teams-verified-resources_get-list-of-verified-emails-of-a-team.md_chunk_6. Total points prepared: 7\n",
                  "✅ Tokenized and filtered chunk cal.com_docs_api-reference_v2_teams-verified-resources_get-list-of-verified-emails-of-a-team.md_chunk_6 for BM25.\n",
                  "Processing chunk 8/311 (ID: cal.com_docs_api-reference_v2_teams-verified-resources_get-list-of-verified-emails-of-a-team.md_chunk_7, Type: markdown)\n"
                ]
              },
              {
                "output_type": "stream",
                "name": "stdout",
                "text": [
                  "✅ Embedding generated for chunk cal.com_docs_api-reference_v2_teams-verified-resources_get-list-of-verified-emails-of-a-team.md_chunk_7\n",
                  "✅ Point prepared for chunk cal.com_docs_api-reference_v2_teams-verified-resources_get-list-of-verified-emails-of-a-team.md_chunk_7. Total points prepared: 8\n",
                  "✅ Tokenized and filtered chunk cal.com_docs_api-reference_v2_teams-verified-resources_get-list-of-verified-emails-of-a-team.md_chunk_7 for BM25.\n",
                  "Processing chunk 9/311 (ID: cal.com_docs_api-reference_v2_teams-verified-resources_get-list-of-verified-emails-of-a-team.md_chunk_8, Type: markdown)\n"
                ]
              },
              {
                "output_type": "stream",
                "name": "stdout",
                "text": [
                  "✅ Embedding generated for chunk cal.com_docs_api-reference_v2_teams-verified-resources_get-list-of-verified-emails-of-a-team.md_chunk_8\n",
                  "✅ Point prepared for chunk cal.com_docs_api-reference_v2_teams-verified-resources_get-list-of-verified-emails-of-a-team.md_chunk_8. Total points prepared: 9\n",
                  "✅ Tokenized and filtered chunk cal.com_docs_api-reference_v2_teams-verified-resources_get-list-of-verified-emails-of-a-team.md_chunk_8 for BM25.\n",
                  "Processing chunk 10/311 (ID: cal.com_docs_api-reference_v2_teams-verified-resources_get-list-of-verified-emails-of-a-team.md_chunk_9, Type: markdown)\n"
                ]
              },
              {
                "output_type": "stream",
                "name": "stdout",
                "text": [
                  "✅ Embedding generated for chunk cal.com_docs_api-reference_v2_teams-verified-resources_get-list-of-verified-emails-of-a-team.md_chunk_9\n",
                  "✅ Point prepared for chunk cal.com_docs_api-reference_v2_teams-verified-resources_get-list-of-verified-emails-of-a-team.md_chunk_9. Total points prepared: 10\n",
                  "✅ Tokenized and filtered chunk cal.com_docs_api-reference_v2_teams-verified-resources_get-list-of-verified-emails-of-a-team.md_chunk_9 for BM25.\n",
                  "Processing chunk 11/311 (ID: cal.com_docs_api-reference_v2_teams-verified-resources_get-list-of-verified-emails-of-a-team.md_chunk_10, Type: markdown)\n"
                ]
              },
              {
                "output_type": "stream",
                "name": "stdout",
                "text": [
                  "✅ Embedding generated for chunk cal.com_docs_api-reference_v2_teams-verified-resources_get-list-of-verified-emails-of-a-team.md_chunk_10\n",
                  "✅ Point prepared for chunk cal.com_docs_api-reference_v2_teams-verified-resources_get-list-of-verified-emails-of-a-team.md_chunk_10. Total points prepared: 11\n",
                  "✅ Tokenized and filtered chunk cal.com_docs_api-reference_v2_teams-verified-resources_get-list-of-verified-emails-of-a-team.md_chunk_10 for BM25.\n",
                  "Processing chunk 12/311 (ID: cal.com_docs_api-reference_v2_teams-verified-resources_get-list-of-verified-emails-of-a-team.md_chunk_11, Type: markdown)\n"
                ]
              },
              {
                "output_type": "stream",
                "name": "stdout",
                "text": [
                  "✅ Embedding generated for chunk cal.com_docs_api-reference_v2_teams-verified-resources_get-list-of-verified-emails-of-a-team.md_chunk_11\n",
                  "✅ Point prepared for chunk cal.com_docs_api-reference_v2_teams-verified-resources_get-list-of-verified-emails-of-a-team.md_chunk_11. Total points prepared: 12\n",
                  "✅ Tokenized and filtered chunk cal.com_docs_api-reference_v2_teams-verified-resources_get-list-of-verified-emails-of-a-team.md_chunk_11 for BM25.\n",
                  "Processing chunk 13/311 (ID: cal.com_docs_api-reference_v2_teams-verified-resources_get-list-of-verified-emails-of-a-team.md_chunk_12, Type: markdown)\n"
                ]
              },
              {
                "output_type": "stream",
                "name": "stdout",
                "text": [
                  "✅ Embedding generated for chunk cal.com_docs_api-reference_v2_teams-verified-resources_get-list-of-verified-emails-of-a-team.md_chunk_12\n",
                  "✅ Point prepared for chunk cal.com_docs_api-reference_v2_teams-verified-resources_get-list-of-verified-emails-of-a-team.md_chunk_12. Total points prepared: 13\n",
                  "✅ Tokenized and filtered chunk cal.com_docs_api-reference_v2_teams-verified-resources_get-list-of-verified-emails-of-a-team.md_chunk_12 for BM25.\n",
                  "Processing chunk 14/311 (ID: cal.com_docs_api-reference_v2_teams-verified-resources_get-list-of-verified-emails-of-a-team.md_chunk_13, Type: markdown)\n"
                ]
              },
              {
                "output_type": "stream",
                "name": "stdout",
                "text": [
                  "✅ Embedding generated for chunk cal.com_docs_api-reference_v2_teams-verified-resources_get-list-of-verified-emails-of-a-team.md_chunk_13\n",
                  "✅ Point prepared for chunk cal.com_docs_api-reference_v2_teams-verified-resources_get-list-of-verified-emails-of-a-team.md_chunk_13. Total points prepared: 14\n",
                  "✅ Tokenized and filtered chunk cal.com_docs_api-reference_v2_teams-verified-resources_get-list-of-verified-emails-of-a-team.md_chunk_13 for BM25.\n",
                  "Processing chunk 15/311 (ID: cal.com_docs_api-reference_v2_teams-verified-resources_get-list-of-verified-emails-of-a-team.md_chunk_14, Type: markdown)\n"
                ]
              },
              {
                "output_type": "stream",
                "name": "stdout",
                "text": [
                  "✅ Embedding generated for chunk cal.com_docs_api-reference_v2_teams-verified-resources_get-list-of-verified-emails-of-a-team.md_chunk_14\n",
                  "✅ Point prepared for chunk cal.com_docs_api-reference_v2_teams-verified-resources_get-list-of-verified-emails-of-a-team.md_chunk_14. Total points prepared: 15\n",
                  "✅ Tokenized and filtered chunk cal.com_docs_api-reference_v2_teams-verified-resources_get-list-of-verified-emails-of-a-team.md_chunk_14 for BM25.\n",
                  "Processing chunk 16/311 (ID: cal.com_docs_api-reference_v2_teams-verified-resources_get-list-of-verified-emails-of-a-team.md_chunk_15, Type: markdown)\n"
                ]
              },
              {
                "output_type": "stream",
                "name": "stdout",
                "text": [
                  "✅ Embedding generated for chunk cal.com_docs_api-reference_v2_teams-verified-resources_get-list-of-verified-emails-of-a-team.md_chunk_15\n",
                  "✅ Point prepared for chunk cal.com_docs_api-reference_v2_teams-verified-resources_get-list-of-verified-emails-of-a-team.md_chunk_15. Total points prepared: 16\n",
                  "✅ Tokenized and filtered chunk cal.com_docs_api-reference_v2_teams-verified-resources_get-list-of-verified-emails-of-a-team.md_chunk_15 for BM25.\n",
                  "Processing chunk 17/311 (ID: cal.com_docs_api-reference_v2_teams-verified-resources_get-list-of-verified-emails-of-a-team.md_chunk_16, Type: markdown)\n"
                ]
              },
              {
                "output_type": "stream",
                "name": "stdout",
                "text": [
                  "✅ Embedding generated for chunk cal.com_docs_api-reference_v2_teams-verified-resources_get-list-of-verified-emails-of-a-team.md_chunk_16\n",
                  "✅ Point prepared for chunk cal.com_docs_api-reference_v2_teams-verified-resources_get-list-of-verified-emails-of-a-team.md_chunk_16. Total points prepared: 17\n",
                  "✅ Tokenized and filtered chunk cal.com_docs_api-reference_v2_teams-verified-resources_get-list-of-verified-emails-of-a-team.md_chunk_16 for BM25.\n",
                  "Processing chunk 18/311 (ID: cal.com_docs_api-reference_v2_teams-verified-resources_get-list-of-verified-emails-of-a-team.md_chunk_17, Type: markdown)\n"
                ]
              },
              {
                "output_type": "stream",
                "name": "stdout",
                "text": [
                  "✅ Embedding generated for chunk cal.com_docs_api-reference_v2_teams-verified-resources_get-list-of-verified-emails-of-a-team.md_chunk_17\n",
                  "✅ Point prepared for chunk cal.com_docs_api-reference_v2_teams-verified-resources_get-list-of-verified-emails-of-a-team.md_chunk_17. Total points prepared: 18\n",
                  "✅ Tokenized and filtered chunk cal.com_docs_api-reference_v2_teams-verified-resources_get-list-of-verified-emails-of-a-team.md_chunk_17 for BM25.\n",
                  "Processing chunk 19/311 (ID: cal.com_docs_api-reference_v2_teams-verified-resources_get-list-of-verified-emails-of-a-team.md_chunk_18, Type: markdown)\n"
                ]
              },
              {
                "output_type": "stream",
                "name": "stdout",
                "text": [
                  "✅ Embedding generated for chunk cal.com_docs_api-reference_v2_teams-verified-resources_get-list-of-verified-emails-of-a-team.md_chunk_18\n",
                  "✅ Point prepared for chunk cal.com_docs_api-reference_v2_teams-verified-resources_get-list-of-verified-emails-of-a-team.md_chunk_18. Total points prepared: 19\n",
                  "✅ Tokenized and filtered chunk cal.com_docs_api-reference_v2_teams-verified-resources_get-list-of-verified-emails-of-a-team.md_chunk_18 for BM25.\n",
                  "Processing chunk 20/311 (ID: cal.com_docs_api-reference_v2_teams-verified-resources_get-list-of-verified-emails-of-a-team.md_chunk_19, Type: markdown)\n"
                ]
              },
              {
                "output_type": "stream",
                "name": "stdout",
                "text": [
                  "✅ Embedding generated for chunk cal.com_docs_api-reference_v2_teams-verified-resources_get-list-of-verified-emails-of-a-team.md_chunk_19\n",
                  "✅ Point prepared for chunk cal.com_docs_api-reference_v2_teams-verified-resources_get-list-of-verified-emails-of-a-team.md_chunk_19. Total points prepared: 20\n",
                  "✅ Tokenized and filtered chunk cal.com_docs_api-reference_v2_teams-verified-resources_get-list-of-verified-emails-of-a-team.md_chunk_19 for BM25.\n",
                  "Processing chunk 21/311 (ID: cal.com_docs_api-reference_v2_teams-verified-resources_get-list-of-verified-emails-of-a-team.md_chunk_20, Type: markdown)\n"
                ]
              },
              {
                "output_type": "stream",
                "name": "stdout",
                "text": [
                  "✅ Embedding generated for chunk cal.com_docs_api-reference_v2_teams-verified-resources_get-list-of-verified-emails-of-a-team.md_chunk_20\n",
                  "✅ Point prepared for chunk cal.com_docs_api-reference_v2_teams-verified-resources_get-list-of-verified-emails-of-a-team.md_chunk_20. Total points prepared: 21\n",
                  "✅ Tokenized and filtered chunk cal.com_docs_api-reference_v2_teams-verified-resources_get-list-of-verified-emails-of-a-team.md_chunk_20 for BM25.\n",
                  "Processing chunk 22/311 (ID: cal.com_docs_api-reference_v2_teams-verified-resources_get-list-of-verified-emails-of-a-team.md_chunk_21, Type: markdown)\n"
                ]
              },
              {
                "output_type": "stream",
                "name": "stdout",
                "text": [
                  "✅ Embedding generated for chunk cal.com_docs_api-reference_v2_teams-verified-resources_get-list-of-verified-emails-of-a-team.md_chunk_21\n",
                  "✅ Point prepared for chunk cal.com_docs_api-reference_v2_teams-verified-resources_get-list-of-verified-emails-of-a-team.md_chunk_21. Total points prepared: 22\n",
                  "✅ Tokenized and filtered chunk cal.com_docs_api-reference_v2_teams-verified-resources_get-list-of-verified-emails-of-a-team.md_chunk_21 for BM25.\n",
                  "Processing chunk 23/311 (ID: cal.com_docs_api-reference_v2_teams-verified-resources_get-list-of-verified-emails-of-a-team.md_chunk_22, Type: markdown)\n"
                ]
              },
              {
                "output_type": "stream",
                "name": "stdout",
                "text": [
                  "✅ Embedding generated for chunk cal.com_docs_api-reference_v2_teams-verified-resources_get-list-of-verified-emails-of-a-team.md_chunk_22\n",
                  "✅ Point prepared for chunk cal.com_docs_api-reference_v2_teams-verified-resources_get-list-of-verified-emails-of-a-team.md_chunk_22. Total points prepared: 23\n",
                  "✅ Tokenized and filtered chunk cal.com_docs_api-reference_v2_teams-verified-resources_get-list-of-verified-emails-of-a-team.md_chunk_22 for BM25.\n",
                  "Processing chunk 24/311 (ID: cal.com_docs_api-reference_v2_teams-verified-resources_get-list-of-verified-emails-of-a-team.md_chunk_23, Type: markdown)\n"
                ]
              }
            ]
          }
        },
        "6948556847e94fb59fca35c2d79d88c2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "e86f5398733d424097c5bfd13dfac39d",
            "0b5a39a6318a450d9070da8d20c8382a",
            "bde6e399975342ddb8997450b87be5e6"
          ]
        },
        "id": "1f05d9b7",
        "outputId": "a0dee08d-37db-4149-eca2-b25354303950"
      },
      "source": [
        "# Hybrid Multimodal Search with Gemini, Qdrant, and BM25\n",
        "# A comprehensive system for semantic and keyword-based document search\n",
        "\n",
        "# ==========================================\n",
        "# 1. SETUP AND AUTHENTICATION\n",
        "# ==========================================\n",
        "\n",
        "# Install required libraries\n",
        "!pip install qdrant-client google-generativeai rank-bm25 PyMuPDF Pillow numpy pandas sentence-transformers\n",
        "\n",
        "import time\n",
        "from typing import List\n",
        "import os\n",
        "import io\n",
        "import base64\n",
        "import json\n",
        "from typing import List, Dict, Any, Optional, Tuple\n",
        "from dataclasses import dataclass\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "\n",
        "# Core libraries\n",
        "import fitz  # PyMuPDF\n",
        "import google.generativeai as genai\n",
        "from qdrant_client import QdrantClient\n",
        "from qdrant_client.models import Distance, VectorParams, PointStruct, Filter, FieldCondition, MatchValue\n",
        "from rank_bm25 import BM25Okapi\n",
        "\n",
        "# NLTK for stop word removal\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt_tab')\n",
        "# Load English stop words\n",
        "stop_words = set(stopwords.words('english'))\n",
        "# Google Colab specific\n",
        "from google.colab import files, userdata\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display, HTML, Image as IPImage\n",
        "\n",
        "print(\"✅ All libraries installed successfully!\")\n",
        "\n",
        "# ==========================================\n",
        "# AUTHENTICATION SETUP\n",
        "# ==========================================\n",
        "\n",
        "# Load API key from Colab secrets\n",
        "try:\n",
        "    GEMINI_API_KEY = userdata.get('GEMINI_API_KEY')\n",
        "    genai.configure(api_key=GEMINI_API_KEY)\n",
        "    print(\"✅ Gemini API configured successfully!\")\n",
        "except Exception as e:\n",
        "    print(\"❌ Please add your GEMINI_API_KEY to Colab secrets\")\n",
        "    print(\"Go to the key icon on the left sidebar and add GEMINI_API_KEY\")\n",
        "    raise e\n",
        "\n",
        "# Function to create Qdrant client\n",
        "def create_qdrant_client():\n",
        "    \"\"\"Initializes and returns a Qdrant client.\"\"\"\n",
        "    try:\n",
        "        # Initialize Qdrant client (using in-memory for demo - replace with your instance)\n",
        "        # qdrant_client = QdrantClient(\":memory.\")  # For production: QdrantClient\n",
        "        qdrant_url = userdata.get('QDRANT_CLOUD_URL')\n",
        "        QDRANT_API_KEY = userdata.get('QDRANT_API')\n",
        "        qdrant_client = QdrantClient(url=qdrant_url, api_key=QDRANT_API_KEY)\n",
        "        print(\"✅ Qdrant client initialized successfully!\")\n",
        "        return qdrant_client\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Error initializing Qdrant client: {e}\")\n",
        "        return None\n",
        "\n",
        "\n",
        "# Initialize NLTK stop words\n",
        "try:\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    print(\"✅ NLTK stop words loaded successfully!\")\n",
        "except LookupError:\n",
        "    print(\"❌ NLTK data not found. Please run the cell to download 'punkt' and 'stopwords'.\")\n",
        "    stop_words = set() # Initialize as empty set to avoid errors later\n",
        "# import json\n",
        "# import os\n",
        "# from qdrant_client import QdrantClient\n",
        "# from qdrant_client.models import Distance, VectorParams, PointStruct, Filter, FieldCondition, MatchValue\n",
        "# from typing import List, Dict, Any, Optional, Tuple\n",
        "# from dataclasses import dataclass\n",
        "# import numpy as np\n",
        "# from rank_bm25 import BM25Okapi\n",
        "# import nltk\n",
        "# from nltk.corpus import stopwords\n",
        "# from nltk.tokenize import word_tokenize\n",
        "# import io\n",
        "# from PIL import Image\n",
        "# import fitz\n",
        "# from IPython.display import display, Image as IPImage\n",
        "\n",
        "# Assuming DocumentChunk is defined elsewhere and accessible\n",
        "@dataclass\n",
        "class DocumentChunk:\n",
        "    id: str\n",
        "    text: str\n",
        "    page_number: int\n",
        "    source_type: str\n",
        "    image_description: Optional[str] = None\n",
        "    image_data: Optional[bytes] = None\n",
        "    metadata: Optional[Dict] = None\n",
        "\n",
        "# Assuming SearchResult is defined elsewhere and accessible\n",
        "@dataclass\n",
        "class SearchResult:\n",
        "    chunk: DocumentChunk\n",
        "    score: float\n",
        "    rank: int\n",
        "    search_type: str  # 'semantic', 'keyword', or 'hybrid'\n",
        "\n",
        "# Assuming CONFIG is defined elsewhere and accessible\n",
        "CONFIG = {\n",
        "    \"collection_name\": \"calcom_help_docs\", # \"hybrid_search_collection_test_bm25\",\n",
        "    \"vector_dimension\": 768,  # gemini-embedding-001 dimension\n",
        "    \"chunk_size\": 1000,\n",
        "    \"chunk_overlap\": 200,\n",
        "    \"top_k_results\": 5, # Changed to 5\n",
        "    \"rrf_k\": 60  # RRF parameter\n",
        "}\n",
        "\n",
        "# Assuming stop_words is defined elsewhere and accessible\n",
        "try:\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "except LookupError:\n",
        "    print(\"❌ NLTK data not found. Please run the cell to download 'punkt' and 'stopwords'.\")\n",
        "    stop_words = set()\n",
        "\n",
        "# Assuming get_gemini_embedding is defined elsewhere and accessible\n",
        "def get_gemini_embedding_with_retry(text: str, task_type: str = \"RETRIEVAL_DOCUMENT\") -> List[float]:\n",
        "    \"\"\"Get embedding from Gemini API with retry logic for rate-limiting.\"\"\"\n",
        "    max_retries = 5\n",
        "    delay = 1 # Initial delay in seconds\n",
        "\n",
        "    for i in range(max_retries):\n",
        "        try:\n",
        "            result = genai.embed_content(\n",
        "                model=\"models/embedding-001\",\n",
        "                content=text,\n",
        "                task_type=task_type\n",
        "            )\n",
        "            return result['embedding']\n",
        "        except Exception as e:\n",
        "            # Convert the exception to a string for case-insensitive checking\n",
        "            error_message = str(e).lower()\n",
        "\n",
        "            # Check for specific rate-limiting or quota errors\n",
        "            if \"quota\" in error_message or \"too many requests\" in error_message:\n",
        "                print(f\"Rate limited. Retrying in {delay} seconds... (Attempt {i+1}/{max_retries})\")\n",
        "                time.sleep(delay)\n",
        "                delay *= 2  # Exponential backoff\n",
        "            else:\n",
        "                # For other errors, print the error and exit the loop\n",
        "                print(f\"Error getting embedding: {e}\")\n",
        "                return None\n",
        "\n",
        "    # If all retries fail\n",
        "    print(f\"Failed to get embedding after {max_retries} attempts.\")\n",
        "    return None\n",
        "\n",
        "# Assuming reciprocal_rank_fusion is defined elsewhere and accessible\n",
        "def reciprocal_rank_fusion(semantic_results: List[Tuple], keyword_results: List[Tuple], k: int = 60) -> List[Tuple]:\n",
        "    \"\"\"Combine semantic and keyword search results using Reciprocal Rank Fusion.\"\"\"\n",
        "    # Create score dictionaries\n",
        "    semantic_scores = {item[0]: 1.0 / (k + rank + 1) for rank, item in enumerate(semantic_results)}\n",
        "    keyword_scores = {item[0]: 1.0 / (k + rank + 1) for rank, item in enumerate(keyword_results)}\n",
        "\n",
        "    # Combine scores\n",
        "    all_ids = set(semantic_scores.keys()) | set(keyword_scores.keys())\n",
        "    fused_scores = {}\n",
        "\n",
        "    for doc_id in all_ids:\n",
        "        fused_scores[doc_id] = semantic_scores.get(doc_id, 0) + keyword_scores.get(doc_id, 0)\n",
        "\n",
        "    # Sort by fused score\n",
        "    sorted_results = sorted(fused_scores.items(), key=lambda x: x[1], reverse=True)\n",
        "    return sorted_results\n",
        "\n",
        "# Assuming extract_and_chunk_text is defined elsewhere and accessible\n",
        "def extract_and_chunk_text(text: str, chunk_size: int = 1000, overlap: int = 200) -> List[str]:\n",
        "    \"\"\"Split text into overlapping chunks.\"\"\"\n",
        "    if len(text) <= chunk_size:\n",
        "        return [text]\n",
        "\n",
        "    chunks = []\n",
        "    start = 0\n",
        "    while start < len(text):\n",
        "        end = start + chunk_size\n",
        "        chunk = text[start:end]\n",
        "\n",
        "        # Try to break at sentence boundary\n",
        "        if end < len(text):\n",
        "            last_period = chunk.rfind('.')\n",
        "            last_newline = chunk.rfind('\\n')\n",
        "            break_point = max(last_period, last_newline)\n",
        "            if break_point > start + chunk_size // 2:\n",
        "                chunk = text[start:break_point + 1]\n",
        "                end = break_point + 1\n",
        "\n",
        "        chunks.append(chunk.strip())\n",
        "        start = end - overlap\n",
        "\n",
        "        if start >= len(text):\n",
        "            break\n",
        "\n",
        "        if start >= len(text):\n",
        "            break\n",
        "\n",
        "\n",
        "    return chunks\n",
        "\n",
        "\n",
        "class DocumentIngester:\n",
        "    def __init__(self, qdrant_client: QdrantClient):\n",
        "        self.qdrant_client = qdrant_client\n",
        "        self.chunks = []\n",
        "        self.bm25_corpus = []\n",
        "        self.bm25_index = None\n",
        "        self.bm25_corpus_file = \"bm25_calcom_corpus.json\"\n",
        "\n",
        "        # Delete collection if it exists (keep for fresh start in demo)\n",
        "        # try:\n",
        "        #     self.qdrant_client.delete_collection(collection_name=CONFIG[\"collection_name\"])\n",
        "        #     print(f\"✅ Deleted existing collection: {CONFIG['collection_name']}\")\n",
        "        # except Exception as e:\n",
        "        #     print(f\"Collection might not exist or error deleting: {e}\")\n",
        "\n",
        "\n",
        "        # Create collection if it doesn't exist\n",
        "        try:\n",
        "            self.qdrant_client.get_collection(collection_name=CONFIG[\"collection_name\"])\n",
        "            print(f\"✅ Collection '{CONFIG['collection_name']}' already exists.\")\n",
        "        except Exception: # Collection does not exist\n",
        "            try:\n",
        "                self.qdrant_client.create_collection(\n",
        "                    collection_name=CONFIG[\"collection_name\"],\n",
        "                    vectors_config=VectorParams(\n",
        "                        size=CONFIG[\"vector_dimension\"],\n",
        "                        distance=Distance.COSINE\n",
        "                    )\n",
        "                )\n",
        "                print(f\"✅ Created collection: {CONFIG['collection_name']}\")\n",
        "            except Exception as e:\n",
        "                print(f\"❌ Error creating collection: {e}\")\n",
        "\n",
        "        # Attempt to load BM25 corpus\n",
        "        self.load_bm25_corpus(self.bm25_corpus_file)\n",
        "\n",
        "        # Rebuild BM25 index if corpus was loaded\n",
        "        if self.bm25_corpus:\n",
        "            try:\n",
        "                self.bm25_index = BM25Okapi(self.bm25_corpus)\n",
        "                print(\"✅ BM25 index rebuilt from loaded corpus.\")\n",
        "            except Exception as e:\n",
        "                print(f\"❌ Error rebuilding BM25 index from loaded corpus: {e}\")\n",
        "                self.bm25_index = None\n",
        "\n",
        "    def describe_image_with_gemini(self, image_bytes: bytes) -> str:\n",
        "        \"\"\"Generate image description using Gemini Vision.\"\"\"\n",
        "        try:\n",
        "            # Convert bytes to PIL Image\n",
        "            image = Image.open(io.BytesIO(image_bytes))\n",
        "\n",
        "            # Initialize Gemini Vision model\n",
        "            # Ensure you have initialized genai with your API key before calling this function\n",
        "            model = genai.GenerativeModel('gemini-1.5-flash')\n",
        "\n",
        "            # prompt = \"\"\"Describe this image in detail. Focus on:\n",
        "            # 1. Main objects, people, or elements, or technical diagrams\n",
        "            # 2. Text content (if any)\n",
        "            # 3. Charts, diagrams, or technical content\n",
        "            # 4. Spatial relationships and layout\n",
        "            # 5. Any relevant contextual information\n",
        "\n",
        "            # Provide a comprehensive description that would help in document search.\"\"\"\n",
        "            prompt = \"\"\"\"Describe the technical diagrams and tables found in the provided document. For each description, follow these steps:\n",
        "\n",
        "            Purpose: Begin by stating the primary function or purpose of the diagram or table (e.g., 'This is a wiring diagram showing the electrical connections,' or 'This table provides the technical specifications for the product').\n",
        "\n",
        "            Components: Provide a detailed breakdown of the visual elements.\n",
        "\n",
        "            For diagrams: List and explain each labeled component, symbol, or annotation. Describe the relationships or processes shown by arrows or other visual cues.\n",
        "\n",
        "            For tables: Identify and explain the meaning of each column and row. Highlight the key data points, including any values and units of measurement.\n",
        "\n",
        "            Synthesis: Conclude with a summary of the most important information presented, explaining how the different parts of the diagram or table work together to convey a complete message.\"\"\"\n",
        "\n",
        "            response = model.generate_content([prompt, image])\n",
        "            return response.text\n",
        "        except Exception as e:\n",
        "            print(f\"Error describing image: {e}\")\n",
        "            return \"Image description unavailable\"\n",
        "\n",
        "    def extract_pdf_content(self, pdf_path: str) -> List[DocumentChunk]:\n",
        "        \"\"\"Extract text and images from PDF.\"\"\"\n",
        "        doc = fitz.open(pdf_path)\n",
        "        all_chunks = []\n",
        "\n",
        "        print(f\"📄 Processing PDF with {min(len(doc), 10)} pages (limited to first 10 for trial)...\") # Updated print statement\n",
        "\n",
        "        # Process only the first 10 pages\n",
        "        for page_num in range(len(doc)):\n",
        "            page = doc.load_page(page_num)\n",
        "            print(f\"Processing page {page_num + 1}...\")\n",
        "\n",
        "            # Extract text\n",
        "            text = page.get_text()\n",
        "            if text.strip():\n",
        "                print(f\"Extracted text from page {page_num + 1}. Text length: {len(text)}\")\n",
        "                # Chunk the text\n",
        "                text_chunks = extract_and_chunk_text(text, CONFIG[\"chunk_size\"], CONFIG[\"chunk_overlap\"])\n",
        "                print(f\"Split into {len(text_chunks)} text chunks on page {page_num + 1}\")\n",
        "                print(f\"Debug: text_chunks content for page {page_num + 1}: {text_chunks}, Type: {type(text_chunks)}\") # Added debug log\n",
        "\n",
        "\n",
        "                if text_chunks: # Add check here\n",
        "                    for i, chunk_text in enumerate(text_chunks):\n",
        "                        print(f\"Debug: Processing text chunk {i} on page {page_num + 1}: {chunk_text[:50]}...\") # Added debug log\n",
        "                        chunk_id = f\"page_{page_num}_chunk_{i}\"\n",
        "                        chunk = DocumentChunk(\n",
        "                            id=chunk_id,\n",
        "                            text=chunk_text,\n",
        "                            page_number=page_num,\n",
        "                            source_type=\"document\",\n",
        "                            metadata={\"pdf_path\": pdf_path}\n",
        "                        )\n",
        "                        all_chunks.append(chunk)\n",
        "                else:\n",
        "                    print(f\"⚠️  No text chunks generated for page {page_num + 1}\") # Added log\n",
        "\n",
        "\n",
        "            # Extract images\n",
        "            image_list = page.get_images()\n",
        "            print(f\"Found {len(image_list)} images on page {page_num + 1}\")\n",
        "            for img_index, img in enumerate(image_list):\n",
        "                try:\n",
        "                    # Get image data\n",
        "                    xref = img[0]\n",
        "                    pix = fitz.Pixmap(doc, xref)\n",
        "\n",
        "                    # Convert pixmap to RGB if it's not already in a compatible format\n",
        "                    if pix.n > 3: # CMYK or other formats\n",
        "                         pix = fitz.Pixmap(fitz.csRGB, pix)\n",
        "                    elif pix.n == 1: # Grayscale\n",
        "                         pix = fitz.Pixmap(fitz.csRGB, pix) # Convert grayscale to RGB\n",
        "\n",
        "\n",
        "                    img_data = pix.tobytes(\"png\")\n",
        "\n",
        "                    # Generate description\n",
        "                    print(f\"Describing image {img_index} on page {page_num + 1}...\")\n",
        "                    description = self.describe_image_with_gemini(img_data) # Use self.describe_image_with_gemini\n",
        "                    print(f\"Image description generated for page {page_num + 1}: {description[:50]}...\")\n",
        "\n",
        "                    # Create image chunk\n",
        "                    chunk_id = f\"page_{page_num}_image_{img_index}\"\n",
        "                    chunk = DocumentChunk(\n",
        "                        id=chunk_id,\n",
        "                        text=description,\n",
        "                        page_number=page_num,\n",
        "                        source_type=\"image\",\n",
        "                        image_description=description,\n",
        "                        image_data=img_data,\n",
        "                        metadata={\"pdf_path\": pdf_path}\n",
        "                    )\n",
        "                    all_chunks.append(chunk)\n",
        "                    print(f\"🖼️  Processed image {img_index} on page {page_num + 1}\")\n",
        "\n",
        "                    pix = None\n",
        "                except Exception as e:\n",
        "                    print(f\"Error processing image {img_index} on page {page_num}: {e}\")\n",
        "\n",
        "        doc.close()\n",
        "        print(f\"✅ Extracted {len(all_chunks)} chunks from PDF\")\n",
        "        return all_chunks\n",
        "\n",
        "\n",
        "    def ingest_chunks(self, chunks: List[DocumentChunk]):\n",
        "        \"\"\"Ingest chunks into vector database and BM25 index.\"\"\"\n",
        "        print(\"🔄 Starting ingestion process...\")\n",
        "        print(f\"Attempting to ingest {len(chunks)} chunks.\")\n",
        "\n",
        "        points = []\n",
        "        newly_added_corpus = [] # Use a temporary list for current ingestion's corpus\n",
        "\n",
        "        for i, chunk in enumerate(chunks):\n",
        "            print(f\"Processing chunk {i+1}/{len(chunks)} (ID: {chunk.id}, Type: {chunk.source_type})\")\n",
        "            # Get embedding\n",
        "            embedding = get_gemini_embedding_with_retry(chunk.text, \"RETRIEVAL_DOCUMENT\")\n",
        "            if embedding is None:\n",
        "                print(f\"⚠️  Skipping chunk {chunk.id} - failed to get embedding\")\n",
        "                continue\n",
        "            print(f\"✅ Embedding generated for chunk {chunk.id}\")\n",
        "\n",
        "            # Prepare point for Qdrant\n",
        "            point = PointStruct(\n",
        "                id=len(self.chunks) + len(points), # Ensure unique IDs across multiple ingestions\n",
        "                vector=embedding,\n",
        "                payload={\n",
        "                    \"chunk_id\": chunk.id,\n",
        "                    \"text\": chunk.text,\n",
        "                    \"page_number\": chunk.page_number,\n",
        "                    \"source_type\": chunk.source_type,\n",
        "                    \"image_description\": chunk.image_description,\n",
        "                    \"has_image\": chunk.image_data is not None,\n",
        "                    \"metadata\": chunk.metadata or {}\n",
        "                }\n",
        "            )\n",
        "            points.append(point)\n",
        "            print(f\"✅ Point prepared for chunk {chunk.id}. Total points prepared: {len(points)}\")\n",
        "\n",
        "\n",
        "            # Prepare for BM25 using NLTK and stop words\n",
        "            try:\n",
        "                tokens = word_tokenize(chunk.text.lower())\n",
        "                filtered_tokens = [word for word in tokens if word.isalnum() and word not in stop_words]\n",
        "                newly_added_corpus.append(filtered_tokens)\n",
        "                print(f\"✅ Tokenized and filtered chunk {chunk.id} for BM25.\")\n",
        "            except Exception as e:\n",
        "                print(f\"❌ Error tokenizing/filtering chunk {chunk.id} for BM25: {e}\")\n",
        "                newly_added_corpus.append([]) # Add empty list to maintain corpus length\n",
        "\n",
        "\n",
        "            # Store chunk reference\n",
        "            self.chunks.extend(chunks) # Extend chunks with the new chunks\n",
        "            print(f\"Debug: self.chunks length after adding current chunks: {len(self.chunks)}\")\n",
        "\n",
        "\n",
        "        print(f\"Prepared {len(points)} points for upserting to Qdrant.\")\n",
        "        if points:\n",
        "            # Upsert to Qdrant\n",
        "            try:\n",
        "                response = self.qdrant_client.upsert(\n",
        "                    collection_name=CONFIG[\"collection_name\"],\n",
        "                    points=points,\n",
        "                    wait=True # Wait for the operation to complete\n",
        "                )\n",
        "                print(f\"✅ Qdrant upsert response: {response}\")\n",
        "                print(f\"✅ Upserted {len(points)} points to Qdrant.\")\n",
        "            except Exception as e:\n",
        "                print(f\"❌ Error during Qdrant upsert: {e}\")\n",
        "\n",
        "        else:\n",
        "            print(\"⚠️ No points to upsert to Qdrant.\")\n",
        "\n",
        "        # Accumulate the new corpus\n",
        "        self.bm25_corpus.extend(newly_added_corpus)\n",
        "\n",
        "        print(f\"Debug: Corpus content after adding newly added corpus: {self.bm25_corpus}, Type: {type(self.bm25_corpus)}\") # Added debug log\n",
        "        # Build BM25 index from the accumulated corpus\n",
        "        if self.bm25_corpus and any(self.bm25_corpus): # Check if corpus is not empty and contains non-empty token lists\n",
        "            self.bm25_index = BM25Okapi(self.bm25_corpus)\n",
        "            print(f\"✅ BM25 index built with {len(self.bm25_corpus)} documents.\")\n",
        "\n",
        "            # Save BM25 corpus to file\n",
        "            self.save_bm25_corpus() # Call the new method\n",
        "\n",
        "        else:\n",
        "            self.bm25_index = None # Ensure index is None if corpus is empty\n",
        "            self.bm25_corpus = []\n",
        "            print(\"⚠️ No corpus for building BM25 index.\")\n",
        "\n",
        "\n",
        "        print(f\"✅ Successfully processed {len(points)} chunks for ingestion\")\n",
        "\n",
        "\n",
        "    def ingest_document(self, pdf_path: str):\n",
        "        \"\"\"Complete ingestion pipeline for a PDF document.\"\"\"\n",
        "        chunks = self.extract_pdf_content(pdf_path)\n",
        "        self.ingest_chunks(chunks)\n",
        "        self.save_bm25_corpus() # Save the corpus after processing each document\n",
        "\n",
        "    def load_bm25_corpus(self, file_path: str):\n",
        "        \"\"\"Loads the tokenized BM25 corpus from a JSON file.\"\"\"\n",
        "        if os.path.exists(file_path):\n",
        "            try:\n",
        "                with open(file_path, \"r\") as f:\n",
        "                    loaded_corpus = json.load(f)\n",
        "                    if isinstance(loaded_corpus, list):\n",
        "                        self.bm25_corpus = loaded_corpus\n",
        "                    else:\n",
        "                        print(f\"⚠️ Loaded BM25 corpus from {file_path} is not a list. Initializing with empty corpus.\")\n",
        "                        self.bm25_corpus = []\n",
        "                print(f\"✅ BM25 corpus loaded from {file_path}\")\n",
        "            except Exception as e:\n",
        "                print(f\"❌ Error loading BM25 corpus from {file_path}: {e}\")\n",
        "                self.bm25_corpus = []\n",
        "        else:\n",
        "            print(f\"⚠️ BM25 corpus file not found at {file_path}. BM25 index will be built during ingestion.\")\n",
        "            self.bm25_corpus = []\n",
        "\n",
        "\n",
        "    def save_bm25_corpus(self):\n",
        "        \"\"\"Saves the tokenized BM25 corpus to a JSON file.\"\"\"\n",
        "        try:\n",
        "            with open(self.bm25_corpus_file, \"w\") as f:\n",
        "                json.dump(self.bm25_corpus, f)\n",
        "            print(f\"✅ BM25 corpus saved to {self.bm25_corpus_file}\")\n",
        "        except Exception as e:\n",
        "            print(f\"❌ Error saving BM25 corpus: {e}\")\n",
        "\n",
        "def create_upload_interface():\n",
        "    \"\"\"Create file upload interface.\"\"\"\n",
        "    print(\"📁 Upload a PDF document to get started:\")\n",
        "\n",
        "    upload_button = widgets.FileUpload(\n",
        "        accept='.pdf',\n",
        "        multiple=False,\n",
        "        description='Choose PDF'\n",
        "    )\n",
        "\n",
        "    def on_upload_change(change):\n",
        "        if change['new']:\n",
        "            # Save uploaded file\n",
        "            filename = list(change['new'].keys())[0]\n",
        "            content = change['new'][filename]['content']\n",
        "\n",
        "            with open(filename, 'wb') as f:\n",
        "                f.write(content)\n",
        "\n",
        "            print(f\"✅ Uploaded: {filename}\")\n",
        "            return filename\n",
        "\n",
        "\n",
        "    upload_button.observe(on_upload_change, names='value')\n",
        "    display(upload_button)\n",
        "\n",
        "    return upload_button\n",
        "\n",
        "\n",
        "# ==========================================\n",
        "# 7. DEMONSTRATION WORKFLOW\n",
        "# ==========================================\n",
        "\n",
        "def run_demo():\n",
        "    \"\"\"Run the complete demonstration.\"\"\"\n",
        "    print(\"🚀 Starting Hybrid Multimodal Search Demo\")\n",
        "    print(\"=\" * 50)\n",
        "    qdrant_client = create_qdrant_client()\n",
        "    # Initialize system\n",
        "    ingester = DocumentIngester(qdrant_client)\n",
        "    # search_engine = HybridSearchEngine(qdrant_client, ingester)\n",
        "\n",
        "    # Create upload interface\n",
        "    upload_widget = create_upload_interface()\n",
        "\n",
        "    return ingester, upload_widget\n",
        "\n",
        "\n",
        "# ==========================================\n",
        "# 8. MAIN EXECUTION\n",
        "# ==========================================\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Run the demo\n",
        "    ingester, upload_widget = run_demo()\n",
        "\n",
        "    print(\"\"\"\n",
        "📋 INSTRUCTIONS:\n",
        "1. Upload a PDF using the file picker above\n",
        "2. Wait for processing to complete\n",
        "3. Run searches using the search_engine.hybrid_search() function\n",
        "\n",
        "Example usage:\n",
        "search_engine.hybrid_search(\"your query here\")\n",
        "search_engine.display_results(results)\n",
        "\"\"\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: qdrant-client in /usr/local/lib/python3.11/dist-packages (1.15.1)\n",
            "Requirement already satisfied: google-generativeai in /usr/local/lib/python3.11/dist-packages (0.8.5)\n",
            "Requirement already satisfied: rank-bm25 in /usr/local/lib/python3.11/dist-packages (0.2.2)\n",
            "Requirement already satisfied: PyMuPDF in /usr/local/lib/python3.11/dist-packages (1.26.3)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (11.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.11/dist-packages (5.1.0)\n",
            "Requirement already satisfied: grpcio>=1.41.0 in /usr/local/lib/python3.11/dist-packages (from qdrant-client) (1.74.0)\n",
            "Requirement already satisfied: httpx>=0.20.0 in /usr/local/lib/python3.11/dist-packages (from httpx[http2]>=0.20.0->qdrant-client) (0.28.1)\n",
            "Requirement already satisfied: portalocker<4.0,>=2.7.0 in /usr/local/lib/python3.11/dist-packages (from qdrant-client) (3.2.0)\n",
            "Requirement already satisfied: protobuf>=3.20.0 in /usr/local/lib/python3.11/dist-packages (from qdrant-client) (5.29.5)\n",
            "Requirement already satisfied: pydantic!=2.0.*,!=2.1.*,!=2.2.0,>=1.10.8 in /usr/local/lib/python3.11/dist-packages (from qdrant-client) (2.11.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.26.14 in /usr/local/lib/python3.11/dist-packages (from qdrant-client) (2.5.0)\n",
            "Requirement already satisfied: google-ai-generativelanguage==0.6.15 in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (0.6.15)\n",
            "Requirement already satisfied: google-api-core in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (2.25.1)\n",
            "Requirement already satisfied: google-api-python-client in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (2.178.0)\n",
            "Requirement already satisfied: google-auth>=2.15.0 in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (2.38.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (4.14.1)\n",
            "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /usr/local/lib/python3.11/dist-packages (from google-ai-generativelanguage==0.6.15->google-generativeai) (1.26.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.55.0)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (2.6.0+cu124)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.16.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (0.34.4)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core->google-generativeai) (1.70.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.18.0 in /usr/local/lib/python3.11/dist-packages (from google-api-core->google-generativeai) (2.32.3)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth>=2.15.0->google-generativeai) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth>=2.15.0->google-generativeai) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth>=2.15.0->google-generativeai) (4.9.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx>=0.20.0->httpx[http2]>=0.20.0->qdrant-client) (4.10.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.20.0->httpx[http2]>=0.20.0->qdrant-client) (2025.8.3)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.20.0->httpx[http2]>=0.20.0->qdrant-client) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx>=0.20.0->httpx[http2]>=0.20.0->qdrant-client) (3.10)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.20.0->httpx[http2]>=0.20.0->qdrant-client) (0.16.0)\n",
            "Requirement already satisfied: h2<5,>=3 in /usr/local/lib/python3.11/dist-packages (from httpx[http2]>=0.20.0->qdrant-client) (4.2.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.3.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.2)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (1.1.7)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=2.0.*,!=2.1.*,!=2.2.0,>=1.10.8->qdrant-client) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=2.0.*,!=2.1.*,!=2.2.0,>=1.10.8->qdrant-client) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=2.0.*,!=2.1.*,!=2.2.0,>=1.10.8->qdrant-client) (0.4.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.21.4)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.6.2)\n",
            "Requirement already satisfied: httplib2<1.0.0,>=0.19.0 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client->google-generativeai) (0.22.0)\n",
            "Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client->google-generativeai) (0.2.0)\n",
            "Requirement already satisfied: uritemplate<5,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client->google-generativeai) (4.2.0)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
            "Requirement already satisfied: grpcio-status<2.0.0,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai) (1.71.2)\n",
            "Requirement already satisfied: hyperframe<7,>=6.1 in /usr/local/lib/python3.11/dist-packages (from h2<5,>=3->httpx[http2]>=0.20.0->qdrant-client) (6.1.0)\n",
            "Requirement already satisfied: hpack<5,>=4.1 in /usr/local/lib/python3.11/dist-packages (from h2<5,>=3->httpx[http2]>=0.20.0->qdrant-client) (4.1.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /usr/local/lib/python3.11/dist-packages (from httplib2<1.0.0,>=0.19.0->google-api-python-client->google-generativeai) (3.2.3)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=2.15.0->google-generativeai) (0.6.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (3.4.3)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx>=0.20.0->httpx[http2]>=0.20.0->qdrant-client) (1.3.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ All libraries installed successfully!\n",
            "✅ Gemini API configured successfully!\n",
            "✅ NLTK stop words loaded successfully!\n",
            "🚀 Starting Hybrid Multimodal Search Demo\n",
            "==================================================\n",
            "✅ Qdrant client initialized successfully!\n",
            "✅ Collection 'calcom_help_docs' already exists.\n",
            "⚠️ BM25 corpus file not found at bm25_calcom_corpus.json. BM25 index will be built during ingestion.\n",
            "📁 Upload a PDF document to get started:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "FileUpload(value={}, accept='.pdf', description='Choose PDF')"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e86f5398733d424097c5bfd13dfac39d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "📋 INSTRUCTIONS:\n",
            "1. Upload a PDF using the file picker above\n",
            "2. Wait for processing to complete\n",
            "3. Run searches using the search_engine.hybrid_search() function\n",
            "\n",
            "Example usage:\n",
            "search_engine.hybrid_search(\"your query here\")\n",
            "search_engine.display_results(results)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9cef2ba9"
      },
      "source": [
        "## Modify hybridsearchengine initialization\n",
        "\n",
        "### Subtask:\n",
        "Update the `HybridSearchEngine` class to load the BM25 corpus from the saved file during its initialization and build its own BM25 index. Remove the dependency on the `DocumentIngester` instance.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "01f040a8"
      },
      "source": [
        "**Reasoning**:\n",
        "Update the HybridSearchEngine class to load the BM25 corpus and build the index in its initialization, removing the dependency on DocumentIngester.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers.cross_encoder import CrossEncoder\n",
        "\n",
        "reranker_model = CrossEncoder('BAAI/bge-reranker-large')\n",
        "print(\"✅ Reranker model loaded successfully!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MaE-Nj-aVofs",
        "outputId": "84476692-2fce-4a9b-fa74-6aa530ffe950"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Reranker model loaded successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "668e6a4f"
      },
      "source": [
        "import json\n",
        "import os\n",
        "from qdrant_client import QdrantClient\n",
        "from qdrant_client.models import Distance, VectorParams, PointStruct, Filter, FieldCondition, MatchValue\n",
        "from typing import List, Dict, Any, Optional, Tuple\n",
        "from dataclasses import dataclass\n",
        "import numpy as np\n",
        "from rank_bm25 import BM25Okapi\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "import io\n",
        "from PIL import Image\n",
        "import fitz\n",
        "from IPython.display import display, Image as IPImage\n",
        "\n",
        "# Assuming DocumentChunk is defined elsewhere and accessible\n",
        "@dataclass\n",
        "class DocumentChunk:\n",
        "    id: str\n",
        "    text: str\n",
        "    page_number: int\n",
        "    source_type: str\n",
        "    image_description: Optional[str] = None\n",
        "    image_data: Optional[bytes] = None\n",
        "    metadata: Optional[Dict] = None\n",
        "\n",
        "# Assuming SearchResult is defined elsewhere and accessible\n",
        "@dataclass\n",
        "class SearchResult:\n",
        "    chunk: DocumentChunk\n",
        "    score: float\n",
        "    rank: int\n",
        "    search_type: str  # 'semantic', 'keyword', or 'hybrid'\n",
        "\n",
        "# Assuming CONFIG is defined elsewhere and accessible\n",
        "CONFIG = {\n",
        "    \"collection_name\": \"calcom_help_docs\",\n",
        "    \"vector_dimension\": 768,  # gemini-embedding-001 dimension\n",
        "    \"chunk_size\": 1000,\n",
        "    \"chunk_overlap\": 200,\n",
        "    \"top_k_results\": 5, # Changed to 5\n",
        "    \"rrf_k\": 60  # RRF parameter\n",
        "}\n",
        "\n",
        "# Assuming stop_words is defined elsewhere and accessible\n",
        "try:\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "except LookupError:\n",
        "    print(\"❌ NLTK data not found. Please run the cell to download 'punkt' and 'stopwords'.\")\n",
        "    stop_words = set()\n",
        "\n",
        "# Assuming get_gemini_embedding is defined elsewhere and accessible\n",
        "def get_gemini_embedding(text: str, task_type: str = \"RETRIEVAL_DOCUMENT\") -> List[float]:\n",
        "    \"\"\"Get embedding from Gemini API.\"\"\"\n",
        "    pass\n",
        "\n",
        "# Assuming describe_image_with_gemini is defined elsewhere and accessible\n",
        "def describe_image_with_gemini(image_bytes: bytes) -> str:\n",
        "    \"\"\"Generate image description using Gemini Vision.\"\"\"\n",
        "    pass\n",
        "\n",
        "# Assuming reciprocal_rank_fusion is defined elsewhere and accessible\n",
        "def reciprocal_rank_fusion(semantic_results: List[Tuple], keyword_results: List[Tuple], k: int = 60) -> List[Tuple]:\n",
        "    \"\"\"Combine semantic and keyword search results using Reciprocal Rank Fusion.\"\"\"\n",
        "    # Create score dictionaries\n",
        "    semantic_scores = {item[0]: 1.0 / (k + rank + 1) for rank, item in enumerate(semantic_results)}\n",
        "    keyword_scores = {item[0]: 1.0 / (k + rank + 1) for rank, item in enumerate(keyword_results)}\n",
        "\n",
        "    # Combine scores\n",
        "    all_ids = set(semantic_scores.keys()) | set(keyword_scores.keys())\n",
        "    fused_scores = {}\n",
        "\n",
        "    for doc_id in all_ids:\n",
        "        fused_scores[doc_id] = semantic_scores.get(doc_id, 0) + keyword_scores.get(doc_id, 0)\n",
        "\n",
        "    # Sort by fused score\n",
        "    sorted_results = sorted(fused_scores.items(), key=lambda x: x[1], reverse=True)\n",
        "    return sorted_results\n",
        "\n",
        "# Assuming extract_and_chunk_text is defined elsewhere and accessible\n",
        "def extract_and_chunk_text(text: str, chunk_size: int = 1000, overlap: int = 200) -> List[str]:\n",
        "    \"\"\"Split text into overlapping chunks.\"\"\"\n",
        "    if len(text) <= chunk_size:\n",
        "        return [text]\n",
        "\n",
        "    chunks = []\n",
        "    start = 0\n",
        "    while start < len(text):\n",
        "        end = start + chunk_size\n",
        "        chunk = text[start:end]\n",
        "\n",
        "        # Try to break at sentence boundary\n",
        "        if end < len(text):\n",
        "            last_period = chunk.rfind('.')\n",
        "            last_newline = chunk.rfind('\\n')\n",
        "            break_point = max(last_period, last_newline)\n",
        "            if break_point > start + chunk_size // 2:\n",
        "                chunk = text[start:break_point + 1]\n",
        "                end = break_point + 1\n",
        "\n",
        "        chunks.append(chunk.strip())\n",
        "        start = end - overlap\n",
        "\n",
        "        if start >= len(text):\n",
        "            break\n",
        "\n",
        "        if start >= len(text):\n",
        "            break\n",
        "\n",
        "\n",
        "    return chunks\n",
        "\n",
        "\n",
        "# Dummy DocumentIngester for necessary type hinting and potential future use if needed elsewhere\n",
        "class DocumentIngester:\n",
        "    def __init__(self, qdrant_client: QdrantClient):\n",
        "        self.qdrant_client = qdrant_client\n",
        "        self.chunks = []\n",
        "        self.bm25_corpus = []\n",
        "        self.bm25_index = None\n",
        "        self.bm25_corpus_file = \"bm25_calcom_corpus.json\"\n",
        "\n",
        "    def extract_pdf_content(self, pdf_path: str) -> List[DocumentChunk]:\n",
        "        pass\n",
        "\n",
        "    def ingest_chunks(self, chunks: List[DocumentChunk]):\n",
        "        pass\n",
        "\n",
        "    def ingest_document(self, pdf_path: str):\n",
        "        pass\n",
        "\n",
        "    def load_bm25_corpus(self, file_path: str):\n",
        "        pass\n",
        "\n",
        "    def save_bm25_corpus(self):\n",
        "        pass\n",
        "\n",
        "\n",
        "class HybridSearchEngine:\n",
        "    def __init__(self, qdrant_client: QdrantClient):\n",
        "        self.qdrant_client = qdrant_client\n",
        "        self.bm25_corpus = []\n",
        "        self.bm25_index = None\n",
        "        self.bm25_corpus_file = \"bm25_corpus.json\"\n",
        "        self.chunks = [] # Need chunks to map BM25 results back to DocumentChunk objects\n",
        "\n",
        "        # Load BM25 corpus and build index\n",
        "        self._load_bm25_corpus_and_build_index()\n",
        "\n",
        "        # Load chunks from Qdrant to map BM25 results back\n",
        "        self._load_chunks_from_qdrant()\n",
        "\n",
        "\n",
        "    def _load_bm25_corpus_and_build_index(self):\n",
        "        \"\"\"Loads the tokenized BM25 corpus from a JSON file and builds the index.\"\"\"\n",
        "        if os.path.exists(self.bm25_corpus_file):\n",
        "            try:\n",
        "                with open(self.bm25_corpus_file, \"r\") as f:\n",
        "                    self.bm25_corpus = json.load(f)\n",
        "                print(f\"✅ BM25 corpus loaded from {self.bm25_corpus_file}\")\n",
        "\n",
        "                if self.bm25_corpus and any(self.bm25_corpus):\n",
        "                    self.bm25_index = BM25Okapi(self.bm25_corpus)\n",
        "                    print(\"✅ BM25 index built from loaded corpus.\")\n",
        "                    print(f\"BM25 corpus size: {len(self.bm25_corpus)}\") # Add logging\n",
        "\n",
        "                else:\n",
        "                    self.bm25_index = None\n",
        "                    print(\"⚠️ Loaded BM25 corpus is empty or contains only empty lists. BM25 index not built.\")\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"❌ Error loading or building BM25 index from {self.bm25_corpus_file}: {e}\")\n",
        "                self.bm25_corpus = []\n",
        "                self.bm25_index = None\n",
        "        else:\n",
        "            print(f\"⚠️ BM25 corpus file not found at {self.bm25_corpus_file}. BM25 keyword search will not be available.\")\n",
        "            self.bm25_corpus = []\n",
        "            self.bm25_index = None\n",
        "\n",
        "    def _load_chunks_from_qdrant(self):\n",
        "        \"\"\"Loads chunk information from Qdrant to map BM25 results.\"\"\"\n",
        "        try:\n",
        "            # Retrieve all points from the collection\n",
        "            scroll_result = self.qdrant_client.scroll(\n",
        "                collection_name=CONFIG[\"collection_name\"],\n",
        "                limit=10000,  # Adjust limit based on expected number of chunks\n",
        "                with_payload=True,\n",
        "                with_vectors=False\n",
        "            )\n",
        "            self.chunks = []\n",
        "            for record in scroll_result[0]:\n",
        "                payload = record.payload\n",
        "                chunk = DocumentChunk(\n",
        "                    id=payload[\"chunk_id\"],\n",
        "                    text=payload[\"text\"],\n",
        "                    page_number=payload[\"page_number\"],\n",
        "                    source_type=payload[\"source_type\"],\n",
        "                    image_description=payload.get(\"image_description\"),\n",
        "                    image_data=None, # Image data is not stored in Qdrant payload in this example\n",
        "                    metadata=payload.get(\"metadata\", {})\n",
        "                )\n",
        "                self.chunks.append(chunk)\n",
        "            print(f\"✅ Loaded {len(self.chunks)} chunks from Qdrant.\")\n",
        "            print(f\"Number of chunks loaded from Qdrant: {len(self.chunks)}\") # Add logging\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"❌ Error loading chunks from Qdrant: {e}\")\n",
        "            self.chunks = []\n",
        "\n",
        "\n",
        "    def semantic_search(self, query: str, top_k: int = 10, source_type: Optional[str] = None) -> List[Tuple]:\n",
        "        \"\"\"Perform semantic search using Qdrant.\"\"\"\n",
        "        # Get query embedding\n",
        "        query_embedding = get_gemini_embedding(query, \"RETRIEVAL_QUERY\")\n",
        "        if query_embedding is None:\n",
        "            return []\n",
        "\n",
        "        # Prepare filter\n",
        "        filter_condition = None\n",
        "        if source_type:\n",
        "            filter_condition = Filter(\n",
        "                must=[FieldCondition(key=\"source_type\", match=MatchValue(value=source_type))]\n",
        "            )\n",
        "\n",
        "        # Search\n",
        "        print(f\"Performing semantic search for query: '{query}'\")\n",
        "        try:\n",
        "            results = self.qdrant_client.search(\n",
        "                collection_name=CONFIG[\"collection_name\"],\n",
        "                query_vector=query_embedding,\n",
        "                query_filter=filter_condition,\n",
        "                limit=top_k\n",
        "            )\n",
        "            print(f\"Semantic search returned {len(results)} results.\")\n",
        "            return [(result.payload[\"chunk_id\"], result.score) for result in results]\n",
        "        except Exception as e:\n",
        "            print(f\"❌ Error during semantic search: {e}\")\n",
        "            return []\n",
        "\n",
        "\n",
        "    def keyword_search(self, query: str, top_k: int = 10) -> List[Tuple]:\n",
        "        \"\"\"Perform keyword search using BM25.\"\"\"\n",
        "        if self.bm25_index is None or not self.chunks:\n",
        "            print(\"⚠️ BM25 index or chunks not available for keyword search.\")\n",
        "            return []\n",
        "\n",
        "        print(f\"Performing keyword search for query: '{query}'\")\n",
        "        # Use NLTK for tokenization and stop word removal for the query as well\n",
        "        query_tokens = word_tokenize(query.lower())\n",
        "        filtered_query_tokens = [word for word in query_tokens if word.isalnum() and word not in stop_words]\n",
        "\n",
        "        if not filtered_query_tokens:\n",
        "            print(\"⚠️ No valid tokens for keyword search.\")\n",
        "            return []\n",
        "\n",
        "        scores = self.bm25_index.get_scores(filtered_query_tokens)\n",
        "\n",
        "        # Get top results and map back to chunk IDs\n",
        "        top_indices = np.argsort(scores)[::-1][:top_k]\n",
        "        results = []\n",
        "        for i in top_indices:\n",
        "            if i < len(self.chunks): # Add bounds check\n",
        "                if scores[i] > 0:\n",
        "                    results.append((self.chunks[i].id, scores[i]))\n",
        "            else:\n",
        "                 print(f\"Warning: Index {i} out of bounds for self.chunks ({len(self.chunks)}). Skipping.\")\n",
        "\n",
        "        print(f\"Keyword search returned {len(results)} results.\")\n",
        "\n",
        "        return results\n",
        "\n",
        "\n",
        "    def hybrid_search(self, query: str, top_k: int = 10, source_type: Optional[str] = None) -> List[SearchResult]:\n",
        "        \"\"\"Perform hybrid search combining semantic and keyword search.\"\"\"\n",
        "        print(f\"🔍 Searching for: '{query}'\")\n",
        "\n",
        "        # Perform both searches\n",
        "        semantic_results = self.semantic_search(query, top_k * 2, source_type) # Fetch more results for RRF\n",
        "        keyword_results = self.keyword_search(query, top_k * 2) # Fetch more results for RRF\n",
        "\n",
        "        print(f\"📊 Semantic results: {len(semantic_results)}, Keyword results: {len(keyword_results)}\")\n",
        "\n",
        "        # Apply RRF\n",
        "        print(\"Applying Reciprocal Rank Fusion (RRF)...\")\n",
        "        fused_results = reciprocal_rank_fusion(semantic_results, keyword_results, CONFIG[\"rrf_k\"])\n",
        "        print(f\"RRF produced {len(fused_results)} fused results.\")\n",
        "\n",
        "        # Prepare for Re-ranking\n",
        "        print(\"Preparing for Re-ranking...\")\n",
        "        rerank_pairs = []\n",
        "        # Create a dictionary to quickly look up chunk by ID\n",
        "        chunk_dict = {chunk.id: chunk for chunk in self.chunks}\n",
        "\n",
        "        # Limit the number of results sent to the reranker to a reasonable amount\n",
        "        rerank_limit = min(len(fused_results), top_k * 3) # Rerank a bit more than final top_k\n",
        "        print(f\"Sending top {rerank_limit} fused results to reranker.\")\n",
        "\n",
        "        rerank_chunk_ids = [item[0] for item in fused_results[:rerank_limit]]\n",
        "        for chunk_id in rerank_chunk_ids:\n",
        "            if chunk_id in chunk_dict:\n",
        "                chunk_text = chunk_dict[chunk_id].text\n",
        "                rerank_pairs.append([query, chunk_text])\n",
        "            else:\n",
        "                 print(f\"Warning: Chunk ID {chunk_id} not found in chunk_dict for reranking.\")\n",
        "\n",
        "\n",
        "        # Perform Re-ranking\n",
        "        print(f\"Performing Re-ranking using BAAI/bge-reranker-large on {len(rerank_pairs)} pairs...\")\n",
        "        if rerank_pairs:\n",
        "            # Ensure reranker_model is accessible (assuming it's loaded in a previous cell)\n",
        "            try:\n",
        "                rerank_scores = reranker_model.predict(rerank_pairs)\n",
        "                print(\"✅ Re-ranking completed.\")\n",
        "            except NameError:\n",
        "                print(\"❌ Reranker model not found. Please ensure 'reranker_model' is loaded.\")\n",
        "                rerank_scores = [0.0] * len(rerank_pairs) # Assign dummy scores if model not found\n",
        "                # Use RRF scores if reranker fails\n",
        "                reranked_results_with_scores = fused_results[:top_k]\n",
        "                print(\"Using RRF scores due to reranker error.\")\n",
        "                # Convert to SearchResult objects using the RRF order and scores\n",
        "                search_results = []\n",
        "                print(f\"Converting RRF results to SearchResult objects (Top {top_k})...\")\n",
        "                for rank, (chunk_id, score) in enumerate(reranked_results_with_scores[:top_k]):\n",
        "                     if chunk_id in chunk_dict:\n",
        "                        result = SearchResult(\n",
        "                            chunk=chunk_dict[chunk_id],\n",
        "                            score=score, # Use the RRF score\n",
        "                            rank=rank + 1,\n",
        "                            search_type=\"hybrid_rrf\" # Indicate RRF results\n",
        "                        )\n",
        "                        search_results.append(result)\n",
        "                        print(f\"Added RRF result for chunk ID: {chunk_id}\")\n",
        "                     else:\n",
        "                        print(f\"Warning: Chunk ID {chunk_id} not found in chunk_dict after RRF.\")\n",
        "\n",
        "                print(f\"✅ Hybrid search completed. Returning {len(search_results)} results.\")\n",
        "                return search_results\n",
        "\n",
        "\n",
        "            # Associate rerank scores with chunk IDs\n",
        "            reranked_results_with_scores = sorted(zip(rerank_chunk_ids, rerank_scores), key=lambda x: x[1], reverse=True)\n",
        "            print(f\"Re-ranked {len(reranked_results_with_scores)} results.\")\n",
        "\n",
        "            # Convert to SearchResult objects using the re-ranked order and scores\n",
        "            search_results = []\n",
        "            print(f\"Converting re-ranked results to SearchResult objects (Top {top_k}...):\")\n",
        "            for rank, (chunk_id, score) in enumerate(reranked_results_with_scores[:top_k]):\n",
        "                 if chunk_id in chunk_dict:\n",
        "                    result = SearchResult(\n",
        "                        chunk=chunk_dict[chunk_id],\n",
        "                        score=score, # Use the rerank score\n",
        "                        rank=rank + 1,\n",
        "                        search_type=\"hybrid_reranked\" # Indicate re-ranked\n",
        "                    )\n",
        "                    search_results.append(result)\n",
        "                    print(f\"Added re-ranked result for chunk ID: {chunk_id}\")\n",
        "                 else:\n",
        "                    print(f\"Warning: Chunk ID {chunk_id} not found in chunk_dict after reranking.\")\n",
        "\n",
        "        else:\n",
        "            search_results = []\n",
        "            print(\"⚠️ No pairs for re-ranking.\")\n",
        "\n",
        "\n",
        "        print(f\"✅ Hybrid search completed. Returning {len(search_results)} results.\")\n",
        "        return search_results\n",
        "\n",
        "\n",
        "    def display_results(self, results: List[SearchResult]):\n",
        "        \"\"\"Display search results in a formatted way.\"\"\"\n",
        "        if not results:\n",
        "            print(\"❌ No results found\")\n",
        "            return\n",
        "\n",
        "        print(f\"\\n🎯 Found {len(results)} results:\\n\")\n",
        "        print(\"=\" * 80)\n",
        "\n",
        "        for result in results:\n",
        "            chunk = result.chunk\n",
        "            print(f\"📄 Rank #{result.rank} | Score: {result.score:.4f} | Type: {chunk.source_type} | Search Type: {result.search_type}\") # Added Search Type\n",
        "            print(f\"📍 Page {chunk.page_number + 1} | ID: {chunk.id}\")\n",
        "\n",
        "            # Note: Image data is not loaded into chunks in HybridSearchEngine\n",
        "            # if chunk.source_type == \"image\" and chunk.image_data:\n",
        "            #     print(\"🖼️  IMAGE CONTENT:\")\n",
        "            #     # Display image (requires image data)\n",
        "            #     img = Image.open(io.BytesIO(chunk.image_data))\n",
        "            #     display(img.resize((200, 200)))\n",
        "            if chunk.source_type == \"image\" and chunk.image_description:\n",
        "                 print(\"🖼️  IMAGE CONTENT (Description Only):\")\n",
        "                 print(chunk.image_description[:200] + (\"...\" if len(chunk.image_description) > 200 else \"\"))\n",
        "            elif chunk.source_type == \"image\":\n",
        "                 print(\"🖼️  IMAGE CONTENT (No Description Available).\")\n",
        "\n",
        "\n",
        "            print(\"📝 Content:\")\n",
        "            print(chunk.text[:500] + (\"...\" if len(chunk.text) > 500 else \"\"))\n",
        "            print(\"=\" * 80)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "46e45390"
      },
      "source": [
        "**Reasoning**:\n",
        "Review the keyword_search method to confirm it uses the internally loaded BM25 index and chunks.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Ingest the uploaded PDF\n",
        "# uploaded_pdf_path = \"/content/series-150s-and-157s-low-water-cut-offs-pump-controllers-mm-217p.pdf\" # Correct path in Colab\n",
        "uploaded_pdf_path=\"/content/3102-flygt-workshop_manual.pdf\"\n",
        "\n",
        "ingester.ingest_document(uploaded_pdf_path)\n",
        "qdrant_client = create_qdrant_client()\n",
        "# Re-initialize the search engine after ingestion\n",
        "search_engine = HybridSearchEngine(qdrant_client)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 382
        },
        "collapsed": true,
        "id": "d-Lk8izeVD0w",
        "outputId": "310b35bc-8ede-42e9-f358-01a5679c4ef4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "no such file: '/content/3102-flygt-workshop_manual.pdf'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3387623286.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0muploaded_pdf_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"/content/3102-flygt-workshop_manual.pdf\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mingester\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mingest_document\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muploaded_pdf_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mqdrant_client\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_qdrant_client\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Re-initialize the search engine after ingestion\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-1553503812.py\u001b[0m in \u001b[0;36mingest_document\u001b[0;34m(self, pdf_path)\u001b[0m\n\u001b[1;32m    472\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mingest_document\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpdf_path\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    473\u001b[0m         \u001b[0;34m\"\"\"Complete ingestion pipeline for a PDF document.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 474\u001b[0;31m         \u001b[0mchunks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextract_pdf_content\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpdf_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    475\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mingest_chunks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_bm25_corpus\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Save the corpus after processing each document\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-1553503812.py\u001b[0m in \u001b[0;36mextract_pdf_content\u001b[0;34m(self, pdf_path)\u001b[0m\n\u001b[1;32m    299\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextract_pdf_content\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpdf_path\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mDocumentChunk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m         \u001b[0;34m\"\"\"Extract text and images from PDF.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 301\u001b[0;31m         \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfitz\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpdf_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    302\u001b[0m         \u001b[0mall_chunks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pymupdf/__init__.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, filename, stream, filetype, rect, width, height, fontsize)\u001b[0m\n\u001b[1;32m   2955\u001b[0m                 \u001b[0;31m# generating warnings etc.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2956\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2957\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0mFileNotFoundError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"no such file: '{filename}'\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2958\u001b[0m                 \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2959\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mFileDataError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"'{filename}' is no file\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: no such file: '/content/3102-flygt-workshop_manual.pdf'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1e8b35ac"
      },
      "source": [
        "**Reasoning**:\n",
        "Review the `_load_chunks_from_qdrant` and `display_results` methods to confirm they correctly handle document and page information.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming search_engine and the reranker_model are already initialized and available\n",
        "\n",
        "# Perform the hybrid search with re-ranking\n",
        "reranked_results = search_engine.hybrid_search(\"How do I replace the mechanical seal on the Flygt 3102?\", top_k=3)\n",
        "\n",
        "# Display the re-ranked results\n",
        "search_engine.display_results(reranked_results)"
      ],
      "metadata": {
        "id": "LrEusIl9Vu2q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming reranked_results and genai are available\n",
        "\n",
        "def format_results_with_llm(results: List[SearchResult], query: str) -> str:\n",
        "    \"\"\"Formats search results into a readable summary using an LLM, including tables and image references, and document/page references.\"\"\"\n",
        "    if not results:\n",
        "        return \"No results found.\"\n",
        "\n",
        "    context = \"Search Query: \" + query + \"\\n\\nSearch Results:\\n\"\n",
        "    for i, result in enumerate(results):\n",
        "        # Include document and page reference in the header\n",
        "        doc_reference = result.chunk.metadata.get(\"pdf_path\", \"Unknown Document\")\n",
        "        page_reference = result.chunk.page_number + 1\n",
        "        context += f\"--- Result {i+1} (Document: {doc_reference}, Page: {page_reference}, Score: {result.score:.4f}) ---\\n\"\n",
        "\n",
        "        # Include image information if available\n",
        "        if result.chunk.source_type == \"image\" and result.chunk.image_description:\n",
        "            context += f\"Image Description: {result.chunk.image_description}\\n\"\n",
        "            context += \"Reference to Image included in this chunk.\\n\" # Indicate image presence\n",
        "        elif result.chunk.image_data: # Handle cases where description might be missing but image data exists\n",
        "             context += \"Reference to Image included in this chunk.\\n\"\n",
        "\n",
        "\n",
        "        # Include text content, attempt to preserve table formatting\n",
        "        context += result.chunk.text[:1000] + (\"...\" if len(result.chunk.text) > 1000 else \"\") + \"\\n\\n\"\n",
        "\n",
        "\n",
        "    # prompt = f\"\"\"Based on the following search query and search results, provide a concise and readable summary of the key information.\n",
        "    # For each piece of information in the summary, include a reference to the document and page number it was pulled from (e.g., [Document: report.pdf, Page: 5]).\n",
        "    # If there is associated part number, show that too. If there is any parenthetical phrase or additional information add that too (e.g. additional text)\n",
        "    # Pay close attention to the formatting of the original text, especially for tables, and try to reproduce them accurately in markdown format.\n",
        "    # Also, mention if any of the results included images.\n",
        "    # Show the summary in markdown in new line instead of one single line statement\n",
        "\n",
        "    # {context}\n",
        "\n",
        "    # Summary:\n",
        "    # \"\"\"\n",
        "\n",
        "\n",
        "    prompt = f\"\"\"You are a technical assistant helping field technicians perform maintenance on industrial equipment using official manuals.\n",
        "    Your goal is to extract clear, complete, and accurate instructions from the provided documentation to help the technician answer their question.\n",
        "    Respond to the question below using only the content from the provided manual or documentation.\n",
        "    ---\n",
        "    Question:\n",
        "    {context}\n",
        "    ---\n",
        "    When answering:\n",
        "    - Break the response into clear **step-by-step instructions**, using bullet points or numbered steps.\n",
        "    - Include **model-specific notes** if the document mentions multiple variants (e.g. 3127 vs. 3102).\n",
        "    - Include **tool names**, **part numbers**, **measurements**, **torque specs**, and **safety precautions** if mentioned.\n",
        "    - Reference the **exact page number(s)** where the information is found.\n",
        "    - If the steps are only partially described, clearly state what is **missing** or **not covered** in the documentation.\n",
        "    - Use a tone that is professional, instructional, and assumes the user has technical knowledge but needs procedural clarity.\n",
        "    - Avoid assumptions or external knowledge. If something is unclear or missing, say so.\n",
        "    ---\n",
        "    If diagrams or figures are referenced in the document, include them in the explanation or summarize what they show.\"\"\"\n",
        "\n",
        "    try:\n",
        "        # Initialize Gemini model (assuming it's already configured)\n",
        "        model = genai.GenerativeModel('gemini-1.5-flash')\n",
        "        response = model.generate_content(prompt)\n",
        "        return response.text\n",
        "    except Exception as e:\n",
        "        print(f\"Error formatting results with LLM: {e}\")\n",
        "        return \"Error generating summary with LLM.\"\n",
        "\n",
        "# Assuming reranked_results from the previous step is available\n",
        "if 'reranked_results' in locals():\n",
        "    llm_summary = format_results_with_llm(reranked_results, \"How do I replace the mechanical seal on the Flygt 3102?\")\n",
        "    print(\"\\n--- LLM Summary ---\")\n",
        "    print(llm_summary)\n",
        "else:\n",
        "    print(\"❌ Reranked results not found. Please run the previous steps.\")"
      ],
      "metadata": {
        "id": "DAjPiQWUUj39"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "49c99870"
      },
      "source": [
        "# Task\n",
        "Update the document ingestion process to handle markdown files in addition to PDF files."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "750b1294"
      },
      "source": [
        "## Modify documentingester\n",
        "\n",
        "### Subtask:\n",
        "Update the `DocumentIngester` class to include a method for extracting text from markdown files.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fd6901d7"
      },
      "source": [
        "**Reasoning**:\n",
        "Define the `extract_markdown_content` method within the `DocumentIngester` class to handle markdown file processing and chunking.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "21eaa10d"
      },
      "source": [
        "class DocumentIngester:\n",
        "    def __init__(self, qdrant_client: QdrantClient):\n",
        "        self.qdrant_client = qdrant_client\n",
        "        self.chunks = []\n",
        "        self.bm25_corpus = []\n",
        "        self.bm25_index = None\n",
        "        self.bm25_corpus_file = \"bm25_calcom_corpus.json\"\n",
        "\n",
        "        # Delete collection if it exists (keep for fresh start in demo)\n",
        "        # try:\n",
        "        #     self.qdrant_client.delete_collection(collection_name=CONFIG[\"collection_name\"])\n",
        "        #     print(f\"✅ Deleted existing collection: {CONFIG['collection_name']}\")\n",
        "        # except Exception as e:\n",
        "        #     print(f\"Collection might not exist or error deleting: {e}\")\n",
        "\n",
        "\n",
        "        # Create collection if it doesn't exist\n",
        "        try:\n",
        "            self.qdrant_client.get_collection(collection_name=CONFIG[\"collection_name\"])\n",
        "            print(f\"✅ Collection '{CONFIG['collection_name']}' already exists.\")\n",
        "        except Exception: # Collection does not exist\n",
        "            try:\n",
        "                self.qdrant_client.create_collection(\n",
        "                    collection_name=CONFIG[\"collection_name\"],\n",
        "                    vectors_config=VectorParams(\n",
        "                        size=CONFIG[\"vector_dimension\"],\n",
        "                        distance=Distance.COSINE\n",
        "                    )\n",
        "                )\n",
        "                print(f\"✅ Created collection: {CONFIG['collection_name']}\")\n",
        "            except Exception as e:\n",
        "                print(f\"❌ Error creating collection: {e}\")\n",
        "\n",
        "        # Attempt to load BM25 corpus\n",
        "        self.load_bm25_corpus(self.bm25_corpus_file)\n",
        "\n",
        "        # Rebuild BM25 index if corpus was loaded\n",
        "        if self.bm25_corpus:\n",
        "            try:\n",
        "                self.bm25_index = BM25Okapi(self.bm25_corpus)\n",
        "                print(\"✅ BM25 index rebuilt from loaded corpus.\")\n",
        "            except Exception as e:\n",
        "                print(f\"❌ Error rebuilding BM25 index from loaded corpus: {e}\")\n",
        "                self.bm25_index = None\n",
        "\n",
        "    def describe_image_with_gemini(self, image_bytes: bytes) -> str:\n",
        "        \"\"\"Generate image description using Gemini Vision.\"\"\"\n",
        "        try:\n",
        "            # Convert bytes to PIL Image\n",
        "            image = Image.open(io.BytesIO(image_bytes))\n",
        "\n",
        "            # Initialize Gemini Vision model\n",
        "            # Ensure you have initialized genai with your API key before calling this function\n",
        "            model = genai.GenerativeModel('gemini-1.5-flash')\n",
        "\n",
        "            # prompt = \"\"\"Describe this image in detail. Focus on:\n",
        "            # 1. Main objects, people, or elements, or technical diagrams\n",
        "            # 2. Text content (if any)\n",
        "            # 3. Charts, diagrams, or technical content\n",
        "            # 4. Spatial relationships and layout\n",
        "            # 5. Any relevant contextual information\n",
        "\n",
        "            # Provide a comprehensive description that would help in document search.\"\"\"\n",
        "            prompt = \"\"\"\"Describe the technical diagrams and tables found in the provided document. For each description, follow these steps:\n",
        "\n",
        "            Purpose: Begin by stating the primary function or purpose of the diagram or table (e.g., 'This is a wiring diagram showing the electrical connections,' or 'This table provides the technical specifications for the product').\n",
        "\n",
        "            Components: Provide a detailed breakdown of the visual elements.\n",
        "\n",
        "            For diagrams: List and explain each labeled component, symbol, or annotation. Describe the relationships or processes shown by arrows or other visual cues.\n",
        "\n",
        "            For tables: Identify and explain the meaning of each column and row. Highlight the key data points, including any values and units of measurement.\n",
        "\n",
        "            Synthesis: Conclude with a summary of the most important information presented, explaining how the different parts of the diagram or table work together to convey a complete message.\"\"\"\n",
        "\n",
        "            response = model.generate_content([prompt, image])\n",
        "            return response.text\n",
        "        except Exception as e:\n",
        "            print(f\"Error describing image: {e}\")\n",
        "            return \"Image description unavailable\"\n",
        "\n",
        "    def extract_pdf_content(self, pdf_path: str) -> List[DocumentChunk]:\n",
        "        \"\"\"Extract text and images from PDF.\"\"\"\n",
        "        doc = fitz.open(pdf_path)\n",
        "        all_chunks = []\n",
        "\n",
        "        print(f\"📄 Processing PDF with {min(len(doc), 10)} pages (limited to first 10 for trial)...\") # Updated print statement\n",
        "\n",
        "        # Process only the first 10 pages\n",
        "        for page_num in range(len(doc)):\n",
        "            page = doc.load_page(page_num)\n",
        "            print(f\"Processing page {page_num + 1}...\")\n",
        "\n",
        "            # Extract text\n",
        "            text = page.get_text()\n",
        "            if text.strip():\n",
        "                print(f\"Extracted text from page {page_num + 1}. Text length: {len(text)}\")\n",
        "                # Chunk the text\n",
        "                text_chunks = extract_and_chunk_text(text, CONFIG[\"chunk_size\"], CONFIG[\"chunk_overlap\"])\n",
        "                print(f\"Split into {len(text_chunks)} text chunks on page {page_num + 1}\")\n",
        "                print(f\"Debug: text_chunks content for page {page_num + 1}: {text_chunks}, Type: {type(text_chunks)}\") # Added debug log\n",
        "\n",
        "\n",
        "                if text_chunks: # Add check here\n",
        "                    for i, chunk_text in enumerate(text_chunks):\n",
        "                        print(f\"Debug: Processing text chunk {i} on page {page_num + 1}: {chunk_text[:50]}...\") # Added debug log\n",
        "                        chunk_id = f\"page_{page_num}_chunk_{i}\"\n",
        "                        chunk = DocumentChunk(\n",
        "                            id=chunk_id,\n",
        "                            text=chunk_text,\n",
        "                            page_number=page_num,\n",
        "                            source_type=\"document\",\n",
        "                            metadata={\"pdf_path\": pdf_path}\n",
        "                        )\n",
        "                        all_chunks.append(chunk)\n",
        "                else:\n",
        "                    print(f\"⚠️  No text chunks generated for page {page_num + 1}\") # Added log\n",
        "\n",
        "\n",
        "            # Extract images\n",
        "            image_list = page.get_images()\n",
        "            print(f\"Found {len(image_list)} images on page {page_num + 1}\")\n",
        "            for img_index, img in enumerate(image_list):\n",
        "                try:\n",
        "                    # Get image data\n",
        "                    xref = img[0]\n",
        "                    pix = fitz.Pixmap(doc, xref)\n",
        "\n",
        "                    # Convert pixmap to RGB if it's not already in a compatible format\n",
        "                    if pix.n > 3: # CMYK or other formats\n",
        "                         pix = fitz.Pixmap(fitz.csRGB, pix)\n",
        "                    elif pix.n == 1: # Grayscale\n",
        "                         pix = fitz.Pixmap(fitz.csRGB, pix) # Convert grayscale to RGB\n",
        "\n",
        "\n",
        "                    img_data = pix.tobytes(\"png\")\n",
        "\n",
        "                    # Generate description\n",
        "                    print(f\"Describing image {img_index} on page {page_num + 1}...\")\n",
        "                    description = self.describe_image_with_gemini(img_data) # Use self.describe_image_with_gemini\n",
        "                    print(f\"Image description generated for page {page_num + 1}: {description[:50]}...\")\n",
        "\n",
        "                    # Create image chunk\n",
        "                    chunk_id = f\"page_{page_num}_image_{img_index}\"\n",
        "                    chunk = DocumentChunk(\n",
        "                        id=chunk_id,\n",
        "                        text=description,\n",
        "                        page_number=page_num,\n",
        "                        source_type=\"image\",\n",
        "                        image_description=description,\n",
        "                        image_data=img_data,\n",
        "                        metadata={\"pdf_path\": pdf_path}\n",
        "                    )\n",
        "                    all_chunks.append(chunk)\n",
        "                    print(f\"🖼️  Processed image {img_index} on page {page_num + 1}\")\n",
        "\n",
        "                    pix = None\n",
        "                except Exception as e:\n",
        "                    print(f\"Error processing image {img_index} on page {page_num}: {e}\")\n",
        "\n",
        "        doc.close()\n",
        "        print(f\"✅ Extracted {len(all_chunks)} chunks from PDF\")\n",
        "        return all_chunks\n",
        "\n",
        "    def extract_markdown_content(self, md_path: str) -> List[DocumentChunk]:\n",
        "        \"\"\"Extract text from markdown files and chunk it.\"\"\"\n",
        "        all_chunks = []\n",
        "        try:\n",
        "            with open(md_path, 'r', encoding='utf-8') as f:\n",
        "                markdown_text = f.read()\n",
        "\n",
        "            # Simple markdown cleaning (remove headers, lists, etc.) - can be improved\n",
        "            # This is a basic approach; a dedicated markdown parser would be better for complex docs\n",
        "            cleaned_text = []\n",
        "            for line in markdown_text.splitlines():\n",
        "                line = line.strip()\n",
        "                if line and not line.startswith('#') and not line.startswith('-') and not line.startswith('*') and not line.startswith('>'):\n",
        "                    cleaned_text.append(line)\n",
        "            cleaned_text = \" \".join(cleaned_text)\n",
        "\n",
        "            if cleaned_text.strip():\n",
        "                print(f\"Extracted text from markdown {md_path}. Text length: {len(cleaned_text)}\")\n",
        "                # Chunk the text\n",
        "                text_chunks = extract_and_chunk_text(cleaned_text, CONFIG[\"chunk_size\"], CONFIG[\"chunk_overlap\"])\n",
        "                print(f\"Split into {len(text_chunks)} text chunks from markdown\")\n",
        "\n",
        "                if text_chunks:\n",
        "                    for i, chunk_text in enumerate(text_chunks):\n",
        "                        chunk_id = f\"{os.path.basename(md_path)}_chunk_{i}\"\n",
        "                        # Markdown doesn't have pages, use 0 or a unique identifier\n",
        "                        chunk = DocumentChunk(\n",
        "                            id=chunk_id,\n",
        "                            text=chunk_text,\n",
        "                            page_number=0, # Or a different indicator for markdown\n",
        "                            source_type=\"markdown\",\n",
        "                            metadata={\"markdown_path\": md_path}\n",
        "                        )\n",
        "                        all_chunks.append(chunk)\n",
        "                else:\n",
        "                     print(f\"⚠️ No text chunks generated for markdown file {md_path}\")\n",
        "\n",
        "            print(f\"✅ Extracted {len(all_chunks)} chunks from markdown {md_path}\")\n",
        "            return all_chunks\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"❌ Error extracting content from markdown file {md_path}: {e}\")\n",
        "            return []\n",
        "\n",
        "\n",
        "    def ingest_chunks(self, chunks: List[DocumentChunk]):\n",
        "        \"\"\"Ingest chunks into vector database and BM25 index.\"\"\"\n",
        "        print(\"🔄 Starting ingestion process...\")\n",
        "        print(f\"Attempting to ingest {len(chunks)} chunks.\")\n",
        "\n",
        "        points = []\n",
        "        newly_added_corpus = [] # Use a temporary list for current ingestion's corpus\n",
        "\n",
        "        for i, chunk in enumerate(chunks):\n",
        "            print(f\"Processing chunk {i+1}/{len(chunks)} (ID: {chunk.id}, Type: {chunk.source_type})\")\n",
        "            # Get embedding\n",
        "            embedding = get_gemini_embedding_with_retry(chunk.text, \"RETRIEVAL_DOCUMENT\")\n",
        "            if embedding is None:\n",
        "                print(f\"⚠️  Skipping chunk {chunk.id} - failed to get embedding\")\n",
        "                continue\n",
        "            print(f\"✅ Embedding generated for chunk {chunk.id}\")\n",
        "\n",
        "            # Prepare point for Qdrant\n",
        "            point = PointStruct(\n",
        "                id=len(self.chunks) + len(points), # Ensure unique IDs across multiple ingestions\n",
        "                vector=embedding,\n",
        "                payload={\n",
        "                    \"chunk_id\": chunk.id,\n",
        "                    \"text\": chunk.text,\n",
        "                    \"page_number\": chunk.page_number,\n",
        "                    \"source_type\": chunk.source_type,\n",
        "                    \"image_description\": chunk.image_description,\n",
        "                    \"has_image\": chunk.image_data is not None,\n",
        "                    \"metadata\": chunk.metadata or {}\n",
        "                }\n",
        "            )\n",
        "            points.append(point)\n",
        "            print(f\"✅ Point prepared for chunk {chunk.id}. Total points prepared: {len(points)}\")\n",
        "\n",
        "\n",
        "            # Prepare for BM25 using NLTK and stop words\n",
        "            try:\n",
        "                tokens = word_tokenize(chunk.text.lower())\n",
        "                filtered_tokens = [word for word in tokens if word.isalnum() and word not in stop_words]\n",
        "                newly_added_corpus.append(filtered_tokens)\n",
        "                print(f\"✅ Tokenized and filtered chunk {chunk.id} for BM25.\")\n",
        "            except Exception as e:\n",
        "                print(f\"❌ Error tokenizing/filtering chunk {chunk.id} for BM25: {e}\")\n",
        "                newly_added_corpus.append([]) # Add empty list to maintain corpus length\n",
        "\n",
        "\n",
        "            # Store chunk reference\n",
        "            self.chunks.extend(chunks) # Extend chunks with the new chunks\n",
        "            print(f\"Debug: self.chunks length after adding current chunks: {len(self.chunks)}\")\n",
        "\n",
        "\n",
        "        print(f\"Prepared {len(points)} points for upserting to Qdrant.\")\n",
        "        if points:\n",
        "            # Upsert to Qdrant\n",
        "            try:\n",
        "                response = self.qdrant_client.upsert(\n",
        "                    collection_name=CONFIG[\"collection_name\"],\n",
        "                    points=points,\n",
        "                    wait=True # Wait for the operation to complete\n",
        "                )\n",
        "                print(f\"✅ Qdrant upsert response: {response}\")\n",
        "                print(f\"✅ Upserted {len(points)} points to Qdrant.\")\n",
        "            except Exception as e:\n",
        "                print(f\"❌ Error during Qdrant upsert: {e}\")\n",
        "\n",
        "        else:\n",
        "            print(\"⚠️ No points to upsert to Qdrant.\")\n",
        "\n",
        "        # Accumulate the new corpus\n",
        "        self.bm25_corpus.extend(newly_added_corpus)\n",
        "\n",
        "        print(f\"Debug: Corpus content after adding newly added corpus: {self.bm25_corpus}, Type: {type(self.bm25_corpus)}\") # Added debug log\n",
        "        # Build BM25 index from the accumulated corpus\n",
        "        if self.bm25_corpus and any(self.bm25_corpus): # Check if corpus is not empty and contains non-empty token lists\n",
        "            self.bm25_index = BM25Okapi(self.bm25_corpus)\n",
        "            print(f\"✅ BM25 index built with {len(self.bm25_corpus)} documents.\")\n",
        "\n",
        "            # Save BM25 corpus to file\n",
        "            self.save_bm25_corpus() # Call the new method\n",
        "\n",
        "        else:\n",
        "            self.bm25_index = None # Ensure index is None if corpus is empty\n",
        "            self.bm25_corpus = []\n",
        "            print(\"⚠️ No corpus for building BM25 index.\")\n",
        "\n",
        "\n",
        "        print(f\"✅ Successfully processed {len(points)} chunks for ingestion\")\n",
        "\n",
        "\n",
        "    def ingest_document(self, doc_path: str):\n",
        "        \"\"\"Complete ingestion pipeline for a document (PDF or Markdown).\"\"\"\n",
        "        if doc_path.lower().endswith('.pdf'):\n",
        "            chunks = self.extract_pdf_content(doc_path)\n",
        "        elif doc_path.lower().endswith('.md'):\n",
        "            chunks = self.extract_markdown_content(doc_path)\n",
        "        else:\n",
        "            print(f\"❌ Unsupported file type: {doc_path}\")\n",
        "            return\n",
        "\n",
        "        self.ingest_chunks(chunks)\n",
        "        self.save_bm25_corpus() # Save the corpus after processing each document\n",
        "\n",
        "    def load_bm25_corpus(self, file_path: str):\n",
        "        \"\"\"Loads the tokenized BM25 corpus from a JSON file.\"\"\"\n",
        "        if os.path.exists(file_path):\n",
        "            try:\n",
        "                with open(file_path, \"r\") as f:\n",
        "                    loaded_corpus = json.load(f)\n",
        "                    if isinstance(loaded_corpus, list):\n",
        "                        self.bm25_corpus = loaded_corpus\n",
        "                    else:\n",
        "                        print(f\"⚠️ Loaded BM25 corpus from {file_path} is not a list. Initializing with empty corpus.\")\n",
        "                        self.bm25_corpus = []\n",
        "                print(f\"✅ BM25 corpus loaded from {file_path}\")\n",
        "            except Exception as e:\n",
        "                print(f\"❌ Error loading BM25 corpus from {file_path}: {e}\")\n",
        "                self.bm25_corpus = []\n",
        "        else:\n",
        "            print(f\"⚠️ BM25 corpus file not found at {file_path}. BM25 index will be built during ingestion.\")\n",
        "            self.bm25_corpus = []\n",
        "\n",
        "\n",
        "    def save_bm25_corpus(self):\n",
        "        \"\"\"Saves the tokenized BM25 corpus to a JSON file.\"\"\"\n",
        "        try:\n",
        "            with open(self.bm25_corpus_file, \"w\") as f:\n",
        "                json.dump(self.bm25_corpus, f)\n",
        "            print(f\"✅ BM25 corpus saved to {self.bm25_corpus_file}\")\n",
        "        except Exception as e:\n",
        "            print(f\"❌ Error saving BM25 corpus: {e}\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "96c45f39"
      },
      "source": [
        "**Reasoning**:\n",
        "Ingest the newly created markdown file using the `ingester.ingest_document()` method and observe the output.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X66vl1M48NW7"
      },
      "source": [
        "class DocumentIngester:\n",
        "    def __init__(self, qdrant_client: QdrantClient):\n",
        "        self.qdrant_client = qdrant_client\n",
        "        self.chunks = []\n",
        "        self.bm25_corpus = []\n",
        "        self.bm25_index = None\n",
        "        self.bm25_corpus_file = \"bm25_calcom_corpus.json\"\n",
        "\n",
        "        # Delete collection if it exists (keep for fresh start in demo)\n",
        "        # try:\n",
        "        #     self.qdrant_client.delete_collection(collection_name=CONFIG[\"collection_name\"])\n",
        "        #     print(f\"✅ Deleted existing collection: {CONFIG['collection_name']}\")\n",
        "        # except Exception as e:\n",
        "        #     print(f\"Collection might not exist or error deleting: {e}\")\n",
        "\n",
        "\n",
        "        # Create collection if it doesn't exist\n",
        "        try:\n",
        "            self.qdrant_client.get_collection(collection_name=CONFIG[\"collection_name\"])\n",
        "            print(f\"✅ Collection '{CONFIG['collection_name']}' already exists.\")\n",
        "        except Exception: # Collection does not exist\n",
        "            try:\n",
        "                self.qdrant_client.create_collection(\n",
        "                    collection_name=CONFIG[\"collection_name\"],\n",
        "                    vectors_config=VectorParams(\n",
        "                        size=CONFIG[\"vector_dimension\"],\n",
        "                        distance=Distance.COSINE\n",
        "                    )\n",
        "                )\n",
        "                print(f\"✅ Created collection: {CONFIG['collection_name']}\")\n",
        "            except Exception as e:\n",
        "                print(f\"❌ Error creating collection: {e}\")\n",
        "\n",
        "        # Attempt to load BM25 corpus\n",
        "        self.load_bm25_corpus(self.bm25_corpus_file)\n",
        "\n",
        "        # Rebuild BM25 index if corpus was loaded\n",
        "        if self.bm25_corpus:\n",
        "            try:\n",
        "                self.bm25_index = BM25Okapi(self.bm25_corpus)\n",
        "                print(\"✅ BM25 index rebuilt from loaded corpus.\")\n",
        "            except Exception as e:\n",
        "                print(f\"❌ Error rebuilding BM25 index from loaded corpus: {e}\")\n",
        "                self.bm25_index = None\n",
        "\n",
        "    def describe_image_with_gemini(self, image_bytes: bytes) -> str:\n",
        "        \"\"\"Generate image description using Gemini Vision.\"\"\"\n",
        "        try:\n",
        "            # Convert bytes to PIL Image\n",
        "            image = Image.open(io.BytesIO(image_bytes))\n",
        "\n",
        "            # Initialize Gemini Vision model\n",
        "            # Ensure you have initialized genai with your API key before calling this function\n",
        "            model = genai.GenerativeModel('gemini-1.5-flash')\n",
        "\n",
        "            # prompt = \"\"\"Describe this image in detail. Focus on:\n",
        "            # 1. Main objects, people, or elements, or technical diagrams\n",
        "            # 2. Text content (if any)\n",
        "            # 3. Charts, diagrams, or technical content\n",
        "            # 4. Spatial relationships and layout\n",
        "            # 5. Any relevant contextual information\n",
        "\n",
        "            # Provide a comprehensive description that would help in document search.\"\"\"\n",
        "            prompt = \"\"\"\"Describe the technical diagrams and tables found in the provided document. For each description, follow these steps:\n",
        "\n",
        "            Purpose: Begin by stating the primary function or purpose of the diagram or table (e.g., 'This is a wiring diagram showing the electrical connections,' or 'This table provides the technical specifications for the product').\n",
        "\n",
        "            Components: Provide a detailed breakdown of the visual elements.\n",
        "\n",
        "            For diagrams: List and explain each labeled component, symbol, or annotation. Describe the relationships or processes shown by arrows or other visual cues.\n",
        "\n",
        "            For tables: Identify and explain the meaning of each column and row. Highlight the key data points, including any values and units of measurement.\n",
        "\n",
        "            Synthesis: Conclude with a summary of the most important information presented, explaining how the different parts of the diagram or table work together to convey a complete message.\"\"\"\n",
        "\n",
        "            response = model.generate_content([prompt, image])\n",
        "            return response.text\n",
        "        except Exception as e:\n",
        "            print(f\"Error describing image: {e}\")\n",
        "            return \"Image description unavailable\"\n",
        "\n",
        "    def extract_pdf_content(self, pdf_path: str) -> List[DocumentChunk]:\n",
        "        \"\"\"Extract text and images from PDF.\"\"\"\n",
        "        doc = fitz.open(pdf_path)\n",
        "        all_chunks = []\n",
        "\n",
        "        print(f\"📄 Processing PDF with {min(len(doc), 10)} pages (limited to first 10 for trial)...\") # Updated print statement\n",
        "\n",
        "        # Process only the first 10 pages\n",
        "        for page_num in range(len(doc)):\n",
        "            page = doc.load_page(page_num)\n",
        "            print(f\"Processing page {page_num + 1}...\")\n",
        "\n",
        "            # Extract text\n",
        "            text = page.get_text()\n",
        "            if text.strip():\n",
        "                print(f\"Extracted text from page {page_num + 1}. Text length: {len(text)}\")\n",
        "                # Chunk the text\n",
        "                text_chunks = extract_and_chunk_text(text, CONFIG[\"chunk_size\"], CONFIG[\"chunk_overlap\"])\n",
        "                print(f\"Split into {len(text_chunks)} text chunks on page {page_num + 1}\")\n",
        "                print(f\"Debug: text_chunks content for page {page_num + 1}: {text_chunks}, Type: {type(text_chunks)}\") # Added debug log\n",
        "\n",
        "\n",
        "                if text_chunks: # Add check here\n",
        "                    for i, chunk_text in enumerate(text_chunks):\n",
        "                        print(f\"Debug: Processing text chunk {i} on page {page_num + 1}: {chunk_text[:50]}...\") # Added debug log\n",
        "                        chunk_id = f\"page_{page_num}_chunk_{i}\"\n",
        "                        chunk = DocumentChunk(\n",
        "                            id=chunk_id,\n",
        "                            text=chunk_text,\n",
        "                            page_number=page_num,\n",
        "                            source_type=\"document\",\n",
        "                            metadata={\"pdf_path\": pdf_path}\n",
        "                        )\n",
        "                        all_chunks.append(chunk)\n",
        "                else:\n",
        "                    print(f\"⚠️  No text chunks generated for page {page_num + 1}\") # Added log\n",
        "\n",
        "\n",
        "            # Extract images\n",
        "            image_list = page.get_images()\n",
        "            print(f\"Found {len(image_list)} images on page {page_num + 1}\")\n",
        "            for img_index, img in enumerate(image_list):\n",
        "                try:\n",
        "                    # Get image data\n",
        "                    xref = img[0]\n",
        "                    pix = fitz.Pixmap(doc, xref)\n",
        "\n",
        "                    # Convert pixmap to RGB if it's not already in a compatible format\n",
        "                    if pix.n > 3: # CMYK or other formats\n",
        "                         pix = fitz.Pixmap(fitz.csRGB, pix)\n",
        "                    elif pix.n == 1: # Grayscale\n",
        "                         pix = fitz.Pixmap(fitz.csRGB, pix) # Convert grayscale to RGB\n",
        "\n",
        "\n",
        "                    img_data = pix.tobytes(\"png\")\n",
        "\n",
        "                    # Generate description\n",
        "                    print(f\"Describing image {img_index} on page {page_num + 1}...\")\n",
        "                    description = self.describe_image_with_gemini(img_data) # Use self.describe_image_with_gemini\n",
        "                    print(f\"Image description generated for page {page_num + 1}: {description[:50]}...\")\n",
        "\n",
        "                    # Create image chunk\n",
        "                    chunk_id = f\"page_{page_num}_image_{img_index}\"\n",
        "                    chunk = DocumentChunk(\n",
        "                        id=chunk_id,\n",
        "                        text=description,\n",
        "                        page_number=page_num,\n",
        "                        source_type=\"image\",\n",
        "                        image_description=description,\n",
        "                        image_data=img_data,\n",
        "                        metadata={\"pdf_path\": pdf_path}\n",
        "                    )\n",
        "                    all_chunks.append(chunk)\n",
        "                    print(f\"🖼️  Processed image {img_index} on page {page_num + 1}\")\n",
        "\n",
        "                    pix = None\n",
        "                except Exception as e:\n",
        "                    print(f\"Error processing image {img_index} on page {page_num}: {e}\")\n",
        "\n",
        "        doc.close()\n",
        "        print(f\"✅ Extracted {len(all_chunks)} chunks from PDF\")\n",
        "        return all_chunks\n",
        "\n",
        "    def extract_markdown_content(self, md_path: str) -> List[DocumentChunk]:\n",
        "        \"\"\"Extract text from markdown files and chunk it.\"\"\"\n",
        "        all_chunks = []\n",
        "        try:\n",
        "            with open(md_path, 'r', encoding='utf-8') as f:\n",
        "                markdown_text = f.read()\n",
        "\n",
        "            # Simple markdown cleaning (remove headers, lists, etc.) - can be improved\n",
        "            # This is a basic approach; a dedicated markdown parser would be better for complex docs\n",
        "            cleaned_text = []\n",
        "            for line in markdown_text.splitlines():\n",
        "                line = line.strip()\n",
        "                # Keep lines that are not headers, lists, blockquotes, or horizontal rules\n",
        "                if line and not line.startswith('#') and not line.startswith('- ') and not line.startswith('* ') and not line.startswith('> ') and not line.startswith('---') and not line.startswith('***') and not line.startswith('___'):\n",
        "                     cleaned_text.append(line)\n",
        "            cleaned_text = \" \".join(cleaned_text)\n",
        "\n",
        "\n",
        "            if cleaned_text.strip():\n",
        "                print(f\"Extracted text from markdown {md_path}. Text length: {len(cleaned_text)}\")\n",
        "                # Chunk the text\n",
        "                text_chunks = extract_and_chunk_text(cleaned_text, CONFIG[\"chunk_size\"], CONFIG[\"chunk_overlap\"])\n",
        "                print(f\"Split into {len(text_chunks)} text chunks from markdown\")\n",
        "\n",
        "                if text_chunks:\n",
        "                    for i, chunk_text in enumerate(text_chunks):\n",
        "                        chunk_id = f\"{os.path.basename(md_path)}_chunk_{i}\"\n",
        "                        # Markdown doesn't have pages, use 0 or a unique identifier\n",
        "                        chunk = DocumentChunk(\n",
        "                            id=chunk_id,\n",
        "                            text=chunk_text,\n",
        "                            page_number=0, # Or a different indicator for markdown\n",
        "                            source_type=\"markdown\",\n",
        "                            metadata={\"markdown_path\": md_path}\n",
        "                        )\n",
        "                        all_chunks.append(chunk)\n",
        "                else:\n",
        "                     print(f\"⚠️ No text chunks generated for markdown file {md_path}\")\n",
        "\n",
        "            print(f\"✅ Extracted {len(all_chunks)} chunks from markdown {md_path}\")\n",
        "            return all_chunks\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"❌ Error extracting content from markdown file {md_path}: {e}\")\n",
        "            return []\n",
        "\n",
        "\n",
        "    def ingest_chunks(self, chunks: List[DocumentChunk]):\n",
        "        \"\"\"Ingest chunks into vector database and BM25 index.\"\"\"\n",
        "        print(\"🔄 Starting ingestion process...\")\n",
        "        print(f\"Attempting to ingest {len(chunks)} chunks.\")\n",
        "\n",
        "        points = []\n",
        "        newly_added_corpus = [] # Use a temporary list for current ingestion's corpus\n",
        "\n",
        "        for i, chunk in enumerate(chunks):\n",
        "            print(f\"Processing chunk {i+1}/{len(chunks)} (ID: {chunk.id}, Type: {chunk.source_type})\")\n",
        "            # Get embedding\n",
        "            embedding = get_gemini_embedding_with_retry(chunk.text, \"RETRIEVAL_DOCUMENT\")\n",
        "            if embedding is None:\n",
        "                print(f\"⚠️  Skipping chunk {chunk.id} - failed to get embedding\")\n",
        "                continue\n",
        "            print(f\"✅ Embedding generated for chunk {chunk.id}\")\n",
        "\n",
        "            # Prepare point for Qdrant\n",
        "            point = PointStruct(\n",
        "                id=len(self.chunks) + len(points), # Ensure unique IDs across multiple ingestions\n",
        "                vector=embedding,\n",
        "                payload={\n",
        "                    \"chunk_id\": chunk.id,\n",
        "                    \"text\": chunk.text,\n",
        "                    \"page_number\": chunk.page_number,\n",
        "                    \"source_type\": chunk.source_type,\n",
        "                    \"image_description\": chunk.image_description,\n",
        "                    \"has_image\": chunk.image_data is not None,\n",
        "                    \"metadata\": chunk.metadata or {}\n",
        "                }\n",
        "            )\n",
        "            points.append(point)\n",
        "            print(f\"✅ Point prepared for chunk {chunk.id}. Total points prepared: {len(points)}\")\n",
        "\n",
        "\n",
        "            # Prepare for BM25 using NLTK and stop words\n",
        "            try:\n",
        "                tokens = word_tokenize(chunk.text.lower())\n",
        "                filtered_tokens = [word for word in tokens if word.isalnum() and word not in stop_words]\n",
        "                newly_added_corpus.append(filtered_tokens)\n",
        "                print(f\"✅ Tokenized and filtered chunk {chunk.id} for BM25.\")\n",
        "            except Exception as e:\n",
        "                print(f\"❌ Error tokenizing/filtering chunk {chunk.id} for BM25: {e}\")\n",
        "                newly_added_corpus.append([]) # Add empty list to maintain corpus length\n",
        "\n",
        "\n",
        "            # Store chunk reference\n",
        "            self.chunks.extend(chunks) # Extend chunks with the new chunks\n",
        "            print(f\"Debug: self.chunks length after adding current chunks: {len(self.chunks)}\")\n",
        "\n",
        "\n",
        "        print(f\"Prepared {len(points)} points for upserting to Qdrant.\")\n",
        "        if points:\n",
        "            # Upsert to Qdrant\n",
        "            try:\n",
        "                response = self.qdrant_client.upsert(\n",
        "                    collection_name=CONFIG[\"collection_name\"],\n",
        "                    points=points,\n",
        "                    wait=True # Wait for the operation to complete\n",
        "                )\n",
        "                print(f\"✅ Qdrant upsert response: {response}\")\n",
        "                print(f\"✅ Upserted {len(points)} points to Qdrant.\")\n",
        "            except Exception as e:\n",
        "                print(f\"❌ Error during Qdrant upsert: {e}\")\n",
        "\n",
        "        else:\n",
        "            print(\"⚠️ No points to upsert to Qdrant.\")\n",
        "\n",
        "        # Accumulate the new corpus\n",
        "        self.bm25_corpus.extend(newly_added_corpus)\n",
        "\n",
        "        print(f\"Debug: Corpus content after adding newly added corpus: {self.bm25_corpus}, Type: {type(self.bm25_corpus)}\") # Added debug log\n",
        "        # Build BM25 index from the accumulated corpus\n",
        "        if self.bm25_corpus and any(self.bm25_corpus): # Check if corpus is not empty and contains non-empty token lists\n",
        "            self.bm25_index = BM25Okapi(self.bm25_corpus)\n",
        "            print(f\"✅ BM25 index built with {len(self.bm25_corpus)} documents.\")\n",
        "\n",
        "            # Save BM25 corpus to file\n",
        "            self.save_bm25_corpus() # Call the new method\n",
        "\n",
        "        else:\n",
        "            self.bm25_index = None # Ensure index is None if corpus is empty\n",
        "            self.bm25_corpus = []\n",
        "            print(\"⚠️ No corpus for building BM25 index.\")\n",
        "\n",
        "\n",
        "        print(f\"✅ Successfully processed {len(points)} chunks for ingestion\")\n",
        "\n",
        "\n",
        "    def ingest_document(self, doc_path: str):\n",
        "        \"\"\"Complete ingestion pipeline for a document (PDF or Markdown).\"\"\"\n",
        "        if doc_path.lower().endswith('.pdf'):\n",
        "            chunks = self.extract_pdf_content(doc_path)\n",
        "        elif doc_path.lower().endswith('.md'):\n",
        "            chunks = self.extract_markdown_content(doc_path)\n",
        "        else:\n",
        "            print(f\"❌ Unsupported file type: {doc_path}\")\n",
        "            return\n",
        "\n",
        "        self.ingest_chunks(chunks)\n",
        "        self.save_bm25_corpus() # Save the corpus after processing each document\n",
        "\n",
        "\n",
        "    def load_bm25_corpus(self, file_path: str):\n",
        "        \"\"\"Loads the tokenized BM25 corpus from a JSON file.\"\"\"\n",
        "        if os.path.exists(file_path):\n",
        "            try:\n",
        "                with open(file_path, \"r\") as f:\n",
        "                    loaded_corpus = json.load(f)\n",
        "                    if isinstance(loaded_corpus, list):\n",
        "                        self.bm25_corpus = loaded_corpus\n",
        "                    else:\n",
        "                        print(f\"⚠️ Loaded BM25 corpus from {file_path} is not a list. Initializing with empty corpus.\")\n",
        "                        self.bm25_corpus = []\n",
        "                print(f\"✅ BM25 corpus loaded from {file_path}\")\n",
        "            except Exception as e:\n",
        "                print(f\"❌ Error loading BM25 corpus from {file_path}: {e}\")\n",
        "                self.bm25_corpus = []\n",
        "        else:\n",
        "            print(f\"⚠️ BM25 corpus file not found at {file_path}. BM25 index will be built during ingestion.\")\n",
        "            self.bm25_corpus = []\n",
        "\n",
        "\n",
        "    def save_bm25_corpus(self):\n",
        "        \"\"\"Saves the tokenized BM25 corpus to a JSON file.\"\"\"\n",
        "        try:\n",
        "            with open(self.bm25_corpus_file, \"w\") as f:\n",
        "                json.dump(self.bm25_corpus, f)\n",
        "            print(f\"✅ BM25 corpus saved to {self.bm25_corpus_file}\")\n",
        "        except Exception as e:\n",
        "            print(f\"❌ Error saving BM25 corpus: {e}\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0c8d7fb0"
      },
      "source": [
        "**Reasoning**:\n",
        "Now that the `ingest_document` method correctly handles markdown files, attempt to ingest the sample markdown file again to verify the ingestion process works as expected.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f0486f74"
      },
      "source": [
        "class DocumentIngester:\n",
        "    def __init__(self, qdrant_client: QdrantClient):\n",
        "        self.qdrant_client = qdrant_client\n",
        "        self.chunks = []\n",
        "        self.bm25_corpus = []\n",
        "        self.bm25_index = None\n",
        "        self.bm25_corpus_file = \"bm25_calcom_corpus.json\"\n",
        "\n",
        "        # Delete collection if it exists (keep for fresh start in demo)\n",
        "        # try:\n",
        "        #     self.qdrant_client.delete_collection(collection_name=CONFIG[\"collection_name\"])\n",
        "        #     print(f\"✅ Deleted existing collection: {CONFIG['collection_name']}\")\n",
        "        # except Exception as e:\n",
        "        #     print(f\"Collection might not exist or error deleting: {e}\")\n",
        "\n",
        "\n",
        "        # Create collection if it doesn't exist\n",
        "        try:\n",
        "            self.qdrant_client.get_collection(collection_name=CONFIG[\"collection_name\"])\n",
        "            print(f\"✅ Collection '{CONFIG['collection_name']}' already exists.\")\n",
        "        except Exception: # Collection does not exist\n",
        "            try:\n",
        "                self.qdrant_client.create_collection(\n",
        "                    collection_name=CONFIG[\"collection_name\"],\n",
        "                    vectors_config=VectorParams(\n",
        "                        size=CONFIG[\"vector_dimension\"],\n",
        "                        distance=Distance.COSINE\n",
        "                    )\n",
        "                )\n",
        "                print(f\"✅ Created collection: {CONFIG['collection_name']}\")\n",
        "            except Exception as e:\n",
        "                print(f\"❌ Error creating collection: {e}\")\n",
        "\n",
        "        # Attempt to load BM25 corpus\n",
        "        self.load_bm25_corpus(self.bm25_corpus_file)\n",
        "\n",
        "        # Rebuild BM25 index if corpus was loaded\n",
        "        if self.bm25_corpus:\n",
        "            try:\n",
        "                self.bm25_index = BM25Okapi(self.bm25_corpus)\n",
        "                print(\"✅ BM25 index rebuilt from loaded corpus.\")\n",
        "            except Exception as e:\n",
        "                print(f\"❌ Error rebuilding BM25 index from loaded corpus: {e}\")\n",
        "                self.bm25_index = None\n",
        "\n",
        "    def describe_image_with_gemini(self, image_bytes: bytes) -> str:\n",
        "        \"\"\"Generate image description using Gemini Vision.\"\"\"\n",
        "        try:\n",
        "            # Convert bytes to PIL Image\n",
        "            image = Image.open(io.BytesIO(image_bytes))\n",
        "\n",
        "            # Initialize Gemini Vision model\n",
        "            # Ensure you have initialized genai with your API key before calling this function\n",
        "            model = genai.GenerativeModel('gemini-1.5-flash')\n",
        "\n",
        "            # prompt = \"\"\"Describe this image in detail. Focus on:\n",
        "            # 1. Main objects, people, or elements, or technical diagrams\n",
        "            # 2. Text content (if any)\n",
        "            # 3. Charts, diagrams, or technical content\n",
        "            # 4. Spatial relationships and layout\n",
        "            # 5. Any relevant contextual information\n",
        "\n",
        "            # Provide a comprehensive description that would help in document search.\"\"\"\n",
        "            prompt = \"\"\"\"Describe the technical diagrams and tables found in the provided document. For each description, follow these steps:\n",
        "\n",
        "            Purpose: Begin by stating the primary function or purpose of the diagram or table (e.g., 'This is a wiring diagram showing the electrical connections,' or 'This table provides the technical specifications for the product').\n",
        "\n",
        "            Components: Provide a detailed breakdown of the visual elements.\n",
        "\n",
        "            For diagrams: List and explain each labeled component, symbol, or annotation. Describe the relationships or processes shown by arrows or other visual cues.\n",
        "\n",
        "            For tables: Identify and explain the meaning of each column and row. Highlight the key data points, including any values and units of measurement.\n",
        "\n",
        "            Synthesis: Conclude with a summary of the most important information presented, explaining how the different parts of the diagram or table work together to convey a complete message.\"\"\"\n",
        "\n",
        "            response = model.generate_content([prompt, image])\n",
        "            return response.text\n",
        "        except Exception as e:\n",
        "            print(f\"Error describing image: {e}\")\n",
        "            return \"Image description unavailable\"\n",
        "\n",
        "    def extract_pdf_content(self, pdf_path: str) -> List[DocumentChunk]:\n",
        "        \"\"\"Extract text and images from PDF.\"\"\"\n",
        "        doc = fitz.open(pdf_path)\n",
        "        all_chunks = []\n",
        "\n",
        "        print(f\"📄 Processing PDF with {min(len(doc), 10)} pages (limited to first 10 for trial)...\") # Updated print statement\n",
        "\n",
        "        # Process only the first 10 pages\n",
        "        for page_num in range(len(doc)):\n",
        "            page = doc.load_page(page_num)\n",
        "            print(f\"Processing page {page_num + 1}...\")\n",
        "\n",
        "            # Extract text\n",
        "            text = page.get_text()\n",
        "            if text.strip():\n",
        "                print(f\"Extracted text from page {page_num + 1}. Text length: {len(text)}\")\n",
        "                # Chunk the text\n",
        "                text_chunks = extract_and_chunk_text(text, CONFIG[\"chunk_size\"], CONFIG[\"chunk_overlap\"])\n",
        "                print(f\"Split into {len(text_chunks)} text chunks on page {page_num + 1}\")\n",
        "                print(f\"Debug: text_chunks content for page {page_num + 1}: {text_chunks}, Type: {type(text_chunks)}\") # Added debug log\n",
        "\n",
        "\n",
        "                if text_chunks: # Add check here\n",
        "                    for i, chunk_text in enumerate(text_chunks):\n",
        "                        print(f\"Debug: Processing text chunk {i} on page {page_num + 1}: {chunk_text[:50]}...\") # Added debug log\n",
        "                        chunk_id = f\"page_{page_num}_chunk_{i}\"\n",
        "                        chunk = DocumentChunk(\n",
        "                            id=chunk_id,\n",
        "                            text=chunk_text,\n",
        "                            page_number=page_num,\n",
        "                            source_type=\"document\",\n",
        "                            metadata={\"pdf_path\": pdf_path}\n",
        "                        )\n",
        "                        all_chunks.append(chunk)\n",
        "                else:\n",
        "                    print(f\"⚠️  No text chunks generated for page {page_num + 1}\") # Added log\n",
        "\n",
        "\n",
        "            # Extract images\n",
        "            image_list = page.get_images()\n",
        "            print(f\"Found {len(image_list)} images on page {page_num + 1}\")\n",
        "            for img_index, img in enumerate(image_list):\n",
        "                try:\n",
        "                    # Get image data\n",
        "                    xref = img[0]\n",
        "                    pix = fitz.Pixmap(doc, xref)\n",
        "\n",
        "                    # Convert pixmap to RGB if it's not already in a compatible format\n",
        "                    if pix.n > 3: # CMYK or other formats\n",
        "                         pix = fitz.Pixmap(fitz.csRGB, pix)\n",
        "                    elif pix.n == 1: # Grayscale\n",
        "                         pix = fitz.Pixmap(fitz.csRGB, pix) # Convert grayscale to RGB\n",
        "\n",
        "\n",
        "                    img_data = pix.tobytes(\"png\")\n",
        "\n",
        "                    # Generate description\n",
        "                    print(f\"Describing image {img_index} on page {page_num + 1}...\")\n",
        "                    description = self.describe_image_with_gemini(img_data) # Use self.describe_image_with_gemini\n",
        "                    print(f\"Image description generated for page {page_num + 1}: {description[:50]}...\")\n",
        "\n",
        "                    # Create image chunk\n",
        "                    chunk_id = f\"page_{page_num}_image_{img_index}\"\n",
        "                    chunk = DocumentChunk(\n",
        "                        id=chunk_id,\n",
        "                        text=description,\n",
        "                        page_number=page_num,\n",
        "                        source_type=\"image\",\n",
        "                        image_description=description,\n",
        "                        image_data=img_data,\n",
        "                        metadata={\"pdf_path\": pdf_path}\n",
        "                    )\n",
        "                    all_chunks.append(chunk)\n",
        "                    print(f\"🖼️  Processed image {img_index} on page {page_num + 1}\")\n",
        "\n",
        "                    pix = None\n",
        "                except Exception as e:\n",
        "                    print(f\"Error processing image {img_index} on page {page_num}: {e}\")\n",
        "\n",
        "        doc.close()\n",
        "        print(f\"✅ Extracted {len(all_chunks)} chunks from PDF\")\n",
        "        return all_chunks\n",
        "\n",
        "    def extract_markdown_content(self, md_path: str) -> List[DocumentChunk]:\n",
        "        \"\"\"Extract text from markdown files and chunk it.\"\"\"\n",
        "        all_chunks = []\n",
        "        try:\n",
        "            with open(md_path, 'r', encoding='utf-8') as f:\n",
        "                markdown_text = f.read()\n",
        "\n",
        "            # Simple markdown cleaning (remove headers, lists, etc.) - can be improved\n",
        "            # This is a basic approach; a dedicated markdown parser would be better for complex docs\n",
        "            cleaned_text = []\n",
        "            for line in markdown_text.splitlines():\n",
        "                line = line.strip()\n",
        "                # Keep lines that are not headers, lists, blockquotes, or horizontal rules\n",
        "                if line and not line.startswith('#') and not line.startswith('- ') and not line.startswith('* ') and not line.startswith('> ') and not line.startswith('---') and not line.startswith('***') and not line.startswith('___'):\n",
        "                     cleaned_text.append(line)\n",
        "            cleaned_text = \" \".join(cleaned_text)\n",
        "\n",
        "\n",
        "            if cleaned_text.strip():\n",
        "                print(f\"Extracted text from markdown {md_path}. Text length: {len(cleaned_text)}\")\n",
        "                # Chunk the text\n",
        "                text_chunks = extract_and_chunk_text(cleaned_text, CONFIG[\"chunk_size\"], CONFIG[\"chunk_overlap\"])\n",
        "                print(f\"Split into {len(text_chunks)} text chunks from markdown\")\n",
        "\n",
        "                if text_chunks:\n",
        "                    for i, chunk_text in enumerate(text_chunks):\n",
        "                        chunk_id = f\"{os.path.basename(md_path)}_chunk_{i}\"\n",
        "                        # Markdown doesn't have pages, use 0 or a unique identifier\n",
        "                        chunk = DocumentChunk(\n",
        "                            id=chunk_id,\n",
        "                            text=chunk_text,\n",
        "                            page_number=0, # Or a different indicator for markdown\n",
        "                            source_type=\"markdown\",\n",
        "                            metadata={\"markdown_path\": md_path}\n",
        "                        )\n",
        "                        all_chunks.append(chunk)\n",
        "                else:\n",
        "                     print(f\"⚠️ No text chunks generated for markdown file {md_path}\")\n",
        "\n",
        "            print(f\"✅ Extracted {len(all_chunks)} chunks from markdown {md_path}\")\n",
        "            return all_chunks\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"❌ Error extracting content from markdown file {md_path}: {e}\")\n",
        "            return []\n",
        "\n",
        "\n",
        "    def ingest_chunks(self, chunks: List[DocumentChunk]):\n",
        "        \"\"\"Ingest chunks into vector database and BM25 index.\"\"\"\n",
        "        print(\"🔄 Starting ingestion process...\")\n",
        "        print(f\"Attempting to ingest {len(chunks)} chunks.\")\n",
        "\n",
        "        points = []\n",
        "        newly_added_corpus = [] # Use a temporary list for current ingestion's corpus\n",
        "\n",
        "        for i, chunk in enumerate(chunks):\n",
        "            print(f\"Processing chunk {i+1}/{len(chunks)} (ID: {chunk.id}, Type: {chunk.source_type})\")\n",
        "            # Get embedding\n",
        "            embedding = get_gemini_embedding_with_retry(chunk.text, \"RETRIEVAL_DOCUMENT\")\n",
        "            if embedding is None:\n",
        "                print(f\"⚠️  Skipping chunk {chunk.id} - failed to get embedding\")\n",
        "                continue\n",
        "            print(f\"✅ Embedding generated for chunk {chunk.id}\")\n",
        "\n",
        "            # Prepare point for Qdrant\n",
        "            point = PointStruct(\n",
        "                id=len(self.chunks) + len(points), # Ensure unique IDs across multiple ingestions\n",
        "                vector=embedding,\n",
        "                payload={\n",
        "                    \"chunk_id\": chunk.id,\n",
        "                    \"text\": chunk.text,\n",
        "                    \"page_number\": chunk.page_number,\n",
        "                    \"source_type\": chunk.source_type,\n",
        "                    \"image_description\": chunk.image_description,\n",
        "                    \"has_image\": chunk.image_data is not None,\n",
        "                    \"metadata\": chunk.metadata or {}\n",
        "                }\n",
        "            )\n",
        "            points.append(point)\n",
        "            print(f\"✅ Point prepared for chunk {chunk.id}. Total points prepared: {len(points)}\")\n",
        "\n",
        "\n",
        "            # Prepare for BM25 using NLTK and stop words\n",
        "            try:\n",
        "                tokens = word_tokenize(chunk.text.lower())\n",
        "                filtered_tokens = [word for word in tokens if word.isalnum() and word not in stop_words]\n",
        "                newly_added_corpus.append(filtered_tokens)\n",
        "                print(f\"✅ Tokenized and filtered chunk {chunk.id} for BM25.\")\n",
        "            except Exception as e:\n",
        "                print(f\"❌ Error tokenizing/filtering chunk {chunk.id} for BM25: {e}\")\n",
        "                newly_added_corpus.append([]) # Add empty list to maintain corpus length\n",
        "\n",
        "\n",
        "            # Store chunk reference\n",
        "            self.chunks.extend(chunks) # Extend chunks with the new chunks\n",
        "            print(f\"Debug: self.chunks length after adding current chunks: {len(self.chunks)}\")\n",
        "\n",
        "\n",
        "        print(f\"Prepared {len(points)} points for upserting to Qdrant.\")\n",
        "        if points:\n",
        "            # Upsert to Qdrant\n",
        "            try:\n",
        "                response = self.qdrant_client.upsert(\n",
        "                    collection_name=CONFIG[\"collection_name\"],\n",
        "                    points=points,\n",
        "                    wait=True # Wait for the operation to complete\n",
        "                )\n",
        "                print(f\"✅ Qdrant upsert response: {response}\")\n",
        "                print(f\"✅ Upserted {len(points)} points to Qdrant.\")\n",
        "            except Exception as e:\n",
        "                print(f\"❌ Error during Qdrant upsert: {e}\")\n",
        "\n",
        "        else:\n",
        "            print(\"⚠️ No points to upsert to Qdrant.\")\n",
        "\n",
        "        # Accumulate the new corpus\n",
        "        self.bm25_corpus.extend(newly_added_corpus)\n",
        "\n",
        "        print(f\"Debug: Corpus content after adding newly added corpus: {self.bm25_corpus}, Type: {type(self.bm25_corpus)}\") # Added debug log\n",
        "        # Build BM25 index from the accumulated corpus\n",
        "        if self.bm25_corpus and any(self.bm25_corpus): # Check if corpus is not empty and contains non-empty token lists\n",
        "            self.bm25_index = BM25Okapi(self.bm25_corpus)\n",
        "            print(f\"✅ BM25 index built with {len(self.bm25_corpus)} documents.\")\n",
        "\n",
        "            # Save BM25 corpus to file\n",
        "            self.save_bm25_corpus() # Call the new method\n",
        "\n",
        "        else:\n",
        "            self.bm25_index = None # Ensure index is None if corpus is empty\n",
        "            self.bm25_corpus = []\n",
        "            print(\"⚠️ No corpus for building BM25 index.\")\n",
        "\n",
        "\n",
        "        print(f\"✅ Successfully processed {len(points)} chunks for ingestion\")\n",
        "\n",
        "\n",
        "    def ingest_document(self, doc_paths: str | List[str]):\n",
        "        \"\"\"Complete ingestion pipeline for a document or list of documents (PDF or Markdown).\"\"\"\n",
        "        if isinstance(doc_paths, str):\n",
        "            doc_paths = [doc_paths]\n",
        "            print(f\"Processing single document: {doc_paths[0]}\")\n",
        "        elif isinstance(doc_paths, list):\n",
        "            print(f\"Processing {len(doc_paths)} documents from a list.\")\n",
        "        else:\n",
        "            print(f\"❌ Unsupported input type for ingestion: {type(doc_paths)}\")\n",
        "            return\n",
        "\n",
        "        all_chunks = []\n",
        "        for doc_path in doc_paths:\n",
        "            try:\n",
        "                if doc_path.lower().endswith('.pdf'):\n",
        "                    print(f\"Extracting content from PDF: {doc_path}\")\n",
        "                    chunks = self.extract_pdf_content(doc_path)\n",
        "                elif doc_path.lower().endswith('.md'):\n",
        "                    print(f\"Extracting content from Markdown: {doc_path}\")\n",
        "                    chunks = self.extract_markdown_content(doc_path)\n",
        "                else:\n",
        "                    print(f\"❌ Skipping unsupported file type: {doc_path}\")\n",
        "                    continue\n",
        "                all_chunks.extend(chunks)\n",
        "            except Exception as e:\n",
        "                print(f\"❌ Error processing document {doc_path}: {e}\")\n",
        "\n",
        "        if all_chunks:\n",
        "            self.ingest_chunks(all_chunks)\n",
        "            self.save_bm25_corpus() # Save the corpus after processing each ingestion batch\n",
        "        else:\n",
        "            print(\"⚠️ No chunks extracted from the provided document(s).\")\n",
        "\n",
        "\n",
        "    def load_bm25_corpus(self, file_path: str):\n",
        "        \"\"\"Loads the tokenized BM25 corpus from a JSON file.\"\"\"\n",
        "        if os.path.exists(file_path):\n",
        "            try:\n",
        "                with open(file_path, \"r\") as f:\n",
        "                    loaded_corpus = json.load(f)\n",
        "                    if isinstance(loaded_corpus, list):\n",
        "                        self.bm25_corpus = loaded_corpus\n",
        "                    else:\n",
        "                        print(f\"⚠️ Loaded BM25 corpus from {file_path} is not a list. Initializing with empty corpus.\")\n",
        "                        self.bm25_corpus = []\n",
        "                print(f\"✅ BM25 corpus loaded from {file_path}\")\n",
        "            except Exception as e:\n",
        "                print(f\"❌ Error loading BM25 corpus from {file_path}: {e}\")\n",
        "                self.bm25_corpus = []\n",
        "        else:\n",
        "            print(f\"⚠️ BM25 corpus file not found at {file_path}. BM25 index will be built during ingestion.\")\n",
        "            self.bm25_corpus = []\n",
        "\n",
        "\n",
        "    def save_bm25_corpus(self):\n",
        "        \"\"\"Saves the tokenized BM25 corpus to a JSON file.\"\"\"\n",
        "        try:\n",
        "            with open(self.bm25_corpus_file, \"w\") as f:\n",
        "                json.dump(self.bm25_corpus, f)\n",
        "            print(f\"✅ BM25 corpus saved to {self.bm25_corpus_file}\")\n",
        "        except Exception as e:\n",
        "            print(f\"❌ Error saving BM25 corpus: {e}\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d78c1219"
      },
      "source": [
        "**Reasoning**:\n",
        "Ingest the sample markdown file using the updated `ingest_document` method to test if it processes the markdown file correctly.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b4eed4b6"
      },
      "source": [
        "qdrant_client = create_qdrant_client()\n",
        "ingester = DocumentIngester(qdrant_client)\n",
        "\n",
        "markdown_file_path = \"/content/\"\n",
        "ingester.ingest_document(markdown_file_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "870bde88"
      },
      "source": [
        "## Refine chunking\n",
        "\n",
        "### Subtask:\n",
        "Review and potentially adjust the `extract_and_chunk_text` function or create a new one to ensure it effectively chunks markdown content while preserving relevant structure and associating images.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0a59f59d"
      },
      "source": [
        "**Reasoning**:\n",
        "Examine the current `extract_and_chunk_text` function and consider its suitability for the already cleaned markdown text. Determine if it's sufficient or if minor adjustments are needed for the current level of markdown handling. Based on this analysis, conclude whether the current approach is adequate.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0a01deab"
      },
      "source": [
        "## Enhance markdown image handling\n",
        "\n",
        "### Subtask:\n",
        "Implement a method within `DocumentIngester` to identify and process images referenced within markdown files. This might involve extracting image paths or data and generating descriptions similar to PDF image handling.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "556275d0"
      },
      "source": [
        "**Reasoning**:\n",
        "Implement the `_extract_markdown_images` method in the `DocumentIngester` class to extract image references from markdown text, process the images by generating descriptions, and modify `extract_markdown_content` to integrate this image processing.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a80323c0"
      },
      "source": [
        "import re\n",
        "import os\n",
        "from PIL import Image\n",
        "import io\n",
        "import fitz\n",
        "import json\n",
        "import uuid # Import the uuid library\n",
        "\n",
        "class DocumentIngester:\n",
        "    def __init__(self, qdrant_client: QdrantClient):\n",
        "        self.qdrant_client = qdrant_client\n",
        "        self.chunks = []\n",
        "        self.bm25_corpus = []\n",
        "        self.bm25_index = None\n",
        "        self.bm25_corpus_file = \"bm25_calcom_corpus.json\"\n",
        "\n",
        "        # Delete collection if it exists (keep for fresh start in demo)\n",
        "        # try:\n",
        "        #     self.qdrant_client.delete_collection(collection_name=CONFIG[\"collection_name\"])\n",
        "        #     print(f\"✅ Deleted existing collection: {CONFIG['collection_name']}\")\n",
        "        # except Exception as e:\n",
        "        #     print(f\"Collection might not exist or error deleting: {e}\")\n",
        "\n",
        "\n",
        "        # Create collection if it doesn't exist\n",
        "        try:\n",
        "            self.qdrant_client.get_collection(collection_name=CONFIG[\"collection_name\"])\n",
        "            print(f\"✅ Collection '{CONFIG['collection_name']}' already exists.\")\n",
        "        except Exception: # Collection does not exist\n",
        "            try:\n",
        "                self.qdrant_client.create_collection(\n",
        "                    collection_name=CONFIG[\"collection_name\"],\n",
        "                    vectors_config=VectorParams(\n",
        "                        size=CONFIG[\"vector_dimension\"],\n",
        "                        distance=Distance.COSINE\n",
        "                    )\n",
        "                )\n",
        "                print(f\"✅ Created collection: {CONFIG['collection_name']}\")\n",
        "            except Exception as e:\n",
        "                print(f\"❌ Error creating collection: {e}\")\n",
        "\n",
        "        # Attempt to load BM25 corpus\n",
        "        self.load_bm25_corpus(self.bm25_corpus_file)\n",
        "\n",
        "        # Rebuild BM25 index if corpus was loaded\n",
        "        if self.bm25_corpus:\n",
        "            try:\n",
        "                self.bm25_index = BM25Okapi(self.bm25_corpus)\n",
        "                print(\"✅ BM25 index rebuilt from loaded corpus.\")\n",
        "            except Exception as e:\n",
        "                print(f\"❌ Error rebuilding BM25 index from loaded corpus: {e}\")\n",
        "                self.bm25_index = None\n",
        "\n",
        "    def describe_image_with_gemini(self, image_bytes: bytes) -> str:\n",
        "        \"\"\"Generate image description using Gemini Vision.\"\"\"\n",
        "        try:\n",
        "            # Convert bytes to PIL Image\n",
        "            image = Image.open(io.BytesIO(image_bytes))\n",
        "\n",
        "            # Initialize Gemini Vision model\n",
        "            # Ensure you have initialized genai with your API key before calling this function\n",
        "            model = genai.GenerativeModel('gemini-1.5-flash')\n",
        "\n",
        "            # prompt = \"\"\"Describe this image in detail. Focus on:\n",
        "            # 1. Main objects, people, or elements, or technical diagrams\n",
        "            # 2. Text content (if any)\n",
        "            # 3. Charts, diagrams, or technical content\n",
        "            # 4. Spatial relationships and layout\n",
        "            # 5. Any relevant contextual information\n",
        "\n",
        "            # Provide a comprehensive description that would help in document search.\"\"\"\n",
        "            prompt = \"\"\"\"Describe the technical diagrams and tables found in the provided document. For each description, follow these steps:\n",
        "\n",
        "            Purpose: Begin by stating the primary function or purpose of the diagram or table (e.g., 'This is a wiring diagram showing the electrical connections,' or 'This table provides the technical specifications for the product').\n",
        "\n",
        "            Components: Provide a detailed breakdown of the visual elements.\n",
        "\n",
        "            For diagrams: List and explain each labeled component, symbol, or annotation. Describe the relationships or processes shown by arrows or other visual cues.\n",
        "\n",
        "            For tables: Identify and explain the meaning of each column and row. Highlight the key data points, including any values and units of measurement.\n",
        "\n",
        "            Synthesis: Conclude with a summary of the most important information presented, explaining how the different parts of the diagram or table work together to convey a complete message.\"\"\"\n",
        "\n",
        "            response = model.generate_content([prompt, image])\n",
        "            return response.text\n",
        "        except Exception as e:\n",
        "            print(f\"Error describing image: {e}\")\n",
        "            return \"Image description unavailable\"\n",
        "\n",
        "    def extract_pdf_content(self, pdf_path: str) -> List[DocumentChunk]:\n",
        "        \"\"\"Extract text and images from PDF.\"\"\"\n",
        "        doc = fitz.open(pdf_path)\n",
        "        all_chunks = []\n",
        "\n",
        "        print(f\"📄 Processing PDF with {min(len(doc), 10)} pages (limited to first 10 for trial)...\") # Updated print statement\n",
        "\n",
        "        # Process only the first 10 pages\n",
        "        for page_num in range(len(doc)):\n",
        "            page = doc.load_page(page_num)\n",
        "            print(f\"Processing page {page_num + 1}...\")\n",
        "\n",
        "            # Extract text\n",
        "            text = page.get_text()\n",
        "            if text.strip():\n",
        "                print(f\"Extracted text from page {page_num + 1}. Text length: {len(text)}\")\n",
        "                # Chunk the text\n",
        "                text_chunks = extract_and_chunk_text(text, CONFIG[\"chunk_size\"], CONFIG[\"chunk_overlap\"])\n",
        "                print(f\"Split into {len(text_chunks)} text chunks on page {page_num + 1}\")\n",
        "                print(f\"Debug: text_chunks content for page {page_num + 1}: {text_chunks}, Type: {type(text_chunks)}\") # Added debug log\n",
        "\n",
        "\n",
        "                if text_chunks: # Add check here\n",
        "                    for i, chunk_text in enumerate(text_chunks):\n",
        "                        print(f\"Debug: Processing text chunk {i} on page {page_num + 1}: {chunk_text[:50]}...\") # Added debug log\n",
        "                        chunk_id = f\"page_{page_num}_chunk_{i}\"\n",
        "                        chunk = DocumentChunk(\n",
        "                            id=chunk_id,\n",
        "                            text=chunk_text,\n",
        "                            page_number=page_num,\n",
        "                            source_type=\"document\",\n",
        "                            metadata={\"pdf_path\": pdf_path}\n",
        "                        )\n",
        "                        all_chunks.append(chunk)\n",
        "                else:\n",
        "                    print(f\"⚠️  No text chunks generated for page {page_num + 1}\") # Added log\n",
        "\n",
        "\n",
        "            # Extract images\n",
        "            image_list = page.get_images()\n",
        "            print(f\"Found {len(image_list)} images on page {page_num + 1}\")\n",
        "            for img_index, img in enumerate(image_list):\n",
        "                try:\n",
        "                    # Get image data\n",
        "                    xref = img[0]\n",
        "                    pix = fitz.Pixmap(doc, xref)\n",
        "\n",
        "                    # Convert pixmap to RGB if it's not already in a compatible format\n",
        "                    if pix.n > 3: # CMYK or other formats\n",
        "                         pix = fitz.Pixmap(fitz.csRGB, pix)\n",
        "                    elif pix.n == 1: # Grayscale\n",
        "                         pix = fitz.Pixmap(fitz.csRGB, pix) # Convert grayscale to RGB\n",
        "\n",
        "\n",
        "                    img_data = pix.tobytes(\"png\")\n",
        "\n",
        "                    # Generate description\n",
        "                    print(f\"Describing image {img_index} on page {page_num + 1}...\")\n",
        "                    description = self.describe_image_with_gemini(img_data) # Use self.describe_image_with_gemini\n",
        "                    print(f\"Image description generated for page {page_num + 1}: {description[:50]}...\")\n",
        "\n",
        "                    # Create image chunk\n",
        "                    chunk_id = f\"page_{page_num}_image_{img_index}\"\n",
        "                    chunk = DocumentChunk(\n",
        "                        id=chunk_id,\n",
        "                        text=description,\n",
        "                        page_number=page_num,\n",
        "                        source_type=\"image\",\n",
        "                        image_description=description,\n",
        "                        image_data=img_data,\n",
        "                        metadata={\"pdf_path\": pdf_path}\n",
        "                    )\n",
        "                    all_chunks.append(chunk)\n",
        "                    print(f\"🖼️  Processed image {img_index} on page {page_num + 1}\")\n",
        "\n",
        "                    pix = None\n",
        "                except Exception as e:\n",
        "                    print(f\"Error processing image {img_index} on page {page_num}: {e}\")\n",
        "\n",
        "        doc.close()\n",
        "        print(f\"✅ Extracted {len(all_chunks)} chunks from PDF\")\n",
        "        return all_chunks\n",
        "\n",
        "    def _extract_markdown_images(self, markdown_text: str, md_dir: str) -> List[Tuple[str, bytes, str]]:\n",
        "        \"\"\"Extract image data and descriptions from markdown image references.\"\"\"\n",
        "        images_info = []\n",
        "        # Regex to find markdown image references: ![alt text](image_path)\n",
        "        # It captures alt text (group 1) and image path (group 2)\n",
        "        image_pattern = re.compile(r'!\\[(.*?)\\]\\((.*?)\\)')\n",
        "\n",
        "        for match in image_pattern.finditer(markdown_text):\n",
        "            alt_text = match.group(1)\n",
        "            image_path = match.group(2)\n",
        "\n",
        "            # Construct absolute image path\n",
        "            # Handle potential absolute or relative paths\n",
        "            if os.path.isabs(image_path):\n",
        "                abs_image_path = image_path\n",
        "            else:\n",
        "                abs_image_path = os.path.join(md_dir, image_path)\n",
        "\n",
        "            print(f\"Found image reference: '{image_path}', attempting to load from '{abs_image_path}'\")\n",
        "\n",
        "            # Check if image file exists\n",
        "            if os.path.exists(abs_image_path):\n",
        "                try:\n",
        "                    # Read image data\n",
        "                    with open(abs_image_path, 'rb') as img_file:\n",
        "                        img_data = img_file.read()\n",
        "\n",
        "                    # Generate description\n",
        "                    description = self.describe_image_with_gemini(img_data)\n",
        "                    print(f\"Generated description for '{image_path}': {description[:50]}...\")\n",
        "\n",
        "                    images_info.append((abs_image_path, img_data, description))\n",
        "                    print(f\"✅ Processed image: {abs_image_path}\")\n",
        "\n",
        "                except Exception as e:\n",
        "                    print(f\"❌ Error processing image file '{abs_image_path}': {e}\")\n",
        "            else:\n",
        "                print(f\"⚠️ Image file not found: {abs_image_path}\")\n",
        "\n",
        "        return images_info\n",
        "\n",
        "\n",
        "    def extract_markdown_content(self, md_path: str) -> List[DocumentChunk]:\n",
        "        \"\"\"Extract text and images from markdown files and chunk it.\"\"\"\n",
        "        all_chunks = []\n",
        "        md_dir = os.path.dirname(md_path) # Get directory of the markdown file\n",
        "        try:\n",
        "            with open(md_path, 'r', encoding='utf-8') as f:\n",
        "                markdown_text = f.read()\n",
        "\n",
        "            # Extract images and generate descriptions first\n",
        "            images_info = self._extract_markdown_images(markdown_text, md_dir)\n",
        "\n",
        "            # Create image chunks\n",
        "            for i, (img_path, img_data, description) in enumerate(images_info):\n",
        "                chunk_id = f\"{os.path.basename(md_path)}_image_{i}\"\n",
        "                image_chunk = DocumentChunk(\n",
        "                    id=chunk_id,\n",
        "                    text=description, # Use description as the text for the chunk\n",
        "                    page_number=0, # Markdown doesn't have pages, use 0 or a unique identifier\n",
        "                    source_type=\"image\",\n",
        "                    image_description=description,\n",
        "                    image_data=img_data, # Store image data in the chunk\n",
        "                    metadata={\"markdown_path\": md_path, \"image_path\": img_path}\n",
        "                )\n",
        "                all_chunks.append(image_chunk)\n",
        "                print(f\"🖼️  Created image chunk for {os.path.basename(img_path)}\")\n",
        "\n",
        "\n",
        "            # Simple markdown cleaning (remove headers, lists, etc.) - can be improved\n",
        "            # This is a basic approach; a dedicated markdown parser would be better for complex docs\n",
        "            cleaned_text = []\n",
        "            for line in markdown_text.splitlines():\n",
        "                line = line.strip()\n",
        "                # Keep lines that are not headers, lists, blockquotes, or horizontal rules\n",
        "                if line and not line.startswith('#') and not line.startswith('- ') and not line.startswith('* ') and not line.startswith('> ') and not line.startswith('---') and not line.startswith('***') and not line.startswith('___'):\n",
        "                     cleaned_text.append(line)\n",
        "            cleaned_text = \" \".join(cleaned_text)\n",
        "\n",
        "\n",
        "            if cleaned_text.strip():\n",
        "                print(f\"Extracted text from markdown {md_path}. Text length: {len(cleaned_text)}\")\n",
        "                # Chunk the text\n",
        "                text_chunks = extract_and_chunk_text(cleaned_text, CONFIG[\"chunk_size\"], CONFIG[\"chunk_overlap\"])\n",
        "                print(f\"Split into {len(text_chunks)} text chunks from markdown\")\n",
        "\n",
        "                if text_chunks:\n",
        "                    for i, chunk_text in enumerate(text_chunks):\n",
        "                        chunk_id = f\"{os.path.basename(md_path)}_chunk_{i}\"\n",
        "                        # Markdown doesn't have pages, use 0 or a different indicator for markdown\n",
        "                        chunk = DocumentChunk(\n",
        "                            id=chunk_id,\n",
        "                            text=chunk_text,\n",
        "                            page_number=0, # Or a different indicator for markdown\n",
        "                            source_type=\"markdown\",\n",
        "                            metadata={\"markdown_path\": md_path}\n",
        "                        )\n",
        "                        all_chunks.append(chunk)\n",
        "                else:\n",
        "                     print(f\"⚠️ No text chunks generated for markdown file {md_path}\")\n",
        "\n",
        "            print(f\"✅ Extracted {len(all_chunks)} chunks (text and image) from markdown {md_path}\")\n",
        "            return all_chunks\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"❌ Error extracting content from markdown file {md_path}: {e}\")\n",
        "            return []\n",
        "\n",
        "\n",
        "    def ingest_chunks(self, chunks: List[DocumentChunk]):\n",
        "        \"\"\"Ingest chunks into vector database and BM25 index.\"\"\"\n",
        "        print(\"🔄 Starting ingestion process...\")\n",
        "        print(f\"Attempting to ingest {len(chunks)} chunks.\")\n",
        "\n",
        "        points = []\n",
        "        newly_added_corpus = [] # Use a temporary list for current ingestion's corpus\n",
        "\n",
        "        for i, chunk in enumerate(chunks):\n",
        "            print(f\"Processing chunk {i+1}/{len(chunks)} (ID: {chunk.id}, Type: {chunk.source_type})\")\n",
        "            # Get embedding\n",
        "            embedding = get_gemini_embedding_with_retry(chunk.text, \"RETRIEVAL_DOCUMENT\")\n",
        "            if embedding is None:\n",
        "                print(f\"⚠️  Skipping chunk {chunk.id} - failed to get embedding\")\n",
        "                continue\n",
        "            print(f\"✅ Embedding generated for chunk {chunk.id}\")\n",
        "\n",
        "            # Prepare point for Qdrant\n",
        "            # Generate a UUID for each point to ensure a valid and unique ID format for Qdrant\n",
        "            unique_point_id = str(uuid.uuid4())\n",
        "\n",
        "\n",
        "            point = PointStruct(\n",
        "                id=unique_point_id, # Ensure unique IDs for each chunk across different ingestions\n",
        "                vector=embedding,\n",
        "                payload={\n",
        "                    \"chunk_id\": chunk.id,\n",
        "                    \"text\": chunk.text,\n",
        "                    \"page_number\": chunk.page_number,\n",
        "                    \"source_type\": chunk.source_type,\n",
        "                    \"image_description\": chunk.image_description,\n",
        "                    \"has_image\": chunk.image_data is not None,\n",
        "                    \"metadata\": chunk.metadata or {}\n",
        "                }\n",
        "            )\n",
        "            points.append(point)\n",
        "            print(f\"✅ Point prepared for chunk {chunk.id}. Total points prepared: {len(points)}\")\n",
        "\n",
        "\n",
        "            # Prepare for BM25 using NLTK and stop words\n",
        "            try:\n",
        "                tokens = word_tokenize(chunk.text.lower())\n",
        "                filtered_tokens = [word for word in tokens if word.isalnum() and word not in stop_words]\n",
        "                newly_added_corpus.append(filtered_tokens)\n",
        "                print(f\"✅ Tokenized and filtered chunk {chunk.id} for BM25.\")\n",
        "            except Exception as e:\n",
        "                print(f\"❌ Error tokenizing/filtering chunk {chunk.id} for BM25: {e}\")\n",
        "                newly_added_corpus.append([]) # Add empty list to maintain corpus length\n",
        "\n",
        "\n",
        "            # Store chunk reference\n",
        "            # We need to store chunks globally or in the ingester instance\n",
        "            # to be able to map BM25 results back to chunk objects later in the search engine.\n",
        "            # Appending to self.chunks here will accumulate chunks across ingestion calls.\n",
        "            self.chunks.append(chunk) # Append the chunk object\n",
        "            # print(f\"Debug: self.chunks length after adding current chunk: {len(self.chunks)}\") # Removed verbose debug\n",
        "\n",
        "\n",
        "        print(f\"Prepared {len(points)} points for upserting to Qdrant.\")\n",
        "        if points:\n",
        "            # Upsert to Qdrant\n",
        "            try:\n",
        "                response = self.qdrant_client.upsert(\n",
        "                    collection_name=CONFIG[\"collection_name\"],\n",
        "                    points=points,\n",
        "                    wait=True # Wait for the operation to complete\n",
        "                )\n",
        "                print(f\"✅ Qdrant upsert response: {response}\")\n",
        "                print(f\"✅ Upserted {len(points)} points to Qdrant.\")\n",
        "            except Exception as e:\n",
        "                print(f\"❌ Error during Qdrant upsert: {e}\")\n",
        "\n",
        "        else:\n",
        "            print(\"⚠️ No points to upsert to Qdrant.\")\n",
        "\n",
        "        # Accumulate the new corpus\n",
        "        self.bm25_corpus.extend(newly_added_corpus)\n",
        "\n",
        "        # print(f\"Debug: Corpus content after adding newly added corpus: {self.bm25_corpus}, Type: {type(self.bm25_corpus)}\") # Removed verbose debug\n",
        "        # Build BM25 index from the accumulated corpus\n",
        "        if self.bm25_corpus and any(self.bm25_corpus): # Check if corpus is not empty and contains non-empty token lists\n",
        "            self.bm25_index = BM25Okapi(self.bm25_corpus)\n",
        "            print(f\"✅ BM25 index built with {len(self.bm25_corpus)} documents.\")\n",
        "\n",
        "            # Save BM25 corpus to file\n",
        "            self.save_bm25_corpus() # Call the new method\n",
        "\n",
        "        else:\n",
        "            self.bm25_index = None # Ensure index is None if corpus is empty\n",
        "            self.bm25_corpus = []\n",
        "            print(\"⚠️ No corpus for building BM25 index.\")\n",
        "\n",
        "\n",
        "        print(f\"✅ Successfully processed {len(points)} chunks for ingestion\")\n",
        "\n",
        "\n",
        "    def ingest_document(self, doc_paths: str | List[str]):\n",
        "        \"\"\"Complete ingestion pipeline for a document or list of documents (PDF or Markdown).\"\"\"\n",
        "        if isinstance(doc_paths, str):\n",
        "            doc_paths = [doc_paths]\n",
        "            print(f\"Processing single document: {doc_paths[0]}\")\n",
        "        elif isinstance(doc_paths, list):\n",
        "            print(f\"Processing {len(doc_paths)} documents from a list.\")\n",
        "        else:\n",
        "            print(f\"❌ Unsupported input type for ingestion: {type(doc_paths)}\")\n",
        "            return\n",
        "\n",
        "        all_chunks = []\n",
        "        for doc_path in doc_paths:\n",
        "            try:\n",
        "                if doc_path.lower().endswith('.pdf'):\n",
        "                    print(f\"Extracting content from PDF: {doc_path}\")\n",
        "                    chunks = self.extract_pdf_content(doc_path)\n",
        "                elif doc_path.lower().endswith('.md'):\n",
        "                    print(f\"Extracting content from Markdown: {doc_path}\")\n",
        "                    chunks = self.extract_markdown_content(doc_path)\n",
        "                else:\n",
        "                    print(f\"❌ Skipping unsupported file type: {doc_path}\")\n",
        "                    continue\n",
        "                all_chunks.extend(chunks)\n",
        "            except Exception as e:\n",
        "                print(f\"❌ Error processing document {doc_path}: {e}\")\n",
        "\n",
        "        if all_chunks:\n",
        "            # Load existing BM25 corpus before ingesting new chunks\n",
        "            self.load_bm25_corpus(self.bm25_corpus_file)\n",
        "            self.ingest_chunks(all_chunks)\n",
        "            self.save_bm25_corpus() # Save the updated corpus after ingestion\n",
        "        else:\n",
        "            print(\"⚠️ No chunks extracted from the provided document(s).\")\n",
        "\n",
        "\n",
        "    def load_bm25_corpus(self, file_path: str):\n",
        "        \"\"\"Loads the tokenized BM25 corpus from a JSON file.\"\"\"\n",
        "        if os.path.exists(file_path):\n",
        "            try:\n",
        "                with open(file_path, \"r\") as f:\n",
        "                    loaded_corpus = json.load(f)\n",
        "                    if isinstance(loaded_corpus, list):\n",
        "                        self.bm25_corpus = loaded_corpus\n",
        "                    else:\n",
        "                        print(f\"⚠️ Loaded BM25 corpus from {file_path} is not a list. Initializing with empty corpus.\")\n",
        "                        self.bm25_corpus = []\n",
        "                print(f\"✅ BM25 corpus loaded from {file_path}\")\n",
        "            except Exception as e:\n",
        "                print(f\"❌ Error loading BM25 corpus from {file_path}: {e}\")\n",
        "                self.bm25_corpus = []\n",
        "        else:\n",
        "            print(f\"⚠️ BM25 corpus file not found at {file_path}. BM25 index will be built during ingestion.\")\n",
        "            self.bm25_corpus = []\n",
        "\n",
        "\n",
        "    def save_bm25_corpus(self):\n",
        "        \"\"\"Saves the tokenized BM25 corpus to a JSON file.\"\"\"\n",
        "        try:\n",
        "            with open(self.bm25_corpus_file, \"w\") as f:\n",
        "                json.dump(self.bm25_corpus, f)\n",
        "            print(f\"✅ BM25 corpus saved to {self.bm25_corpus_file}\")\n",
        "        except Exception as e:\n",
        "            print(f\"❌ Error saving BM25 corpus: {e}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "59ac7d5d"
      },
      "source": [
        "## Update file selection interface\n",
        "\n",
        "### Subtask:\n",
        "Modify the file upload interface to allow selecting a folder instead of just individual files.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dae926b3"
      },
      "source": [
        "**Reasoning**:\n",
        "Modify the existing file upload interface to allow multiple file selection and update the callback function to handle a list of uploaded files.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 353,
          "referenced_widgets": [
            "f65efc21c32244d1a9e57bf469d22466",
            "0d09532573d54da5bfbd5cb327882b6e",
            "be66be03bd754782bca7fb001e7971b0"
          ]
        },
        "id": "a7816f25",
        "outputId": "01132650-8072-4038-caa7-9c8259bcb714"
      },
      "source": [
        "def create_upload_interface():\n",
        "    \"\"\"Create file upload interface allowing multiple file selection.\"\"\"\n",
        "    print(\"📁 Upload PDF or Markdown documents (select multiple files):\")\n",
        "\n",
        "    upload_button = widgets.FileUpload(\n",
        "        accept='.pdf,.md', # Accept both PDF and Markdown files\n",
        "        multiple=True,    # Allow multiple file selection\n",
        "        description='Choose Files'\n",
        "    )\n",
        "\n",
        "    def on_upload_change(change):\n",
        "        if change['new']:\n",
        "            uploaded_files_info = change['new']\n",
        "            uploaded_file_paths = []\n",
        "            for filename, file_info in uploaded_files_info.items():\n",
        "                # Save each uploaded file\n",
        "                with open(filename, 'wb') as f:\n",
        "                    f.write(file_info['content'])\n",
        "                print(f\"✅ Uploaded: {filename}\")\n",
        "                uploaded_file_paths.append(filename)\n",
        "\n",
        "            print(f\"Processing {len(uploaded_file_paths)} files...\")\n",
        "            # Return the list of file paths\n",
        "            return uploaded_file_paths\n",
        "\n",
        "\n",
        "    upload_button.observe(on_upload_change, names='value')\n",
        "    display(upload_button)\n",
        "\n",
        "    return upload_button\n",
        "\n",
        "# Update the main execution block to use the modified interface\n",
        "if __name__ == \"__main__\":\n",
        "    # Run the demo\n",
        "    print(\"🚀 Starting Hybrid Multimodal Search Demo\")\n",
        "    print(\"=\" * 50)\n",
        "    qdrant_client = create_qdrant_client()\n",
        "    # Initialize system\n",
        "    ingester = DocumentIngester(qdrant_client)\n",
        "    # search_engine = HybridSearchEngine(qdrant_client, ingester) # SearchEngine initialization moved after ingestion\n",
        "\n",
        "    # Create upload interface\n",
        "    upload_widget = create_upload_interface()\n",
        "\n",
        "    # Note: The actual ingestion triggered by the upload happens within the on_upload_change callback.\n",
        "    # We need a way to signal when upload is complete and then initialize the search engine.\n",
        "    # For this simple demo, the user will need to manually run the next cell after uploading files.\n",
        "\n",
        "    print(\"\"\"\n",
        "📋 INSTRUCTIONS:\n",
        "1. Upload PDF and/or Markdown files using the file picker above (select multiple files).\n",
        "2. Wait for processing to complete (watch the output).\n",
        "3. After uploading and processing, run the *next* code cell to initialize the search engine.\n",
        "4. Then, run searches using the search_engine.hybrid_search() function.\n",
        "\n",
        "Example usage:\n",
        "search_engine.hybrid_search(\"your query here\")\n",
        "search_engine.display_results(results)\n",
        "\"\"\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🚀 Starting Hybrid Multimodal Search Demo\n",
            "==================================================\n",
            "✅ Qdrant client initialized successfully!\n",
            "✅ Collection 'calcom_help_docs' already exists.\n",
            "⚠️ BM25 corpus file not found at bm25_calcom_corpus.json. BM25 index will be built during ingestion.\n",
            "📁 Upload PDF or Markdown documents (select multiple files):\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "FileUpload(value={}, accept='.pdf,.md', description='Choose Files', multiple=True)"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f65efc21c32244d1a9e57bf469d22466"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "📋 INSTRUCTIONS:\n",
            "1. Upload PDF and/or Markdown files using the file picker above (select multiple files).\n",
            "2. Wait for processing to complete (watch the output).\n",
            "3. After uploading and processing, run the *next* code cell to initialize the search engine.\n",
            "4. Then, run searches using the search_engine.hybrid_search() function.\n",
            "\n",
            "Example usage:\n",
            "search_engine.hybrid_search(\"your query here\")\n",
            "search_engine.display_results(results)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5826ddfb"
      },
      "source": [
        "## Testing\n",
        "\n",
        "### Subtask:\n",
        "Test the updated ingestion process with a sample markdown file containing images and with a folder containing multiple markdown files to ensure chunks and image information are created correctly and embeddings are generated.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8dcb5d19"
      },
      "source": [
        "**Reasoning**:\n",
        "Create a sample markdown file that includes references to images, create sample image files, create a directory for testing, place the files in the directory, create another markdown file without images, initialize the Qdrant client and DocumentIngester, and call the ingest_document method with a list of the markdown file paths.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0b95faf3",
        "outputId": "97021df5-2ba7-44f1-8192-7108b4a5164b"
      },
      "source": [
        "import os\n",
        "from PIL import Image\n",
        "\n",
        "\n",
        "# 6. Initialize the Qdrant client and DocumentIngester\n",
        "qdrant_client = create_qdrant_client()\n",
        "ingester = DocumentIngester(qdrant_client)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Qdrant client initialized successfully!\n",
            "✅ Collection 'calcom_help_docs' already exists.\n",
            "⚠️ BM25 corpus file not found at bm25_calcom_corpus.json. BM25 index will be built during ingestion.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ff05279"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous command failed because the markdown file was not written before attempting to move it. The markdown content should be written to the file first, and then the file should be moved to the test directory along with the images. The code needs to be fixed to write the markdown file before moving it.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "95cee2ad"
      },
      "source": [
        "## Update demo workflow\n",
        "\n",
        "### Subtask:\n",
        "Adjust the `run_demo` function to utilize the updated file selection interface.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b0e8a4c1"
      },
      "source": [
        "**Reasoning**:\n",
        "Modify the `run_demo` function to use the updated `create_upload_interface` and adjust the user instructions in the main execution block.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 402,
          "referenced_widgets": [
            "70c2b8e136fc435d8fc013d79c4013ef",
            "2aaf2d22c83e416e930cd74e261de936",
            "b29d4b667301420a83707574cfdbf9e5",
            "8335f47a98384b3b84ddd2cf55041a64",
            "907654fdf8274042b11ab5f60e5cf052",
            "7141585f94ed4fcc8302051072f2738c",
            "c8f8529c0b9945628f1ae6a20e0041c2",
            "cabdb67687224e71b3d2d6daf638f7d0"
          ]
        },
        "id": "1be42e26",
        "outputId": "5f86ae10-9dff-4d46-eb0f-30536808fd15"
      },
      "source": [
        "def create_upload_interface_and_button():\n",
        "    \"\"\"Create file upload interface and a button to trigger ingestion.\"\"\"\n",
        "    print(\"📁 Upload PDF or Markdown documents (select multiple files):\")\n",
        "\n",
        "    upload_button = widgets.FileUpload(\n",
        "        accept='.pdf,.md', # Accept both PDF and Markdown files\n",
        "        multiple=True,    # Allow multiple file selection\n",
        "        description='Choose Files'\n",
        "    )\n",
        "\n",
        "    ingest_button = widgets.Button(\n",
        "        description='Start Ingestion',\n",
        "        disabled=True, # Disable initially until files are uploaded\n",
        "        button_style='', # 'success', 'info', 'warning', 'danger' or ''\n",
        "        tooltip='Click to start ingestion of selected files'\n",
        "    )\n",
        "\n",
        "    output_area = widgets.Output() # Area to display messages\n",
        "\n",
        "\n",
        "    def on_upload_change(change):\n",
        "        with output_area:\n",
        "            print(\"Debug: on_upload_change triggered\") # Debug print\n",
        "            if change['new']:\n",
        "                print(\"Debug: 'change[new]' is not empty. Enabling Ingest button.\") # Debug print\n",
        "                # Files have been selected, enable the ingest button\n",
        "                ingest_button.disabled = False\n",
        "                # Clear previous output area content related to upload change\n",
        "                output_area.clear_output(wait=True)\n",
        "                print(f\"✅ {len(change['new'])} file(s) selected. Click 'Start Ingestion' to proceed.\")\n",
        "            else:\n",
        "                print(\"Debug: 'change[new]' is empty. Disabling Ingest button.\") # Debug print\n",
        "                # No files selected, disable the ingest button\n",
        "                ingest_button.disabled = True\n",
        "                output_area.clear_output(wait=True)\n",
        "                print(\"Please select file(s) to enable ingestion.\")\n",
        "\n",
        "\n",
        "    def on_ingest_button_click(b):\n",
        "        with output_area:\n",
        "            print(\"Debug: on_ingest_button_click triggered\") # Debug print\n",
        "            output_area.clear_output(wait=True) # Clear previous output\n",
        "            print(\"🔄 Starting ingestion process...\")\n",
        "            uploaded_files_info = upload_button.value\n",
        "            uploaded_file_paths = []\n",
        "            current_dir = os.getcwd() # Get the current working directory (which is /content/ in Colab)\n",
        "\n",
        "            if not uploaded_files_info:\n",
        "                print(\"⚠️ No files selected for ingestion.\")\n",
        "                return\n",
        "\n",
        "            # Save files to the current directory and collect paths\n",
        "            for filename, file_info in uploaded_files_info.items():\n",
        "                file_path = os.path.join(current_dir, filename)\n",
        "                try:\n",
        "                    with open(file_path, 'wb') as f:\n",
        "                        f.write(file_info['content'])\n",
        "                    print(f\"✅ Uploading and saving: {file_path}\")\n",
        "                    uploaded_file_paths.append(file_path)\n",
        "                except Exception as e:\n",
        "                    print(f\"❌ Error saving file {filename}: {e}\")\n",
        "\n",
        "            print(f\"Processing {len(uploaded_file_paths)} files...\")\n",
        "            # Trigger ingestion with the list of successfully uploaded file paths\n",
        "            if uploaded_file_paths:\n",
        "                print(\"Debug: uploaded_file_paths is not empty. Calling ingester.ingest_document.\") # Debug print\n",
        "                try:\n",
        "                    # Assuming 'ingester' is accessible in this scope (it is if run after run_demo)\n",
        "                    ingester.ingest_document(uploaded_file_paths)\n",
        "                    print(\"✅ Ingestion complete for uploaded files.\")\n",
        "                    # Note: Re-initializing the search engine after ingestion is recommended\n",
        "                    # but for this demo structure, the user will be instructed to do so manually\n",
        "                    # after the ingestion callback finishes.\n",
        "                    print(\"\\nReady to re-initialize the search engine. Please run the next code cell.\")\n",
        "                except NameError:\n",
        "                    print(\"❌ Ingester not initialized. Please run the setup cells first.\")\n",
        "                except Exception as e:\n",
        "                    print(f\"❌ Error during ingestion of uploaded files: {e}\")\n",
        "            else:\n",
        "                print(\"⚠️ No files were successfully uploaded for ingestion.\")\n",
        "\n",
        "            # Disable the ingest button after processing\n",
        "            ingest_button.disabled = True\n",
        "\n",
        "\n",
        "    upload_button.observe(on_upload_change, names='value')\n",
        "    ingest_button.on_click(on_ingest_button_click)\n",
        "\n",
        "    display(upload_button, ingest_button, output_area)\n",
        "\n",
        "    return upload_button, ingest_button, output_area\n",
        "\n",
        "\n",
        "# Update the main execution block to use the modified interface\n",
        "if __name__ == \"__main__\":\n",
        "    # Run the demo\n",
        "    print(\"🚀 Starting Hybrid Multimodal Search Demo\")\n",
        "    print(\"=\" * 50)\n",
        "    qdrant_client = create_qdrant_client()\n",
        "    # Initialize system\n",
        "    # ingester needs to be in a scope accessible by the upload widget's callback\n",
        "    # Let's define it here before creating the widget\n",
        "    ingester = DocumentIngester(qdrant_client)\n",
        "\n",
        "\n",
        "    # Create upload interface and button\n",
        "    upload_widget, ingest_button, output_area = create_upload_interface_and_button()\n",
        "\n",
        "    # Note: The actual ingestion is now triggered by clicking the \"Start Ingestion\" button.\n",
        "    # We instruct the user to re-initialize the search engine manually after ingestion.\n",
        "\n",
        "    print(\"\"\"\n",
        "📋 INSTRUCTIONS:\n",
        "1. Use the 'Choose Files' button above to select PDF and/or Markdown files (select multiple files).\n",
        "2. After selecting files, click the 'Start Ingestion' button to begin processing.\n",
        "3. Wait for processing and ingestion to complete for all uploaded files (watch the output above the button).\n",
        "4. After ingestion, run the code cell below this one to re-initialize the search engine with the newly ingested data.\n",
        "5. Then, run searches using the search_engine.hybrid_search() function.\n",
        "\n",
        "Example usage:\n",
        "search_engine.hybrid_search(\"your query here\")\n",
        "search_engine.display_results(results)\n",
        "\"\"\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🚀 Starting Hybrid Multimodal Search Demo\n",
            "==================================================\n",
            "✅ Qdrant client initialized successfully!\n",
            "✅ Collection 'calcom_help_docs' already exists.\n",
            "⚠️ BM25 corpus file not found at bm25_calcom_corpus.json. BM25 index will be built during ingestion.\n",
            "📁 Upload PDF or Markdown documents (select multiple files):\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "FileUpload(value={}, accept='.pdf,.md', description='Choose Files', multiple=True)"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "70c2b8e136fc435d8fc013d79c4013ef"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Button(description='Start Ingestion', disabled=True, style=ButtonStyle(), tooltip='Click to start ingestion of…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8335f47a98384b3b84ddd2cf55041a64"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Output()"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c8f8529c0b9945628f1ae6a20e0041c2"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "📋 INSTRUCTIONS:\n",
            "1. Use the 'Choose Files' button above to select PDF and/or Markdown files (select multiple files).\n",
            "2. After selecting files, click the 'Start Ingestion' button to begin processing.\n",
            "3. Wait for processing and ingestion to complete for all uploaded files (watch the output above the button).\n",
            "4. After ingestion, run the code cell below this one to re-initialize the search engine with the newly ingested data.\n",
            "5. Then, run searches using the search_engine.hybrid_search() function.\n",
            "\n",
            "Example usage:\n",
            "search_engine.hybrid_search(\"your query here\")\n",
            "search_engine.display_results(results)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "da221c0f"
      },
      "source": [
        "# Task\n",
        "Implement batch ingestion for a large number of Markdown files located in a user-selected folder, ensuring the process handles potential runtime disconnections and provides progress feedback."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ec717387"
      },
      "source": [
        "## Modify ingestion logic for batching\n",
        "\n",
        "### Subtask:\n",
        "Update the `on_ingest_button_click` function to divide the list of uploaded file paths into smaller batches.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1edb4e97"
      },
      "source": [
        "**Reasoning**:\n",
        "Update the `on_ingest_button_click` function to implement batch processing for uploaded files, including defining a batch size, iterating through batches, slicing file paths for each batch, calling `ingest_document` with the batch, and adding a time delay.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 402,
          "referenced_widgets": [
            "c71e4c547d8744bfaac76426e8c32656",
            "29edaeda177d4b92bad00b631bcaa3e7",
            "efec971084ad4e2fb5bcf42f22d442c8",
            "f32e3b4835114e9e87c32aa046f52893",
            "66c75cd36d9944a694b6dc508f596909",
            "8e71e6b2be6246168b1fb804882c5535",
            "4b097d69c2b5444a80d9b2c0669d2ac6",
            "81279f304fac441b97f03448c30a8f7e"
          ]
        },
        "id": "79eefad1",
        "outputId": "943aff70-b3fa-4356-e1b5-45402c7de2ec"
      },
      "source": [
        "def create_upload_interface_and_button():\n",
        "    \"\"\"Create file upload interface and a button to trigger ingestion.\"\"\"\n",
        "    print(\"📁 Upload PDF or Markdown documents (select multiple files):\")\n",
        "\n",
        "    upload_button = widgets.FileUpload(\n",
        "        accept='.pdf,.md', # Accept both PDF and Markdown files\n",
        "        multiple=True,    # Allow multiple file selection\n",
        "        description='Choose Files'\n",
        "    )\n",
        "\n",
        "    ingest_button = widgets.Button(\n",
        "        description='Start Ingestion',\n",
        "        disabled=True, # Disable initially until files are uploaded\n",
        "        button_style='', # 'success', 'info', 'warning', 'danger' or ''\n",
        "        tooltip='Click to start ingestion of selected files'\n",
        "    )\n",
        "\n",
        "    output_area = widgets.Output() # Area to display messages\n",
        "\n",
        "\n",
        "    def on_upload_change(change):\n",
        "        with output_area:\n",
        "            print(\"Debug: on_upload_change triggered\") # Debug print\n",
        "            if change['new']:\n",
        "                print(\"Debug: 'change[new]' is not empty. Enabling Ingest button.\") # Debug print\n",
        "                # Files have been selected, enable the ingest button\n",
        "                ingest_button.disabled = False\n",
        "                # Clear previous output area content related to upload change\n",
        "                output_area.clear_output(wait=True)\n",
        "                print(f\"✅ {len(change['new'])} file(s) selected. Click 'Start Ingestion' to proceed.\")\n",
        "            else:\n",
        "                print(\"Debug: 'change[new]' is empty. Disabling Ingest button.\") # Debug print\n",
        "                # No files selected, disable the ingest button\n",
        "                ingest_button.disabled = True\n",
        "                output_area.clear_output(wait=True)\n",
        "                print(\"Please select file(s) to enable ingestion.\")\n",
        "\n",
        "\n",
        "    def on_ingest_button_click(b):\n",
        "        with output_area:\n",
        "            print(\"Debug: on_ingest_button_click triggered\") # Debug print\n",
        "            output_area.clear_output(wait=True) # Clear previous output\n",
        "            print(\"🔄 Starting ingestion process...\")\n",
        "            uploaded_files_info = upload_button.value\n",
        "            uploaded_file_paths = []\n",
        "            current_dir = os.getcwd() # Get the current working directory (which is /content/ in Colab)\n",
        "\n",
        "            if not uploaded_files_info:\n",
        "                print(\"⚠️ No files selected for ingestion.\")\n",
        "                return\n",
        "\n",
        "            # Save files to the current directory and collect paths\n",
        "            for filename, file_info in uploaded_files_info.items():\n",
        "                file_path = os.path.join(current_dir, filename)\n",
        "                try:\n",
        "                    with open(file_path, 'wb') as f:\n",
        "                        f.write(file_info['content'])\n",
        "                    print(f\"✅ Uploading and saving: {file_path}\")\n",
        "                    uploaded_file_paths.append(file_path)\n",
        "                except Exception as e:\n",
        "                    print(f\"❌ Error saving file {filename}: {e}\")\n",
        "\n",
        "            print(f\"Processing {len(uploaded_file_paths)} files...\")\n",
        "\n",
        "            # Implement batch processing\n",
        "            batch_size = 10  # Define batch size\n",
        "            num_files = len(uploaded_file_paths)\n",
        "            num_batches = (num_files + batch_size - 1) // batch_size  # Calculate number of batches\n",
        "\n",
        "            if not uploaded_file_paths:\n",
        "                print(\"⚠️ No files were successfully uploaded for ingestion.\")\n",
        "                # Disable the ingest button after processing\n",
        "                ingest_button.disabled = True\n",
        "                return\n",
        "\n",
        "\n",
        "            print(f\"Total files: {num_files}, Batch size: {batch_size}, Number of batches: {num_batches}\")\n",
        "\n",
        "            for i in range(num_batches):\n",
        "                start_index = i * batch_size\n",
        "                end_index = min((i + 1) * batch_size, num_files)\n",
        "                current_batch_paths = uploaded_file_paths[start_index:end_index]\n",
        "\n",
        "                print(f\"\\nProcessing batch {i+1}/{num_batches} ({len(current_batch_paths)} files)...\")\n",
        "\n",
        "                try:\n",
        "                    # Assuming 'ingester' is accessible in this scope (it is if run after run_demo)\n",
        "                    ingester.ingest_document(current_batch_paths)\n",
        "                    print(f\"✅ Ingestion complete for batch {i+1}.\")\n",
        "                except NameError:\n",
        "                    print(\"❌ Ingester not initialized. Please run the setup cells first.\")\n",
        "                    break # Stop processing batches if ingester is not available\n",
        "                except Exception as e:\n",
        "                    print(f\"❌ Error during ingestion of batch {i+1}: {e}\")\n",
        "                    # Decide if you want to continue with the next batch or stop\n",
        "                    # For now, let's continue with the next batch\n",
        "                    continue\n",
        "\n",
        "                # Add a small time delay between batches\n",
        "                if i < num_batches - 1: # Don't wait after the last batch\n",
        "                    time.sleep(1) # 1-second delay\n",
        "\n",
        "\n",
        "            print(\"\\n✅ All ingestion batches processed.\")\n",
        "            # Note: Re-initializing the search engine after ingestion is recommended\n",
        "            # but for this demo structure, the user will be instructed to do so manually\n",
        "            # after the ingestion callback finishes.\n",
        "            print(\"\\nReady to re-initialize the search engine. Please run the next code cell.\")\n",
        "\n",
        "\n",
        "            # Disable the ingest button after processing\n",
        "            ingest_button.disabled = True\n",
        "\n",
        "\n",
        "    upload_button.observe(on_upload_change, names='value')\n",
        "    ingest_button.on_click(on_ingest_button_click)\n",
        "\n",
        "    display(upload_button, ingest_button, output_area)\n",
        "\n",
        "    return upload_button, ingest_button, output_area\n",
        "\n",
        "# Update the main execution block to use the modified interface\n",
        "if __name__ == \"__main__\":\n",
        "    # Run the demo\n",
        "    print(\"🚀 Starting Hybrid Multimodal Search Demo\")\n",
        "    print(\"=\" * 50)\n",
        "    qdrant_client = create_qdrant_client()\n",
        "    # Initialize system\n",
        "    # ingester needs to be in a scope accessible by the upload widget's callback\n",
        "    # Let's define it here before creating the widget\n",
        "    ingester = DocumentIngester(qdrant_client)\n",
        "\n",
        "\n",
        "    # Create upload interface and button\n",
        "    upload_widget, ingest_button, output_area = create_upload_interface_and_button()\n",
        "\n",
        "    # Note: The actual ingestion is now triggered by clicking the \"Start Ingestion\" button.\n",
        "    # We instruct the user to re-initialize the search engine manually after ingestion.\n",
        "\n",
        "    print(\"\"\"\n",
        "📋 INSTRUCTIONS:\n",
        "1. Use the 'Choose Files' button above to select PDF and/or Markdown files (select multiple files).\n",
        "2. After selecting files, click the 'Start Ingestion' button to begin processing.\n",
        "3. Wait for processing and ingestion to complete for all uploaded files (watch the output above the button).\n",
        "4. After ingestion, run the code cell below this one to re-initialize the search engine with the newly ingested data.\n",
        "5. Then, run searches using the search_engine.hybrid_search() function.\n",
        "\n",
        "Example usage:\n",
        "search_engine.hybrid_search(\"your query here\")\n",
        "search_engine.display_results(results)\n",
        "\"\"\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🚀 Starting Hybrid Multimodal Search Demo\n",
            "==================================================\n",
            "✅ Qdrant client initialized successfully!\n",
            "✅ Collection 'calcom_help_docs' already exists.\n",
            "⚠️ BM25 corpus file not found at bm25_calcom_corpus.json. BM25 index will be built during ingestion.\n",
            "📁 Upload PDF or Markdown documents (select multiple files):\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "FileUpload(value={}, accept='.pdf,.md', description='Choose Files', multiple=True)"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c71e4c547d8744bfaac76426e8c32656"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Button(description='Start Ingestion', disabled=True, style=ButtonStyle(), tooltip='Click to start ingestion of…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f32e3b4835114e9e87c32aa046f52893"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Output()"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4b097d69c2b5444a80d9b2c0669d2ac6"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "📋 INSTRUCTIONS:\n",
            "1. Use the 'Choose Files' button above to select PDF and/or Markdown files (select multiple files).\n",
            "2. After selecting files, click the 'Start Ingestion' button to begin processing.\n",
            "3. Wait for processing and ingestion to complete for all uploaded files (watch the output above the button).\n",
            "4. After ingestion, run the code cell below this one to re-initialize the search engine with the newly ingested data.\n",
            "5. Then, run searches using the search_engine.hybrid_search() function.\n",
            "\n",
            "Example usage:\n",
            "search_engine.hybrid_search(\"your query here\")\n",
            "search_engine.display_results(results)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e9846e19"
      },
      "source": [
        "## Process batches\n",
        "\n",
        "### Subtask:\n",
        "Iterate through the batches and call the `ingest_document` method (or a modified version of it) for each batch.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "30bf323f"
      },
      "source": [
        "## Update progress feedback\n",
        "\n",
        "### Subtask:\n",
        "Add feedback to the output area to show the progress of batch ingestion (e.g., \"Processing batch 1 of 10\").\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0eefea65"
      },
      "source": [
        "**Reasoning**:\n",
        "Update the `on_ingest_button_click` function to add feedback to the output area, showing the progress of batch ingestion.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "465f05bdcc594872963ba0009c795210",
            "7aa1912dfe5a454cbb6a4ce89eec789c",
            "39fcc31d3f6d45859f95cb449328f852",
            "9f89549e4c3c4eb1a029170795fb47b9",
            "adc8212b185144708b2ecc169d4be567",
            "96da2fb0af1d4bb78f7c3e866362e4e2",
            "e4feeed8d92c42d38474b392eefa534b",
            "6948556847e94fb59fca35c2d79d88c2"
          ]
        },
        "id": "792f8b24",
        "outputId": "50788f3b-28f8-48ef-acf6-b0b1965bf2b4"
      },
      "source": [
        "def create_upload_interface_and_button():\n",
        "    \"\"\"Create file upload interface and a button to trigger ingestion.\"\"\"\n",
        "    print(\"📁 Upload PDF or Markdown documents (select multiple files):\")\n",
        "\n",
        "    upload_button = widgets.FileUpload(\n",
        "        accept='.pdf,.md', # Accept both PDF and Markdown files\n",
        "        multiple=True,    # Allow multiple file selection\n",
        "        description='Choose Files'\n",
        "    )\n",
        "\n",
        "    ingest_button = widgets.Button(\n",
        "        description='Start Ingestion',\n",
        "        disabled=True, # Disable initially until files are uploaded\n",
        "        button_style='', # 'success', 'info', 'warning', 'danger' or ''\n",
        "        tooltip='Click to start ingestion of selected files'\n",
        "    )\n",
        "\n",
        "    output_area = widgets.Output() # Area to display messages\n",
        "\n",
        "\n",
        "    def on_upload_change(change):\n",
        "        with output_area:\n",
        "            print(\"Debug: on_upload_change triggered\") # Debug print\n",
        "            if change['new']:\n",
        "                print(\"Debug: 'change[new]' is not empty. Enabling Ingest button.\") # Debug print\n",
        "                # Files have been selected, enable the ingest button\n",
        "                ingest_button.disabled = False\n",
        "                # Clear previous output area content related to upload change\n",
        "                output_area.clear_output(wait=True)\n",
        "                print(f\"✅ {len(change['new'])} file(s) selected. Click 'Start Ingestion' to proceed.\")\n",
        "            else:\n",
        "                print(\"Debug: 'change[new]' is empty. Disabling Ingest button.\") # Debug print\n",
        "                # No files selected, disable the ingest button\n",
        "                ingest_button.disabled = True\n",
        "                output_area.clear_output(wait=True)\n",
        "                print(\"Please select file(s) to enable ingestion.\")\n",
        "\n",
        "\n",
        "    def on_ingest_button_click(b):\n",
        "        with output_area:\n",
        "            print(\"Debug: on_ingest_button_click triggered\") # Debug print\n",
        "            output_area.clear_output(wait=True) # Clear previous output\n",
        "            print(\"🔄 Starting ingestion process...\")\n",
        "            uploaded_files_info = upload_button.value\n",
        "            uploaded_file_paths = []\n",
        "            current_dir = os.getcwd() # Get the current working directory (which is /content/ in Colab)\n",
        "\n",
        "            if not uploaded_files_info:\n",
        "                print(\"⚠️ No files selected for ingestion.\")\n",
        "                return\n",
        "\n",
        "            # Save files to the current directory and collect paths\n",
        "            for filename, file_info in uploaded_files_info.items():\n",
        "                file_path = os.path.join(current_dir, filename)\n",
        "                try:\n",
        "                    with open(file_path, 'wb') as f:\n",
        "                        f.write(file_info['content'])\n",
        "                    # Use append_stdout to show continuous output in the widget's output area\n",
        "                    output_area.append_stdout(f\"✅ Uploading and saving: {file_path}\\n\")\n",
        "                    uploaded_file_paths.append(file_path)\n",
        "                except Exception as e:\n",
        "                    output_area.append_stdout(f\"❌ Error saving file {filename}: {e}\\n\")\n",
        "\n",
        "            output_area.append_stdout(f\"\\nProcessing {len(uploaded_file_paths)} files...\\n\")\n",
        "\n",
        "            # Implement batch processing\n",
        "            batch_size = 10  # Define batch size\n",
        "            num_files = len(uploaded_file_paths)\n",
        "            num_batches = (num_files + batch_size - 1) // batch_size  # Calculate number of batches\n",
        "\n",
        "            if not uploaded_file_paths:\n",
        "                output_area.append_stdout(\"⚠️ No files were successfully uploaded for ingestion.\\n\")\n",
        "                # Disable the ingest button after processing\n",
        "                ingest_button.disabled = True\n",
        "                return\n",
        "\n",
        "\n",
        "            output_area.append_stdout(f\"Total files: {num_files}, Batch size: {batch_size}, Number of batches: {num_batches}\\n\")\n",
        "\n",
        "            for i in range(num_batches):\n",
        "                start_index = i * batch_size\n",
        "                end_index = min((i + 1) * batch_size, num_files)\n",
        "                current_batch_paths = uploaded_file_paths[start_index:end_index]\n",
        "\n",
        "                # Add feedback for processing each batch\n",
        "                output_area.append_stdout(f\"\\nProcessing batch {i+1}/{num_batches} ({len(current_batch_paths)} files)...\\n\")\n",
        "\n",
        "                try:\n",
        "                    # Assuming 'ingester' is accessible in this scope (it is if run after run_demo)\n",
        "                    ingester.ingest_document(current_batch_paths)\n",
        "                    output_area.append_stdout(f\"✅ Ingestion complete for batch {i+1}.\\n\")\n",
        "                except NameError:\n",
        "                    output_area.append_stdout(\"❌ Ingester not initialized. Please run the setup cells first.\\n\")\n",
        "                    break # Stop processing batches if ingester is not available\n",
        "                except Exception as e:\n",
        "                    output_area.append_stdout(f\"❌ Error during ingestion of batch {i+1}: {e}\\n\")\n",
        "                    # Decide if you want to continue with the next batch or stop\n",
        "                    # For now, let's continue with the next batch\n",
        "                    continue\n",
        "\n",
        "                # Add a small time delay between batches\n",
        "                if i < num_batches - 1: # Don't wait after the last batch\n",
        "                    time.sleep(1) # 1-second delay\n",
        "\n",
        "\n",
        "            # Add final message after batch processing is complete\n",
        "            output_area.append_stdout(\"\\n✅ All ingestion batches processed.\\n\")\n",
        "            # Note: Re-initializing the search engine after ingestion is recommended\n",
        "            # but for this demo structure, the user will be instructed to do so manually\n",
        "            # after the ingestion callback finishes.\n",
        "            output_area.append_stdout(\"\\nReady to re-initialize the search engine. Please run the next code cell.\\n\")\n",
        "\n",
        "\n",
        "            # Disable the ingest button after processing\n",
        "            ingest_button.disabled = True\n",
        "\n",
        "\n",
        "    upload_button.observe(on_upload_change, names='value')\n",
        "    ingest_button.on_click(on_ingest_button_click)\n",
        "\n",
        "    display(upload_button, ingest_button, output_area)\n",
        "\n",
        "    return upload_button, ingest_button, output_area\n",
        "\n",
        "# Update the main execution block to use the modified interface\n",
        "if __name__ == \"__main__\":\n",
        "    # Run the demo\n",
        "    print(\"🚀 Starting Hybrid Multimodal Search Demo\")\n",
        "    print(\"=\" * 50)\n",
        "    qdrant_client = create_qdrant_client()\n",
        "    # Initialize system\n",
        "    # ingester needs to be in a scope accessible by the upload widget's callback\n",
        "    # Let's define it here before creating the widget\n",
        "    ingester = DocumentIngester(qdrant_client)\n",
        "\n",
        "\n",
        "    # Create upload interface and button\n",
        "    upload_widget, ingest_button, output_area = create_upload_interface_and_button()\n",
        "\n",
        "    # Note: The actual ingestion is now triggered by clicking the \"Start Ingestion\" button.\n",
        "    # We instruct the user to re-initialize the search engine manually after ingestion.\n",
        "\n",
        "    print(\"\"\"\n",
        "📋 INSTRUCTIONS:\n",
        "1. Use the 'Choose Files' button above to select PDF and/or Markdown files (select multiple files).\n",
        "2. After selecting files, click the 'Start Ingestion' button to begin processing.\n",
        "3. Wait for processing and ingestion to complete for all uploaded files (watch the output above the button).\n",
        "4. After ingestion, run the code cell below this one to re-initialize the search engine with the newly ingested data.\n",
        "5. Then, run searches using the search_engine.hybrid_search() function.\n",
        "\n",
        "Example usage:\n",
        "search_engine.hybrid_search(\"your query here\")\n",
        "search_engine.display_results(results)\n",
        "\"\"\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🚀 Starting Hybrid Multimodal Search Demo\n",
            "==================================================\n",
            "✅ Qdrant client initialized successfully!\n",
            "✅ Collection 'calcom_help_docs' already exists.\n",
            "✅ BM25 corpus loaded from bm25_calcom_corpus.json\n",
            "✅ BM25 index rebuilt from loaded corpus.\n",
            "📁 Upload PDF or Markdown documents (select multiple files):\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "FileUpload(value={}, accept='.pdf,.md', description='Choose Files', multiple=True)"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "465f05bdcc594872963ba0009c795210"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Button(description='Start Ingestion', disabled=True, style=ButtonStyle(), tooltip='Click to start ingestion of…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9f89549e4c3c4eb1a029170795fb47b9"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Output()"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e4feeed8d92c42d38474b392eefa534b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "📋 INSTRUCTIONS:\n",
            "1. Use the 'Choose Files' button above to select PDF and/or Markdown files (select multiple files).\n",
            "2. After selecting files, click the 'Start Ingestion' button to begin processing.\n",
            "3. Wait for processing and ingestion to complete for all uploaded files (watch the output above the button).\n",
            "4. After ingestion, run the code cell below this one to re-initialize the search engine with the newly ingested data.\n",
            "5. Then, run searches using the search_engine.hybrid_search() function.\n",
            "\n",
            "Example usage:\n",
            "search_engine.hybrid_search(\"your query here\")\n",
            "search_engine.display_results(results)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "65b490b1"
      },
      "source": [
        "## Testing with batches\n",
        "\n",
        "### Subtask:\n",
        "Test the batch ingestion process with a smaller number of files that can be divided into multiple batches to ensure the logic works correctly.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e60888a7"
      },
      "source": [
        "**Reasoning**:\n",
        "Create a directory named `test_batch_ingestion`, create several simple markdown files within it with some basic text content, and create sample image files to be referenced in the markdown.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b8e132d4"
      },
      "source": [
        "**Reasoning**:\n",
        "Initialize the Qdrant client and DocumentIngester, then call the `ingest_document` method with the list of created markdown file paths to test the batch ingestion process.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4c9dcd01"
      },
      "source": [
        "# 6. Initialize the Qdrant client and DocumentIngester\n",
        "# Assuming create_qdrant_client and DocumentIngester are defined in previous cells and accessible\n",
        "qdrant_client = create_qdrant_client()\n",
        "ingester = DocumentIngester(qdrant_client)\n",
        "\n",
        "# 7. Call the ingest_document method with the list of markdown file paths\n",
        "# Assuming markdown_file_paths list is available from the previous step\n",
        "ingester.ingest_document(markdown_file_paths)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fec9c2f1"
      },
      "source": [
        "## Update demo workflow instructions\n",
        "\n",
        "### Subtask:\n",
        "Adjust the instructions for the user to reflect the batch ingestion process.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e6c0597b"
      },
      "source": [
        "**Reasoning**:\n",
        "Modify the instructions in the main execution block to clearly explain the batch ingestion process using the updated interface, guiding the user to select multiple files, click the ingest button, and watch the output for progress.\n",
        "\n"
      ]
    }
  ]
}